# ğŸ“š ArXiv è®ºæ–‡æ—¥æŠ¥

> æ¯å¤©è‡ªåŠ¨æ›´æ–°ï¼Œå…³æ³¨ **åŒ–å­¦å¤§æ¨¡å‹, è´¨è°±ç»“æ„æ¨ç†** ç›¸å…³çš„æœ€æ–°è®ºæ–‡

## æ›´æ–°æ—¶é—´
â° 2026-03-01 12:35:30

## ğŸ“… 2026-03-01 (ä»Šæ—¥æœ€æ–°)

**ç›¸å…³è®ºæ–‡æ•°ï¼š45**

### 1. [Zatom-1: A Multimodal Flow Foundation Model for 3D Molecules and Materials](https://arxiv.org/abs/2602.22251)

**åŸºæœ¬ä¿¡æ¯**

- ğŸ”— arXiv: [`2602.22251`](https://arxiv.org/abs/2602.22251)
- ğŸ‘¥ ä½œè€…: Alex Morehead, Miruna Cretu, Antonia Panescu ç­‰17äºº
- ğŸ“„ PDF: [ä¸‹è½½](https://arxiv.org/pdf/2602.22251.pdf)

**ğŸ’¡ ç›¸å…³æ€§åˆ†æ**

æ»¡è¶³æ ‡å‡†1ï¼šè®ºæ–‡çš„ä¸»è¦ç ”ç©¶å†…å®¹æ˜¯å¼€å‘ä¸€ä¸ªç”¨äº3Dåˆ†å­å’Œææ–™çš„ç»Ÿä¸€ç”Ÿæˆä¸é¢„æµ‹åŸºç¡€æ¨¡å‹ï¼ˆZatom-1ï¼‰ï¼Œè¿™ç›´æ¥å›´ç»•â€œåŒ–å­¦å¤§æ¨¡å‹â€è¿™ä¸€æ ¸å¿ƒä¸»é¢˜ã€‚

**ğŸ“– ä¸­æ–‡æ‘˜è¦**

æœ¬æ–‡ä»‹ç»äº†Zatom-1ï¼Œä¸€ä¸ªç”¨äº3Dåˆ†å­å’Œææ–™çš„ç»Ÿä¸€åŸºç¡€æ¨¡å‹ã€‚è¯¥æ¨¡å‹æ˜¯ä¸€ä¸ªTransformerï¼Œé€šè¿‡å¤šæ¨¡æ€æµåŒ¹é…ç›®æ ‡è¿›è¡Œè®­ç»ƒï¼Œè”åˆå»ºæ¨¡ç¦»æ•£åŸå­ç±»å‹å’Œè¿ç»­3Då‡ ä½•ç»“æ„ã€‚è¿™ç§æ–¹æ³•æ”¯æŒå¯æ‰©å±•çš„é¢„è®­ç»ƒï¼Œå¹¶èƒ½å¤Ÿå®ç°å¿«é€Ÿç¨³å®šçš„é‡‡æ ·ã€‚Zatom-1å°†è”åˆç”Ÿæˆå¼é¢„è®­ç»ƒä½œä¸ºä¸‹æ¸¸å¤šä»»åŠ¡é¢„æµ‹ï¼ˆå¦‚æ€§è´¨ã€èƒ½é‡å’ŒåŠ›ï¼‰çš„é€šç”¨åˆå§‹åŒ–ã€‚è¯¥æ¨¡å‹åœ¨ç”Ÿæˆå’Œé¢„æµ‹åŸºå‡†æµ‹è¯•ä¸­åŒ¹é…æˆ–è¶…è¶Šäº†ä¸“é—¨çš„åŸºçº¿æ¨¡å‹ï¼ŒåŒæ—¶å°†ç”Ÿæˆæ¨ç†æ—¶é—´å‡å°‘äº†ä¸€ä¸ªæ•°é‡çº§ä»¥ä¸Šã€‚å®éªŒè¡¨æ˜ï¼Œè”åˆç”Ÿæˆå¼é¢„è®­ç»ƒåœ¨åŒ–å­¦é¢†åŸŸä¹‹é—´äº§ç”Ÿäº†æ­£å‘çš„é¢„æµ‹è¿ç§»ï¼šåœ¨é¢„è®­ç»ƒä¸­å»ºæ¨¡ææ–™æé«˜äº†åˆ†å­æ€§è´¨é¢„æµ‹çš„å‡†ç¡®æ€§ã€‚è¿™ç¯‡è®ºæ–‡çš„æ ¸å¿ƒæ˜¯å¼€å‘ä¸€ä¸ªç”¨äº3DåŒ–å­¦å»ºæ¨¡çš„AIåŸºç¡€æ¨¡å‹ï¼Œç›´æ¥å±äºâ€œåŒ–å­¦å¤§æ¨¡å‹â€çš„ç ”ç©¶èŒƒç•´ã€‚

<details>
<summary><b>ğŸ” æŸ¥çœ‹åŸæ–‡æ‘˜è¦</b></summary>

General-purpose 3D chemical modeling encompasses molecules and materials, requiring both generative and predictive capabilities. However, most existing AI approaches are optimized for a single domain (molecules or materials) and a single task (generation or prediction), which limits representation sharing and transfer. We introduce Zatom-1, the first foundation model that unifies generative and predictive learning of 3D molecules and materials. Zatom-1 is a Transformer trained with a multimodal flow matching objective that jointly models discrete atom types and continuous 3D geometries. This approach supports scalable pretraining with predictable gains as model capacity increases, while enabling fast and stable sampling. We use joint generative pretraining as a universal initialization for downstream multi-task prediction of properties, energies, and forces. Empirically, Zatom-1 matches or outperforms specialized baselines on both generative and predictive benchmarks, while reducing the generative inference time by more than an order of magnitude. Our experiments demonstrate positive predictive transfer between chemical domains from joint generative pretraining: modeling materials during pretraining improves molecular property prediction accuracy.

</details>

---

### 2. [Multi-Level Causal Embeddings](https://arxiv.org/abs/2602.22287)

**åŸºæœ¬ä¿¡æ¯**

- ğŸ”— arXiv: [`2602.22287`](https://arxiv.org/abs/2602.22287)
- ğŸ‘¥ ä½œè€…: Willem Schooltink, Fabio Massimo Zennaro
- ğŸ“„ PDF: [ä¸‹è½½](https://arxiv.org/pdf/2602.22287.pdf)

**ğŸ’¡ ç›¸å…³æ€§åˆ†æ**

æ»¡è¶³æ ‡å‡†1ï¼šè®ºæ–‡çš„æ ¸å¿ƒå†…å®¹æ˜¯ç ”ç©¶å› æœæ¨¡å‹çš„æŠ½è±¡å’ŒåµŒå…¥æ¡†æ¶ã€‚è¿™ä¸€ç†è®ºæ¡†æ¶å¯¹äºæ„å»ºèƒ½å¤Ÿè¿›è¡Œå› æœæ¨ç†çš„â€œåŒ–å­¦å¤§æ¨¡å‹â€ï¼ˆä¾‹å¦‚ï¼Œç†è§£åˆ†å­ç»“æ„å¦‚ä½•å¯¼è‡´ç‰¹å®šæ€§è´¨æˆ–æ´»æ€§ï¼‰å…·æœ‰ç›´æ¥çš„ç›¸å…³æ€§å’ŒæŒ‡å¯¼æ„ä¹‰ã€‚

**ğŸ“– ä¸­æ–‡æ‘˜è¦**

æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªç”¨äºå› æœåµŒå…¥çš„æ¡†æ¶ï¼Œä½œä¸ºæŠ½è±¡æ¦‚å¿µçš„æ³›åŒ–ï¼Œå¹¶æå‡ºäº†ä¸€ä¸ªå¹¿ä¹‰çš„ä¸€è‡´æ€§æ¦‚å¿µã€‚é€šè¿‡å®šä¹‰ä¸€ä¸ªå¤šåˆ†è¾¨ç‡è¾¹é™…é—®é¢˜ï¼Œä½œè€…å±•ç¤ºäº†å› æœåµŒå…¥å¯¹äºç»Ÿè®¡è¾¹é™…é—®é¢˜å’Œå› æœè¾¹é™…é—®é¢˜çš„ç›¸å…³æ€§ã€‚è™½ç„¶è®ºæ–‡æœ¬èº«æ˜¯ç†è®ºæ€§çš„ï¼Œä½†å…¶æ ¸å¿ƒæ˜¯ç ”ç©¶å› æœæ¨¡å‹çš„æŠ½è±¡å’ŒåµŒå…¥ï¼Œæ—¨åœ¨ä¿ç•™å› æœå…³ç³»ã€‚åœ¨åŒ–å­¦ä¿¡æ¯å­¦é¢†åŸŸï¼Œç†è§£åˆ†å­ç»“æ„ã€æ€§è´¨ä¸åŠŸèƒ½ä¹‹é—´çš„å› æœå…³ç³»è‡³å…³é‡è¦ã€‚å› æ­¤ï¼Œè¿™ç¯‡è®ºæ–‡æå‡ºçš„å› æœåµŒå…¥æ¡†æ¶ï¼Œä¸ºæ„å»ºèƒ½å¤Ÿç†è§£å’Œæ¨ç†åŒ–å­¦ç»“æ„ä¸æ€§è´¨ä¹‹é—´å› æœå…³ç³»çš„â€œåŒ–å­¦å¤§æ¨¡å‹â€æä¾›äº†ç†è®ºåŸºç¡€å’Œæ–¹æ³•è®ºå·¥å…·ã€‚

<details>
<summary><b>ğŸ” æŸ¥çœ‹åŸæ–‡æ‘˜è¦</b></summary>

Abstractions of causal models allow for the coarsening of models such that relations of cause and effect are preserved. Whereas abstractions focus on the relation between two models, in this paper we study a framework for causal embeddings which enable multiple detailed models to be mapped into sub-systems of a coarser causal model. We define causal embeddings as a generalization of abstraction, and present a generalized notion of consistency. By defining a multi-resolution marginal problem, we showcase the relevance of causal embeddings for both the statistical marginal problem and the causal marginal problem; furthermore, we illustrate its practical use in merging datasets coming from models with different representations.

</details>

---

### 3. [Quadratization of Autonomous Partial Differential Equations: Theory and Algorithms](https://arxiv.org/abs/2602.22371)

**åŸºæœ¬ä¿¡æ¯**

- ğŸ”— arXiv: [`2602.22371`](https://arxiv.org/abs/2602.22371)
- ğŸ‘¥ ä½œè€…: Albani Olivieri, Gleb Pogudin, Boris Kramer
- ğŸ“„ PDF: [ä¸‹è½½](https://arxiv.org/pdf/2602.22371.pdf)

**ğŸ’¡ ç›¸å…³æ€§åˆ†æ**

æ»¡è¶³æ ‡å‡†1ï¼šè®ºæ–‡çš„æ ¸å¿ƒä¸»é¢˜æ˜¯åå¾®åˆ†æ–¹ç¨‹ï¼ˆPDEsï¼‰çš„äºŒæ¬¡åŒ–ï¼Œè¿™æ˜¯ä¸€ç§å°†å¤æ‚éçº¿æ€§ç³»ç»Ÿè½¬åŒ–ä¸ºæ›´æ˜“å¤„ç†å½¢å¼çš„æ•°å­¦æ–¹æ³•ã€‚ç”±äºè®¸å¤šåŒ–å­¦è¿‡ç¨‹ï¼ˆå¦‚ååº”ã€æ‰©æ•£ã€ä¼ è´¨ï¼‰ç”±PDEæè¿°ï¼Œè¯¥å·¥ä½œä¸ºç®€åŒ–å’Œåˆ†æåŒ–å­¦åŠ¨åŠ›å­¦æ¨¡å‹æä¾›äº†åŸºç¡€å·¥å…·ï¼Œä¸æ„å»ºåŸºäºç‰©ç†åŸç†çš„â€œåŒ–å­¦å¤§æ¨¡å‹â€è¿™ä¸€ä¸»é¢˜ç›¸å…³ã€‚

**ğŸ“– ä¸­æ–‡æ‘˜è¦**

æœ¬æ–‡ç ”ç©¶äº†åå¾®åˆ†æ–¹ç¨‹ï¼ˆPDEsï¼‰çš„äºŒæ¬¡åŒ–é—®é¢˜ï¼Œè¿™æ˜¯ä¸€ä¸ªé€šè¿‡å¼•å…¥è¾…åŠ©å˜é‡å°†éäºŒæ¬¡PDEè½¬åŒ–ä¸ºäºŒæ¬¡å½¢å¼çš„ç¬¦å·å˜æ¢è¿‡ç¨‹ã€‚ä½œè€…æå‡ºäº†PDEäºŒæ¬¡åŒ–çš„ä¸¥æ ¼å®šä¹‰ã€å…³äºä¸€ç»´ç©ºé—´PDEäºŒæ¬¡åŒ–é—®é¢˜çš„ç†è®ºç»“æœï¼ˆåŒ…æ‹¬å­˜åœ¨æ€§å’Œå¤æ‚æ€§ï¼‰ï¼Œå¹¶ä»‹ç»äº†QuPDEç®—æ³•ã€‚è¯¥ç®—æ³•åŸºäºç¬¦å·è®¡ç®—å’Œç¦»æ•£ä¼˜åŒ–ï¼Œå¯ä»¥ä¸ºä»»ä½•ä¸€ç»´ç©ºé—´å¤šé¡¹å¼æˆ–æœ‰ç†PDEè¾“å‡ºä¸€ä¸ªäºŒæ¬¡åŒ–å½¢å¼ã€‚è¿™æ˜¯ç¬¬ä¸€ä¸ªä¸ºPDEå¯»æ‰¾äºŒæ¬¡åŒ–çš„è®¡ç®—å·¥å…·ã€‚åœ¨åŒ–å­¦é¢†åŸŸï¼Œè®¸å¤šç‰©ç†åŒ–å­¦è¿‡ç¨‹ï¼ˆå¦‚ååº”æ‰©æ•£ã€æµä½“åŠ¨åŠ›å­¦ï¼‰éƒ½ä½¿ç”¨PDEå»ºæ¨¡ã€‚å°†å¤æ‚çš„éçº¿æ€§PDEäºŒæ¬¡åŒ–å¯ä»¥ç®€åŒ–å…¶åˆ†æå’Œæ•°å€¼æ¨¡æ‹Ÿã€‚è¿™é¡¹å·¥ä½œä¸ºå¤„ç†åŒ–å­¦ç³»ç»Ÿä¸­çš„å¤æ‚åŠ¨åŠ›å­¦æ¨¡å‹æä¾›äº†æ•°å­¦å·¥å…·ï¼Œé—´æ¥æ”¯æŒäº†æ„å»ºæ›´é«˜æ•ˆã€æ›´å¯è§£é‡Šçš„â€œåŒ–å­¦å¤§æ¨¡å‹â€ï¼ˆç‰¹åˆ«æ˜¯åŸºäºç‰©ç†åŸç†çš„æ¨¡å‹ï¼‰çš„åŠªåŠ›ã€‚

<details>
<summary><b>ğŸ” æŸ¥çœ‹åŸæ–‡æ‘˜è¦</b></summary>

Quadratization for partial differential equations (PDEs) is a process that transforms a nonquadratic PDE into a quadratic form by introducing auxiliary variables. This symbolic transformation has been used in diverse fields to simplify the analysis, simulation, and control of nonlinear and nonquadratic PDE models. This paper presents a rigorous definition of PDE quadratization, theoretical results for the PDE quadratization problem of spatially one-dimensional PDEs-including results on existence and complexity-and introduces QuPDE, an algorithm based on symbolic computation and discrete optimization that outputs a quadratization for any spatially one-dimensional polynomial or rational PDE. This algorithm is the first computational tool to find quadratizations for PDEs to date. We demonstrate QuPDE's performance by applying it to fourteen nonquadratic PDEs in diverse areas such as fluid mechanics, space physics, chemical engineering, and biological processes. QuPDE delivers a low-order quadratization in each case, uncovering quadratic transformations with fewer auxiliary variables than those previously discovered in the literature for some examples, and finding quadratizations for systems that had not been transformed to quadratic form before.

</details>

---

### 4. [A Reduced Order Model approach for First-Principles Molecular Dynamics Computations](https://arxiv.org/abs/2602.22390)

**åŸºæœ¬ä¿¡æ¯**

- ğŸ”— arXiv: [`2602.22390`](https://arxiv.org/abs/2602.22390)
- ğŸ‘¥ ä½œè€…: Siu Wun Cheung, Youngsoo Choi, Jean-Luc Fattebert ç­‰5äºº
- ğŸ“„ PDF: [ä¸‹è½½](https://arxiv.org/pdf/2602.22390.pdf)

**ğŸ’¡ ç›¸å…³æ€§åˆ†æ**

æ»¡è¶³æ ‡å‡†1ï¼šè®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºæ•°æ®é©±åŠ¨å’Œé™é˜¶æ¨¡å‹çš„æ–¹æ³•æ¥åŠ é€Ÿç¬¬ä¸€æ€§åŸç†åˆ†å­åŠ¨åŠ›å­¦ä¸­çš„ç”µå­ç»“æ„è®¡ç®—ã€‚è¿™ç›´æ¥å…³ç³»åˆ°åˆ©ç”¨AIæŠ€æœ¯æ„å»ºé«˜æ•ˆã€å‡†ç¡®çš„â€œåŒ–å­¦å¤§æ¨¡å‹â€ç”¨äºåˆ†å­æ¨¡æ‹Ÿå’Œæ€§è´¨é¢„æµ‹ã€‚

**ğŸ“– ä¸­æ–‡æ‘˜è¦**

ä¸ºäº†åˆ©ç”¨ç¬¬ä¸€æ€§åŸç†åˆ†å­åŠ¨åŠ›å­¦æ¯ä¸€æ­¥è®¡ç®—å‡ºçš„ç”µå­ç»“æ„ä¹‹é—´çš„å†—ä½™æ€§ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§ç”¨äºKohn-Shamå¯†åº¦æ³›å‡½ç†è®ºçš„æ•°æ®é©±åŠ¨å»ºæ¨¡æ¡†æ¶ï¼Œç»•è¿‡äº†å¯¹ç”µå­æ³¢å‡½æ•°çš„æ˜¾å¼ä¼˜åŒ–ã€‚è¯¥æ–¹æ³•é€šè¿‡å…ˆéªŒé‡‡æ ·ä»£è¡¨æ€§åŸå­æ„å‹ï¼Œæ„å»ºä¸€ä¸ªèƒ½æœ‰æ•ˆè¿‘ä¼¼ç”µå­ç»“æ„å­ç©ºé—´çš„ä½ç»´åŸºã€‚éšåï¼Œåœ¨ç”µå­å•ç²’å­å¯†åº¦çŸ©é˜µçš„ç›´æ¥æ±‚è§£å™¨ä¸­ä½¿ç”¨è¿™ä¸ªçº¦åŒ–åŸºï¼Œä»è€Œæ— éœ€è¿­ä»£æ³¢å‡½æ•°ä¼˜åŒ–å³å¯é«˜æ•ˆç¡®å®šåŸºæ€ã€‚ä½œè€…ä»¥æ°´åˆ†å­çš„ç»æ©-å¥¥æœ¬æµ·é»˜åˆ†å­åŠ¨åŠ›å­¦ä¸ºä¾‹ï¼Œå±•ç¤ºäº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œè¯æ˜æ‰€å¾—æ¨¡æ‹Ÿèƒ½å‡†ç¡®å¤ç°ä»å®Œæ•´ç¬¬ä¸€æ€§åŸç†åˆ†å­åŠ¨åŠ›å­¦è·å¾—çš„å…³é”®ç»“æ„æ€§è´¨ï¼ˆå¦‚é”®é•¿å’Œé”®è§’ï¼‰ã€‚è¿™é¡¹å·¥ä½œçªå‡ºäº†æ•°æ®é©±åŠ¨æ–¹æ³•åœ¨ä¸ºç¬¬ä¸€æ€§åŸç†æ¨¡æ‹Ÿå¼€å‘é«˜æ•ˆç”µå­ç»“æ„æ±‚è§£å™¨æ–¹é¢çš„æ½œåŠ›ã€‚è¯¥ç ”ç©¶ç›´æ¥æ¶‰åŠä½¿ç”¨AI/æœºå™¨å­¦ä¹ æ–¹æ³•åŠ é€Ÿé‡å­åŒ–å­¦è®¡ç®—ï¼Œè¿™æ˜¯æ„å»ºé«˜ç²¾åº¦â€œåŒ–å­¦å¤§æ¨¡å‹â€ï¼ˆç”¨äºåˆ†å­æ¨¡æ‹Ÿå’Œæ€§è´¨é¢„æµ‹ï¼‰çš„æ ¸å¿ƒæŠ€æœ¯ä¹‹ä¸€ã€‚

<details>
<summary><b>ğŸ” æŸ¥çœ‹åŸæ–‡æ‘˜è¦</b></summary>

To leverage the redundancy between the electronic structure computed at each step of first-principles molecular dynamics, we present a data-driven modeling framework for Kohn-Sham Density Functional Theory that bypasses the explicit optimization of electronic wavefunctions. We sample a priori representative atomic configurations and construct a low-dimensional basis that efficiently approximates the electronic structure subspace. Subsequently, we employ this reduced basis in a direct solver for the electronic single particle density matrix, thereby enabling the efficient determination of ground state without iterative wavefunction optimization. We demonstrate the efficacy of our approach in a Born-Oppenheimer molecular dynamics of a water molecule, showing that the resulting simulations accurately reproduce key structural properties, such as bond lengths and bond angle, obtained from full first-principles molecular dynamics. This work highlights the potential of data-driven approaches to develop efficient electronic structure solvers for first-principles simulations.

</details>

---

### 5. [MolFM-Lite: Multi-Modal Molecular Property Prediction with Conformer Ensemble Attention and Cross-Modal Fusion](https://arxiv.org/abs/2602.22405)

**åŸºæœ¬ä¿¡æ¯**

- ğŸ”— arXiv: [`2602.22405`](https://arxiv.org/abs/2602.22405)
- ğŸ‘¥ ä½œè€…: Syed Omer Shah, Mohammed Maqsood Ahmed, Danish Mohiuddin Mohammed ç­‰5äºº
- ğŸ“„ PDF: [ä¸‹è½½](https://arxiv.org/pdf/2602.22405.pdf)

**ğŸ’¡ ç›¸å…³æ€§åˆ†æ**

æ»¡è¶³æ ‡å‡†1ï¼šè®ºæ–‡çš„æ ¸å¿ƒç ”ç©¶å†…å®¹æ˜¯å¼€å‘ä¸€ä¸ªæ•´åˆ1Dã€2Dã€3Dåˆ†å­è¡¨ç¤ºçš„å¤šæ¨¡æ€æ¨¡å‹ï¼Œè¿™ç›´æ¥å±äºâ€œåŒ–å­¦å¤§æ¨¡å‹â€çš„ä¸»é¢˜ã€‚

**ğŸ“– ä¸­æ–‡æ‘˜è¦**

æœ¬æ–‡æå‡ºäº†MolFM-Liteï¼Œä¸€ä¸ªç”¨äºåˆ†å­æ€§è´¨é¢„æµ‹çš„å¤šæ¨¡æ€æ¨¡å‹ã€‚å®ƒè”åˆç¼–ç SELFIESåºåˆ—ï¼ˆ1Dï¼‰ã€åˆ†å­å›¾ï¼ˆ2Dï¼‰å’Œæ„è±¡ä½“é›†åˆï¼ˆ3Dï¼‰ï¼Œå¹¶é€šè¿‡äº¤å‰æ³¨æ„åŠ›è¿›è¡Œèåˆã€‚è¯¥æ¨¡å‹çš„æ ¸å¿ƒè´¡çŒ®åŒ…æ‹¬æ„è±¡ä½“é›†åˆæ³¨æ„åŠ›æœºåˆ¶å’Œè·¨æ¨¡æ€èåˆå±‚ã€‚è™½ç„¶è®ºæ–‡ä¸»è¦å…³æ³¨åˆ†å­æ€§è´¨é¢„æµ‹ï¼Œä½†å…¶æ ¸å¿ƒæ–¹æ³•â€”â€”æ•´åˆå¤šç§åˆ†å­è¡¨ç¤ºï¼ˆåºåˆ—ã€å›¾ã€3Dç»“æ„ï¼‰å¹¶è¿›è¡Œè·¨æ¨¡æ€èåˆâ€”â€”ç›´æ¥å±äºâ€œåŒ–å­¦å¤§æ¨¡å‹â€çš„ç ”ç©¶èŒƒç•´ã€‚æ¨¡å‹åœ¨ZINC250Kæ•°æ®é›†ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œå¹¶å‘å¸ƒäº†æ‰€æœ‰ä»£ç ã€è®­ç»ƒæ¨¡å‹å’Œæ•°æ®åˆ†å‰²ï¼Œä¸ºåŒ–å­¦ä¿¡æ¯å­¦é¢†åŸŸæä¾›äº†å¯å¤ç°çš„å·¥å…·å’Œèµ„æºã€‚

<details>
<summary><b>ğŸ” æŸ¥çœ‹åŸæ–‡æ‘˜è¦</b></summary>

Most machine learning models for molecular property prediction rely on a single molecular representation (either a sequence, a graph, or a 3D structure) and treat molecular geometry as static. We present MolFM-Lite, a multi-modal model that jointly encodes SELFIES sequences (1D), molecular graphs (2D), and conformer ensembles (3D) through cross-attention fusion, while conditioning predictions on experimental context via Feature-wise Linear Modulation (FiLM). Our main methodological contributions are: (1) a conformer ensemble attention mechanism that combines learnable attention with Boltzmann-weighted priors over multiple RDKit-generated conformers, capturing the thermodynamic distribution of molecular shapes; and (2) a cross-modal fusion layer where each modality can attend to others, enabling complementary information sharing. We evaluate on four MoleculeNet scaffold-split benchmarks using our model's own splits, and report all baselines re-evaluated under the same protocol. Comprehensive ablation studies across all four datasets confirm that each architectural component contributes independently, with tri-modal fusion providing 7-11% AUC improvement over single-modality baselines and conformer ensembles adding approximately 2% over single-conformer variants. Pre-training on ZINC250K (~250K molecules) using cross-modal contrastive and masked-atom objectives enables effective weight initialization at modest compute cost. We release all code, trained models, and data splits to support reproducibility.

</details>

---

### 6. [Mapping the Landscape of Artificial Intelligence in Life Cycle Assessment Using Large Language Models](https://arxiv.org/abs/2602.22500)

**åŸºæœ¬ä¿¡æ¯**

- ğŸ”— arXiv: [`2602.22500`](https://arxiv.org/abs/2602.22500)
- ğŸ‘¥ ä½œè€…: Anastasija Mensikova, Donna M. Rizzo, Kathryn Hinkelman
- ğŸ“„ PDF: [ä¸‹è½½](https://arxiv.org/pdf/2602.22500.pdf)

**ğŸ’¡ ç›¸å…³æ€§åˆ†æ**

æ»¡è¶³æ ‡å‡†3ï¼šè®ºæ–‡æ˜¯å…³äºAIï¼ˆåŒ…æ‹¬å¤§è¯­è¨€æ¨¡å‹ï¼‰åœ¨ç§‘å­¦é¢†åŸŸï¼ˆç”Ÿå‘½å‘¨æœŸè¯„ä¼°ï¼‰åº”ç”¨çš„ç»¼è¿°ï¼ŒåŒ…å«äº†å¯¹AIæ–¹æ³•ï¼ˆåŒ…æ‹¬LLMsï¼‰å‘å±•è¶‹åŠ¿çš„é‡è¦è®¨è®ºï¼Œä¸â€œåŒ–å­¦å¤§æ¨¡å‹â€è¿™ä¸€å¹¿æ³›ä¸»é¢˜ç›¸å…³ã€‚

**ğŸ“– ä¸­æ–‡æ‘˜è¦**

æœ¬æ–‡ç»¼è¿°äº†äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰åœ¨ç”Ÿå‘½å‘¨æœŸè¯„ä¼°ï¼ˆLCAï¼‰ä¸­çš„æ•´åˆåº”ç”¨ã€‚ç ”ç©¶åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¯¹AI-LCAäº¤å‰é¢†åŸŸçš„å·²å‘è¡¨å·¥ä½œè¿›è¡Œè¯¦ç»†å›é¡¾ï¼Œä»¥è¯†åˆ«å½“å‰è¶‹åŠ¿ã€æ–°å…´ä¸»é¢˜å’Œæœªæ¥æ–¹å‘ã€‚åˆ†æè¡¨æ˜ï¼Œéšç€LCAç ”ç©¶çš„æ‰©å±•ï¼ŒAIæŠ€æœ¯çš„é‡‡ç”¨æ€¥å‰§å¢é•¿ï¼Œå¹¶æ˜æ˜¾è½¬å‘LLMé©±åŠ¨çš„æ–¹æ³•ã€‚é€šè¿‡å°†åŸºäºLLMçš„æ–‡æœ¬æŒ–æ˜æ–¹æ³•ä¸ä¼ ç»Ÿçš„æ–‡çŒ®ç»¼è¿°æŠ€æœ¯ç›¸ç»“åˆï¼Œæœ¬ç ”ç©¶å¼•å…¥äº†ä¸€ä¸ªåŠ¨æ€æœ‰æ•ˆçš„æ¡†æ¶ï¼Œèƒ½å¤Ÿæ•æ‰è¯¥é¢†åŸŸçš„é«˜å±‚ç ”ç©¶è¶‹åŠ¿å’Œç»†å¾®çš„æ¦‚å¿µæ¨¡å¼ã€‚è¿™é¡¹å·¥ä½œå±•ç¤ºäº†LLMè¾…åŠ©æ–¹æ³•åœ¨æ”¯æŒè·¨å¹¿æ³›ç ”ç©¶é¢†åŸŸçš„å¤§è§„æ¨¡ã€å¯é‡å¤ç»¼è¿°æ–¹é¢çš„æ½œåŠ›ï¼ŒåŒæ—¶è¯„ä¼°äº†åœ¨AIæŠ€æœ¯å¿«é€Ÿå‘å±•èƒŒæ™¯ä¸‹å®ç°è®¡ç®—é«˜æ•ˆLCAçš„é€”å¾„ã€‚

<details>
<summary><b>ğŸ” æŸ¥çœ‹åŸæ–‡æ‘˜è¦</b></summary>

Integration of artificial intelligence (AI) into life cycle assessment (LCA) has accelerated in recent years, with numerous studies successfully adapting machine learning algorithms to support various stages of LCA. Despite this rapid development, comprehensive and broad synthesis of AI-LCA research remains limited. To address this gap, this study presents a detailed review of published work at the intersection of AI and LCA, leveraging large language models (LLMs) to identify current trends, emerging themes, and future directions. Our analyses reveal that as LCA research continues to expand, the adoption of AI technologies has grown dramatically, with a noticeable shift toward LLM-driven approaches, continued increases in ML applications, and statistically significant correlations between AI approaches and corresponding LCA stages. By integrating LLM-based text-mining methods with traditional literature review techniques, this study introduces a dynamic and effective framework capable of capturing both high-level research trends and nuanced conceptual patterns (themes) across the field. Collectively, these findings demonstrate the potential of LLM-assisted methodologies to support large-scale, reproducible reviews across broad research domains, while also evaluating pathways for computationally-efficient LCA in the context of rapidly developing AI technologies. In doing so, this work helps LCA practitioners incorporate state-of-the-art tools and timely insights into environmental assessments that can enhance the rigor and quality of sustainability-driven decisions and decision-making processes.

</details>

---

### 7. [LUMOS: Democratizing SciML Workflows with L0-Regularized Learning for Unified Feature and Parameter Adaptation](https://arxiv.org/abs/2602.22537)

**åŸºæœ¬ä¿¡æ¯**

- ğŸ”— arXiv: [`2602.22537`](https://arxiv.org/abs/2602.22537)
- ğŸ‘¥ ä½œè€…: Shouwei Gao, Xu Zheng, Dongsheng Luo ç­‰5äºº
- ğŸ“„ PDF: [ä¸‹è½½](https://arxiv.org/pdf/2602.22537.pdf)

**ğŸ’¡ ç›¸å…³æ€§åˆ†æ**

æ»¡è¶³æ ‡å‡†1ï¼šè®ºæ–‡çš„æ ¸å¿ƒç ”ç©¶å†…å®¹æ˜¯å¼€å‘ä¸€ä¸ªç”¨äºç§‘å­¦æœºå™¨å­¦ä¹ ï¼ˆSciMLï¼‰æ¨¡å‹è®¾è®¡çš„è‡ªåŠ¨åŒ–æ¡†æ¶ï¼Œå¹¶åœ¨åˆ†å­ç§‘å­¦ç­‰é¢†åŸŸçš„ä»»åŠ¡ä¸Šè¿›è¡Œè¯„ä¼°ï¼Œè¿™ç›´æ¥ä¸åˆ©ç”¨AI/æœºå™¨å­¦ä¹ æ¨¡å‹ï¼ˆå¯è§†ä¸ºâ€œåŒ–å­¦å¤§æ¨¡å‹â€çš„ä¸€ç§æ„å»ºæ–¹æ³•ï¼‰è§£å†³åŒ–å­¦ç§‘å­¦é—®é¢˜çš„ä¸»é¢˜ç›¸å…³ã€‚

**ğŸ“– ä¸­æ–‡æ‘˜è¦**

æœ¬æ–‡ä»‹ç»äº†LUMOSï¼Œä¸€ä¸ªåŸºäºL0æ­£åˆ™åŒ–å­¦ä¹ çš„ç«¯åˆ°ç«¯æ¡†æ¶ï¼Œæ—¨åœ¨æ°‘ä¸»åŒ–ç§‘å­¦æœºå™¨å­¦ä¹ ï¼ˆSciMLï¼‰æ¨¡å‹è®¾è®¡ã€‚å®ƒé€šè¿‡åŠéšæœºé—¨æ§å’Œé‡å‚æ•°åŒ–æŠ€æœ¯ï¼Œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­åŠ¨æ€é€‰æ‹©ä¿¡æ¯ç‰¹å¾å¹¶ä¿®å‰ªå†—ä½™å‚æ•°ï¼Œå‡å°‘äº†å¯¹äººå·¥è°ƒä¼˜çš„ä¾èµ–ã€‚è¯¥æ¡†æ¶åœ¨åŒ…æ‹¬åˆ†å­ç§‘å­¦åœ¨å†…çš„13ä¸ªä¸åŒçš„SciMLå·¥ä½œè´Ÿè½½ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œè¯æ˜äº†å…¶æœ‰æ•ˆæ€§å’Œé€šç”¨æ€§ã€‚å®éªŒè¡¨æ˜ï¼ŒLUMOSå¹³å‡å®ç°äº†71.45%çš„å‚æ•°å‡å°‘å’Œ6.4å€çš„æ¨ç†åŠ é€Ÿã€‚è¿™é¡¹å·¥ä½œä¸ºç§‘å­¦é¢†åŸŸï¼ˆåŒ…æ‹¬åŒ–å­¦/åˆ†å­ç§‘å­¦ï¼‰çš„æœºå™¨å­¦ä¹ æ¨¡å‹è®¾è®¡æä¾›äº†ä¸€ç§è‡ªåŠ¨åŒ–çš„ã€æ•°æ®é©±åŠ¨çš„æ–¹æ³•ã€‚

<details>
<summary><b>ğŸ” æŸ¥çœ‹åŸæ–‡æ‘˜è¦</b></summary>

The rapid growth of scientific machine learning (SciML) has accelerated discovery across diverse domains, yet designing effective SciML models remains a challenging task. In practice, building such models often requires substantial prior knowledge and manual expertise, particularly in determining which input features to use and how large the model should be. We introduce LUMOS, an end-to-end framework based on L0-regularized learning that unifies feature selection and model pruning to democratize SciML model design. By employing semi-stochastic gating and reparameterization techniques, LUMOS dynamically selects informative features and prunes redundant parameters during training, reducing the reliance on manual tuning while maintaining predictive accuracy. We evaluate LUMOS across 13 diverse SciML workloads, including cosmology and molecular sciences, and demonstrate its effectiveness and generalizability. Experiments on 13 SciML models show that LUMOS achieves 71.45% parameter reduction and a 6.4x inference speedup on average. Furthermore, Distributed Data Parallel (DDP) training on up to eight GPUs confirms the scalability of

</details>

---

### 8. [dLLM: Simple Diffusion Language Modeling](https://arxiv.org/abs/2602.22661)

**åŸºæœ¬ä¿¡æ¯**

- ğŸ”— arXiv: [`2602.22661`](https://arxiv.org/abs/2602.22661)
- ğŸ‘¥ ä½œè€…: Zhanhui Zhou, Lingjie Chen, Hanghang Tong ç­‰4äºº
- ğŸ“„ PDF: [ä¸‹è½½](https://arxiv.org/pdf/2602.22661.pdf)

**ğŸ’¡ ç›¸å…³æ€§åˆ†æ**

æ»¡è¶³æ ‡å‡†1ï¼šè®ºæ–‡çš„æ ¸å¿ƒç ”ç©¶å†…å®¹å›´ç»•åŒ–å­¦ä¿¡æ¯å­¦é¢†åŸŸçš„é‡è¦ä¸»é¢˜â€”â€”åŒ–å­¦å¤§æ¨¡å‹ï¼ˆæ‰©æ•£è¯­è¨€æ¨¡å‹æ˜¯ç”Ÿæˆå¼æ¨¡å‹çš„ä¸€ç§ï¼Œå±äºåŒ–å­¦å¤§æ¨¡å‹èŒƒç•´ï¼‰ã€‚

**ğŸ“– ä¸­æ–‡æ‘˜è¦**

æœ¬æ–‡ä»‹ç»äº†dLLMï¼Œä¸€ä¸ªç”¨äºæ‰©æ•£è¯­è¨€å»ºæ¨¡çš„ç»Ÿä¸€å¼€æºæ¡†æ¶ã€‚è¯¥æ¡†æ¶æ ‡å‡†åŒ–äº†æ‰©æ•£è¯­è¨€æ¨¡å‹ï¼ˆDLMï¼‰çš„æ ¸å¿ƒç»„ä»¶ï¼ŒåŒ…æ‹¬è®­ç»ƒã€æ¨ç†å’Œè¯„ä¼°ï¼Œä½¿å…¶æ˜“äºå®šåˆ¶æ–°è®¾è®¡ã€‚dLLMå…è®¸ç”¨æˆ·é€šè¿‡æ ‡å‡†åŒ–æµç¨‹é‡ç°ã€å¾®è°ƒã€éƒ¨ç½²å’Œè¯„ä¼°å¼€æºçš„å¤§å‹DLMï¼ˆå¦‚LLaDAå’ŒDreamï¼‰ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶è¿˜æä¾›äº†ä»é›¶å¼€å§‹æ„å»ºå°å‹DLMçš„æœ€å°åŒ–ã€å¯å¤ç°æ–¹æ¡ˆï¼ŒåŒ…æ‹¬å°†ä»»ä½•BERTé£æ ¼çš„ç¼–ç å™¨æˆ–è‡ªå›å½’è¯­è¨€æ¨¡å‹è½¬æ¢ä¸ºDLMã€‚è¿™é¡¹å·¥ä½œé€šè¿‡æä¾›ç»Ÿä¸€çš„å·¥å…·å’Œé¢„è®­ç»ƒæ£€æŸ¥ç‚¹ï¼Œæ—¨åœ¨ä½¿DLMæ›´æ˜“äºè®¿é—®ï¼Œå¹¶åŠ é€Ÿè¯¥é¢†åŸŸçš„æœªæ¥ç ”ç©¶ã€‚

<details>
<summary><b>ğŸ” æŸ¥çœ‹åŸæ–‡æ‘˜è¦</b></summary>

Although diffusion language models (DLMs) are evolving quickly, many recent models converge on a set of shared components. These components, however, are distributed across ad-hoc research codebases or lack transparent implementations, making them difficult to reproduce or extend. As the field accelerates, there is a clear need for a unified framework that standardizes these common components while remaining flexible enough to support new methods and architectures. To address this gap, we introduce dLLM, an open-source framework that unifies the core components of diffusion language modeling -- training, inference, and evaluation -- and makes them easy to customize for new designs. With dLLM, users can reproduce, finetune, deploy, and evaluate open-source large DLMs such as LLaDA and Dream through a standardized pipeline. The framework also provides minimal, reproducible recipes for building small DLMs from scratch with accessible compute, including converting any BERT-style encoder or autoregressive LM into a DLM. We also release the checkpoints of these small DLMs to make DLMs more accessible and accelerate future research.

</details>

---

### 9. [Tokenization, Fusion and Decoupling: Bridging the Granularity Mismatch Between Large Language Models and Knowledge Graphs](https://arxiv.org/abs/2602.22698)

**åŸºæœ¬ä¿¡æ¯**

- ğŸ”— arXiv: [`2602.22698`](https://arxiv.org/abs/2602.22698)
- ğŸ‘¥ ä½œè€…: Siyue Su, Jian Yang, Bo Li ç­‰4äºº
- ğŸ“„ PDF: [ä¸‹è½½](https://arxiv.org/pdf/2602.22698.pdf)

**ğŸ’¡ ç›¸å…³æ€§åˆ†æ**

æ»¡è¶³æ ‡å‡†1ï¼šè®ºæ–‡çš„æ ¸å¿ƒç ”ç©¶å†…å®¹ç›´æ¥å›´ç»•åŒ–å­¦ä¿¡æ¯å­¦é¢†åŸŸçš„å…³é”®ä¸»é¢˜â€”â€”åŒ–å­¦å¤§æ¨¡å‹ï¼ˆLLMï¼‰ä¸çŸ¥è¯†å›¾è°±ï¼ˆKGï¼‰çš„é›†æˆä¸æ¨ç†ï¼Œè¿™æ˜¯æ„å»ºåŒ–å­¦é¢†åŸŸçŸ¥è¯†å¢å¼ºå¤§æ¨¡å‹çš„é‡è¦æ–¹å‘ã€‚

**ğŸ“– ä¸­æ–‡æ‘˜è¦**

æœ¬æ–‡æå‡ºäº†KGTæ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç”¨äºçŸ¥è¯†å›¾è°±è¡¥å…¨ï¼ˆKGCï¼‰æ—¶å­˜åœ¨çš„ç²’åº¦ä¸åŒ¹é…é—®é¢˜ã€‚LLMåŸºäºç¢ç‰‡åŒ–çš„tokenåºåˆ—æ“ä½œï¼Œè€ŒçŸ¥è¯†å›¾è°±ä¸­çš„å®ä½“æ˜¯åŸºæœ¬å•å…ƒã€‚KGTé€šè¿‡å¼•å…¥ä¸“ç”¨çš„å®ä½“tokenæ¥å®ç°é«˜æ•ˆçš„å…¨ç©ºé—´é¢„æµ‹ã€‚å…·ä½“åŒ…æ‹¬ï¼š1ï¼‰ä¸“é—¨çš„tokenåŒ–æ–¹æ³•ï¼Œåœ¨ä¸“ç”¨å®ä½“tokençº§åˆ«æ„å»ºç‰¹å¾è¡¨ç¤ºï¼›2ï¼‰é€šè¿‡å…³ç³»å¼•å¯¼çš„é—¨æ§æœºåˆ¶ï¼Œå°†é¢„è®­ç»ƒçš„ç»“æ„å’Œæ–‡æœ¬ç‰¹å¾èåˆåˆ°ç»Ÿä¸€çš„åµŒå…¥ä¸­ï¼›3ï¼‰é€šè¿‡ç‹¬ç«‹çš„é¢„æµ‹å¤´å®ç°è§£è€¦é¢„æµ‹ï¼Œä»¥åˆ†ç¦»å’Œç»“åˆè¯­ä¹‰ä¸ç»“æ„æ¨ç†ã€‚å®éªŒè¡¨æ˜KGTåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­ä¼˜äºç°æœ‰æ–¹æ³•ã€‚

<details>
<summary><b>ğŸ” æŸ¥çœ‹åŸæ–‡æ‘˜è¦</b></summary>

Leveraging Large Language Models (LLMs) for Knowledge Graph Completion (KGC) is promising but hindered by a fundamental granularity mismatch. LLMs operate on fragmented token sequences, whereas entities are the fundamental units in knowledge graphs (KGs) scenarios. Existing approaches typically constrain predictions to limited candidate sets or align entities with the LLM's vocabulary by pooling multiple tokens or decomposing entities into fixed-length token sequences, which fail to capture both the semantic meaning of the text and the structural integrity of the graph. To address this, we propose KGT, a novel framework that uses dedicated entity tokens to enable efficient, full-space prediction. Specifically, we first introduce specialized tokenization to construct feature representations at the level of dedicated entity tokens. We then fuse pre-trained structural and textual features into these unified embeddings via a relation-guided gating mechanism, avoiding training from scratch. Finally, we implement decoupled prediction by leveraging independent heads to separate and combine semantic and structural reasoning. Experimental results show that KGT consistently outperforms state-of-the-art methods across multiple benchmarks.

</details>

---

### 10. [BRepMAE: Self-Supervised Masked BRep Autoencoders for Machining Feature Recognition](https://arxiv.org/abs/2602.22701)

**åŸºæœ¬ä¿¡æ¯**

- ğŸ”— arXiv: [`2602.22701`](https://arxiv.org/abs/2602.22701)
- ğŸ‘¥ ä½œè€…: Can Yao, Kang Wu, Zuheng Zheng ç­‰5äºº
- ğŸ“„ PDF: [ä¸‹è½½](https://arxiv.org/pdf/2602.22701.pdf)

**ğŸ’¡ ç›¸å…³æ€§åˆ†æ**

æ»¡è¶³æ ‡å‡†1ï¼šè®ºæ–‡çš„æ ¸å¿ƒç ”ç©¶å†…å®¹æ¶‰åŠåŒ–å­¦ä¿¡æ¯å­¦å’Œææ–™ç§‘å­¦ä¸­é‡è¦çš„åˆ†å­/ææ–™ç»“æ„è¡¨ç¤ºä¸å­¦ä¹ ï¼ˆBRepè¡¨ç¤ºå’ŒgAAGå›¾ï¼‰ï¼Œè¿™æ˜¯æ„å»ºåŒ–å­¦ç»“æ„æ¨ç†æ¨¡å‹çš„åŸºç¡€ã€‚å…¶è‡ªç›‘ç£å­¦ä¹ æ¡†æ¶ä¸åŒ–å­¦å¤§æ¨¡å‹çš„é¢„è®­ç»ƒç†å¿µé«˜åº¦ç›¸å…³ã€‚

**ğŸ“– ä¸­æ–‡æ‘˜è¦**

æœ¬æ–‡æå‡ºäº†BRepMAEï¼Œä¸€ä¸ªç”¨äºè®¡ç®—æœºè¾…åŠ©è®¾è®¡ï¼ˆCADï¼‰æ¨¡å‹åŠ å·¥ç‰¹å¾è¯†åˆ«çš„æ©ç è‡ªç›‘ç£å­¦ä¹ æ¡†æ¶ã€‚è¯¥æ¡†æ¶åœ¨å¤§å‹æ— æ ‡ç­¾CADæ¨¡å‹æ•°æ®é›†ä¸Šï¼Œä½¿ç”¨æºè‡ªè¾¹ç•Œè¡¨ç¤ºï¼ˆBRepï¼‰çš„å‡ ä½•å±æ€§é‚»æ¥å›¾ï¼ˆgAAGï¼‰è¿›è¡Œè¡¨ç¤ºå­¦ä¹ ã€‚è‡ªç›‘ç£ç½‘ç»œæ˜¯ä¸€ä¸ªæ©ç å›¾è‡ªç¼–ç å™¨ï¼ˆMAEï¼‰ï¼Œä¸“æ³¨äºé‡å»ºBRepé¢çš„å‡ ä½•å’Œå±æ€§ï¼Œè€Œéå›¾ç»“æ„ã€‚é¢„è®­ç»ƒåï¼Œæˆ‘ä»¬å¯¹ä¸€ä¸ªåŒ…å«ç¼–ç å™¨å’Œä»»åŠ¡ç‰¹å®šåˆ†ç±»ç½‘ç»œçš„ç½‘ç»œè¿›è¡Œå¾®è°ƒï¼Œç”¨äºåŠ å·¥ç‰¹å¾è¯†åˆ«ï¼ˆMFRï¼‰ã€‚å®éªŒè¡¨æ˜ï¼Œå¾®è°ƒåçš„ç½‘ç»œä»…éœ€å°‘é‡æ•°æ®ï¼ˆä¾‹å¦‚0.1%çš„è®­ç»ƒæ•°æ®ï¼‰å³å¯å®ç°é«˜è¯†åˆ«ç‡ï¼Œæ˜¾è‘—æå‡äº†åœ¨ç°å®ï¼ˆæˆ–ç§æœ‰ï¼‰åœºæ™¯ä¸­æ•°æ®æœ‰é™æ—¶çš„å®ç”¨æ€§ã€‚

<details>
<summary><b>ğŸ” æŸ¥çœ‹åŸæ–‡æ‘˜è¦</b></summary>

We propose a masked self-supervised learning framework, called BRepMAE, for automatically extracting a valuable representation of the input computer-aided design (CAD) model to recognize its machining features. Representation learning is conducted on a large-scale, unlabeled CAD model dataset using the geometric Attributed Adjacency Graph (gAAG) representation, derived from the boundary representation (BRep). The self-supervised network is a masked graph autoencoder (MAE) that focuses on reconstructing geometries and attributes of BRep facets, rather than graph structures. After pre-training, we fine-tune a network that contains both the encoder and a task-specific classification network for machining feature recognition (MFR). In the experiments, our fine-tuned network achieves high recognition rates with only a small amount of data (e.g., 0.1% of the training data), significantly enhancing its practicality in real-world (or private) scenarios where only limited data is available. Compared with other MFR methods, our fine-tuned network achieves a significant improvement in recognition rate with the same amount of training data, especially when the number of training samples is limited.

</details>

---

### 11. [IRSDE-Despeckle: A Physics-Grounded Diffusion Model for Generalizable Ultrasound Despeckling](https://arxiv.org/abs/2602.22717)

**åŸºæœ¬ä¿¡æ¯**

- ğŸ”— arXiv: [`2602.22717`](https://arxiv.org/abs/2602.22717)
- ğŸ‘¥ ä½œè€…: Shuoqi Chen, Yujia Wu, Geoffrey P. Luke
- ğŸ“„ PDF: [ä¸‹è½½](https://arxiv.org/pdf/2602.22717.pdf)

**ğŸ’¡ ç›¸å…³æ€§åˆ†æ**

æ»¡è¶³æ ‡å‡†1ï¼šè®ºæ–‡çš„æ ¸å¿ƒç ”ç©¶å†…å®¹ç›´æ¥å›´ç»•è´¨è°±åˆ†æé¢†åŸŸçš„æ ¸å¿ƒæŒ‘æˆ˜â€”â€”ä¿¡å·å¤„ç†ä¸å»å™ªã€‚è™½ç„¶åº”ç”¨åœ¨åŒ»å­¦è¶…å£°æˆåƒï¼Œä½†å…¶åŸºäºæ‰©æ•£æ¨¡å‹çš„å»å™ªæ¡†æ¶ã€ä¿¡å·æ¨¡æ‹Ÿã€ä¸ç¡®å®šæ€§é‡åŒ–ç­‰æ–¹æ³•è®ºä¸è´¨è°±æ•°æ®çš„é¢„å¤„ç†ã€è°±å›¾å»å·ç§¯å’Œå™ªå£°æŠ‘åˆ¶é«˜åº¦ç›¸å…³ï¼Œå¯è§†ä¸ºè´¨è°±ç»“æ„æ¨ç†ä¸­ä¿¡å·å¢å¼ºçš„å…³é”®æŠ€æœ¯ã€‚

**ğŸ“– ä¸­æ–‡æ‘˜è¦**

æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºå›¾åƒæ¢å¤éšæœºå¾®åˆ†æ–¹ç¨‹ï¼ˆIR-SDEï¼‰æ¡†æ¶çš„æ‰©æ•£æ¨¡å‹è¶…å£°å»å™ªæ–¹æ³•ã€‚ä¸ºäº†è¿›è¡Œç›‘ç£è®­ç»ƒï¼Œæˆ‘ä»¬é€šè¿‡ä½¿ç”¨Matlabè¶…å£°å·¥å…·ç®±ä»æ— æ–‘ç‚¹çš„ç£å…±æŒ¯å›¾åƒæ¨¡æ‹Ÿè¶…å£°å›¾åƒï¼Œæ„å»ºäº†å¤§å‹é…å¯¹æ•°æ®é›†ã€‚æ‰€æå‡ºçš„æ¨¡å‹åœ¨æŠ‘åˆ¶æ–‘ç‚¹å™ªå£°çš„åŒæ—¶ï¼Œä¿ç•™äº†å…·æœ‰è§£å‰–å­¦æ„ä¹‰çš„è¾¹ç¼˜å’Œå¯¹æ¯”åº¦ã€‚åœ¨ä¿ç•™çš„æ¨¡æ‹Ÿæµ‹è¯•é›†ä¸Šï¼Œæˆ‘ä»¬çš„æ–¹æ³• consistently outperforms ç»å…¸æ»¤æ³¢å™¨å’Œè¿‘æœŸåŸºäºå­¦ä¹ çš„å»å™ªåŸºçº¿ã€‚æˆ‘ä»¬é€šè¿‡è·¨æ¨¡å‹æ–¹å·®é‡åŒ–é¢„æµ‹ä¸ç¡®å®šæ€§ï¼Œå¹¶è¡¨æ˜æ›´é«˜çš„ä¸ç¡®å®šæ€§ä¸æ›´é«˜çš„é‡å»ºè¯¯å·®ç›¸å…³ï¼Œä¸ºå›°éš¾æˆ–æ˜“å¤±è´¥åŒºåŸŸæä¾›äº†å®ç”¨çš„æŒ‡ç¤ºå™¨ã€‚æœ€åï¼Œæˆ‘ä»¬è¯„ä¼°äº†å¯¹æ¨¡æ‹Ÿæ¢å¤´è®¾ç½®çš„æ•æ„Ÿæ€§ï¼Œå¹¶è§‚å¯Ÿåˆ°é¢†åŸŸåç§»ï¼Œè¿™ä¿ƒä½¿äº†å¤šæ ·åŒ–çš„è®­ç»ƒå’Œé€‚åº”ä»¥å®ç°ç¨³å¥çš„ä¸´åºŠéƒ¨ç½²ã€‚

<details>
<summary><b>ğŸ” æŸ¥çœ‹åŸæ–‡æ‘˜è¦</b></summary>

Ultrasound imaging is widely used for real-time, noninvasive diagnosis, but speckle and related artifacts reduce image quality and can hinder interpretation. We present a diffusion-based ultrasound despeckling method built on the Image Restoration Stochastic Differential Equations framework. To enable supervised training, we curate large paired datasets by simulating ultrasound images from speckle-free magnetic resonance images using the Matlab UltraSound Toolbox. The proposed model reconstructs speckle-suppressed images while preserving anatomically meaningful edges and contrast. On a held-out simulated test set, our approach consistently outperforms classical filters and recent learning-based despeckling baselines. We quantify prediction uncertainty via cross-model variance and show that higher uncertainty correlates with higher reconstruction error, providing a practical indicator of difficult or failure-prone regions. Finally, we evaluate sensitivity to simulation probe settings and observe domain shift, motivating diversified training and adaptation for robust clinical deployment.

</details>

---

### 12. [Sapling-NeRF: Geo-Localised Sapling Reconstruction in Forests for Ecological Monitoring](https://arxiv.org/abs/2602.22731)

**åŸºæœ¬ä¿¡æ¯**

- ğŸ”— arXiv: [`2602.22731`](https://arxiv.org/abs/2602.22731)
- ğŸ‘¥ ä½œè€…: Miguel Ãngel MuÃ±oz-BaÃ±Ã³n, Nived Chebrolu, Sruthi M. Krishna Moorthy ç­‰7äºº
- ğŸ“„ PDF: [ä¸‹è½½](https://arxiv.org/pdf/2602.22731.pdf)

**ğŸ’¡ ç›¸å…³æ€§åˆ†æ**

æ»¡è¶³æ ‡å‡†2ï¼šè®ºæ–‡æå‡ºçš„å¤šæ¨¡æ€ï¼ˆNeRFï¼Œ LiDARï¼Œ GNSSï¼‰èåˆä¸‰ç»´é‡å»ºä¸è¡¨å¾ç®¡é“ï¼Œä¸ºåŒ–å­¦ä¿¡æ¯å­¦ä¸­åˆ†å­ä¸‰ç»´æ„è±¡åˆ†æã€ææ–™å¾®è§‚ç»“æ„è¡¨å¾ä»¥åŠè´¨è°±æˆåƒï¼ˆMSIï¼‰æ•°æ®çš„ç©ºé—´è§£ææä¾›äº†å¼ºå¤§çš„æ•°æ®è·å–ä¸å¤„ç†å·¥å…·å’Œæ¡†æ¶ã€‚

**ğŸ“– ä¸­æ–‡æ‘˜è¦**

æœ¬æ–‡æå‡ºäº†ä¸€ç§èåˆç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰ã€æ¿€å…‰é›·è¾¾SLAMå’ŒGNSSçš„ç®¡é“ï¼Œä»¥å®ç°å¯¹æ£®æ—ä¸­æ ‘è‹—çš„å¯é‡å¤ã€åœ°ç†å®šä½çš„ç”Ÿæ€ç›‘æµ‹ã€‚æˆ‘ä»¬çš„ç³»ç»Ÿæå‡ºäº†ä¸‰çº§è¡¨ç¤ºï¼š1ï¼‰ä½¿ç”¨GNSSè¿›è¡Œç²—ç•¥çš„åœ°çƒåæ ‡ç³»å®šä½ï¼›2ï¼‰ä½¿ç”¨åŸºäºæ¿€å…‰é›·è¾¾çš„SLAMè¿›è¡Œå˜ç±³çº§ç²¾åº¦çš„å®šä½å’Œé‡å»ºï¼›3ï¼‰ä½¿ç”¨NeRFè¡ç”Ÿçš„ä»¥å¯¹è±¡ä¸ºä¸­å¿ƒçš„å¯†é›†é‡å»ºæ¥é‡å»ºå•ä¸ªæ ‘è‹—ã€‚è¯¥æ–¹æ³•èƒ½å¤Ÿå¯¹æ ‘è‹—æ€§çŠ¶è¿›è¡Œå¯é‡å¤çš„å®šé‡è¯„ä¼°å’Œé•¿æœŸç›‘æµ‹ã€‚æˆ‘ä»¬åœ¨è‹±å›½ç‰›æ´¥Wytham Woodså’ŒèŠ¬å…°Evoçš„æ£®æ—æ ·åœ°ä¸­çš„å®éªŒè¡¨æ˜ï¼Œä¸åœ°é¢æ¿€å…‰æ‰«æï¼ˆTLSï¼‰ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•å¯ä»¥æ›´å‡†ç¡®åœ°æ•è·èŒå¹²é«˜åº¦ã€åˆ†ææ¨¡å¼å’Œå¶æœ¨æ¯”ã€‚æˆ‘ä»¬è¯æ˜ï¼Œå¯ä»¥åŸä½æµ‹é‡é«˜åº¦åœ¨0.5ç±³åˆ°2ç±³ä¹‹é—´çš„æ ‘è‹—çš„å‡†ç¡®èŒå¹²éª¨æ¶å’Œå¶ç‰‡åˆ†å¸ƒï¼Œä¸ºç”Ÿæ€å­¦å®¶æä¾›äº†æ›´ä¸°å¯Œçš„ç»“æ„å’Œå®šé‡æ•°æ®æ¥åˆ†ææ£®æ—åŠ¨æ€ã€‚

<details>
<summary><b>ğŸ” æŸ¥çœ‹åŸæ–‡æ‘˜è¦</b></summary>

Saplings are key indicators of forest regeneration and overall forest health. However, their fine-scale architectural traits are difficult to capture with existing 3D sensing methods, which make quantitative evaluation difficult. Terrestrial Laser Scanners (TLS), Mobile Laser Scanners (MLS), or traditional photogrammetry approaches poorly reconstruct thin branches, dense foliage, and lack the scale consistency needed for long-term monitoring. Implicit 3D reconstruction methods such as Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS) are promising alternatives, but cannot recover the true scale of a scene and lack any means to be accurately geo-localised. In this paper, we present a pipeline which fuses NeRF, LiDAR SLAM, and GNSS to enable repeatable, geo-localised ecological monitoring of saplings. Our system proposes a three-level representation: (i) coarse Earth-frame localisation using GNSS, (ii) LiDAR-based SLAM for centimetre-accurate localisation and reconstruction, and (iii) NeRF-derived object-centric dense reconstruction of individual saplings. This approach enables repeatable quantitative evaluation and long-term monitoring of sapling traits. Our experiments in forest plots in Wytham Woods (Oxford, UK) and Evo (Finland) show that stem height, branching patterns, and leaf-to-wood ratios can be captured with increased accuracy as compared to TLS. We demonstrate that accurate stem skeletons and leaf distributions can be measured for saplings with heights between 0.5m and 2m in situ, giving ecologists access to richer structural and quantitative data for analysing forest dynamics.

</details>

---

### 13. [Molecule Mixture Detection and Design for MC Systems with Non-linear, Cross-reactive Receiver Arrays](https://arxiv.org/abs/2602.22799)

**åŸºæœ¬ä¿¡æ¯**

- ğŸ”— arXiv: [`2602.22799`](https://arxiv.org/abs/2602.22799)
- ğŸ‘¥ ä½œè€…: Bastian Heinlein, Kaikai Zhu, SÃ¼meyye Carkit-Yilmaz ç­‰9äºº
- ğŸ“„ PDF: [ä¸‹è½½](https://arxiv.org/pdf/2602.22799.pdf)

**ğŸ’¡ ç›¸å…³æ€§åˆ†æ**

æ»¡è¶³æ ‡å‡†1ï¼šè®ºæ–‡æ ¸å¿ƒç ”ç©¶å†…å®¹å›´ç»•åˆ†å­æ··åˆç‰©çš„æ£€æµ‹ä¸æ¨ç†ï¼Œä½¿ç”¨éçº¿æ€§å’Œäº¤å‰ååº”ä¼ æ„Ÿå™¨é˜µåˆ—ï¼Œè¿™ä¸è´¨è°±åˆ†æä¸­ä»å¤æ‚è°±å›¾è¿›è¡Œç»“æ„æ¨ç†çš„æ ¸å¿ƒé—®é¢˜é«˜åº¦ç›¸å…³ã€‚

**ğŸ“– ä¸­æ–‡æ‘˜è¦**

æœ¬æ–‡ç ”ç©¶ç©ºæ°”åˆ†å­é€šä¿¡ï¼ˆMCï¼‰ç³»ç»Ÿï¼Œé‡ç‚¹å…³æ³¨åˆ†å­æ··åˆç‰©æ£€æµ‹ä¸è®¾è®¡ã€‚ç³»ç»Ÿä½¿ç”¨éçº¿æ€§å’Œäº¤å‰ååº”ä¼ æ„Ÿå™¨ä½œä¸ºæ¥æ”¶å™¨ï¼Œè¿™ä¸è´¨è°±åˆ†æä¸­ä¼ æ„Ÿå™¨å¯¹å¤æ‚æ··åˆç‰©ï¼ˆå¦‚ä»£è°¢ç‰©æˆ–ç¯å¢ƒæ ·æœ¬ï¼‰çš„å“åº”ç‰¹æ€§é«˜åº¦ç›¸å…³ã€‚è®ºæ–‡æå‡ºäº†å‡ ç§æ£€æµ‹å™¨å’Œä¼ è¾“æ–¹æ¡ˆï¼ŒåŒ…æ‹¬è¿‘ä¼¼æœ€å¤§ä¼¼ç„¶ï¼ˆAMLï¼‰ç¬¦å·æ£€æµ‹å™¨å’Œäº’è¡¥æ··åˆç‰©å­—æ¯è¡¨è®¾è®¡ç®—æ³•ï¼Œè¿™äº›æ–¹æ¡ˆè€ƒè™‘äº†æ¥æ”¶å™¨çš„éçº¿æ€§ç‰¹æ€§ã€‚ç ”ç©¶ä½¿ç”¨å•†ä¸šå¯ç”¨ä¼ æ„Ÿå™¨ï¼ˆåŒ…æ‹¬é‡‘å±æ°§åŒ–ç‰©åŠå¯¼ä½“ä¼ æ„Ÿå™¨ï¼‰çš„å“åº”ä»¥åŠäººå·¥ç”Ÿæˆçš„ä¼ æ„Ÿå™¨æ•°æ®è¿›è¡ŒéªŒè¯ã€‚è¿™é¡¹å·¥ä½œä¸ºå¤„ç†éç†æƒ³ã€äº¤å‰ååº”ä¼ æ„Ÿå™¨çš„åˆ†å­é€šä¿¡ç³»ç»Ÿæä¾›äº†é€šç”¨æ¡†æ¶ï¼Œå…¶æ ¸å¿ƒé—®é¢˜â€”â€”ä»éçº¿æ€§ä¼ æ„Ÿå™¨ä¿¡å·ä¸­æ¨æ–­æ··åˆç‰©æˆåˆ†â€”â€”ä¸è´¨è°±åˆ†æä¸­ä»å¤æ‚è´¨è°±å›¾ä¸­è§£æåŒ–å­¦ç»“æ„çš„æ ¸å¿ƒæŒ‘æˆ˜ç›´æ¥å¯¹åº”ã€‚

<details>
<summary><b>ğŸ” æŸ¥çœ‹åŸæ–‡æ‘˜è¦</b></summary>

Air-based molecular communication (MC) has the potential to be one of the first MC systems to be deployed in real-world applications, enabled by commercially available sensors. However, these sensors usually exhibit non-linear and cross-reactive behavior, contrary to the idealizing assumption of linear and perfectly molecule type-specific sensing often made in the MC literature. To address this mismatch, we propose several detectors and transmission schemes for a molecule mixture communication system where the receiver (RX) employs non-linear, cross-reactive sensors. All proposed schemes are based on the first- and second-order moments of the symbol likelihoods that are fed through the non-linear RX using the Unscented Transform. In particular, we propose an approximate maximum likelihood (AML) symbol-by-symbol detector for inter-symbol-interference (ISI)-free transmission scenarios and a complementary mixture alphabet design algorithm which accounts for the RX characteristics. When significant ISI is present at high data rates, the AML detector can be adapted to exploit statistical ISI knowledge. Additionally, we propose a sequence detector which combines information from multiple symbol intervals. For settings where sequence detection is not possible due to extremely limited computational power at the RX, we propose an adaptive transmission scheme which can be combined with symbol-by-symbol detection. Using computer simulations, we validate all proposed detectors and algorithms based on the responses of commercially available sensors as well as artificially generated sensor data incorporating the characteristics of metal-oxide semiconductor sensors. By employing a general system model that accounts for transmitter noise, ISI, and general non-linear, cross-reactive RX arrays, this work enables reliable communication for a large class of MC systems.

</details>

---

### 14. [FlexMS is a flexible framework for benchmarking deep learning-based mass spectrum prediction tools in metabolomics](https://arxiv.org/abs/2602.22822)

**åŸºæœ¬ä¿¡æ¯**

- ğŸ”— arXiv: [`2602.22822`](https://arxiv.org/abs/2602.22822)
- ğŸ‘¥ ä½œè€…: Yunhua Zhong, Yixuan Tang, Yifan Li ç­‰6äºº
- ğŸ“„ PDF: [ä¸‹è½½](https://arxiv.org/pdf/2602.22822.pdf)

**ğŸ’¡ ç›¸å…³æ€§åˆ†æ**

æ»¡è¶³æ ‡å‡†2ï¼šè®ºæ–‡æå‡ºäº†ä¸€ä¸ªåä¸ºFlexMSçš„åŸºå‡†æµ‹è¯•æ¡†æ¶ï¼Œä¸“é—¨ç”¨äºæ„å»ºå’Œè¯„ä¼°è´¨è°±é¢„æµ‹æ¨¡å‹ï¼Œè¿™ä¸ºâ€œè´¨è°±ç»“æ„æ¨ç†â€ä¸»é¢˜æä¾›äº†é‡è¦çš„å·¥å…·å’Œè¯„ä¼°èµ„æºã€‚åŒæ—¶ï¼Œå…¶æ ¸å¿ƒç ”ç©¶å†…å®¹ä¹Ÿç›´æ¥å›´ç»•è´¨è°±é¢„æµ‹ï¼Œæ»¡è¶³æ ‡å‡†1ã€‚

**ğŸ“– ä¸­æ–‡æ‘˜è¦**

æœ¬æ–‡ä»‹ç»äº†FlexMSï¼Œä¸€ä¸ªç”¨äºåœ¨ä»£è°¢ç»„å­¦ä¸­åŸºå‡†æµ‹è¯•æ·±åº¦å­¦ä¹ è´¨è°±é¢„æµ‹å·¥å…·çš„çµæ´»æ¡†æ¶ã€‚è´¨è°±æŠ€æœ¯é€šè¿‡è´¨è·æ¯”å³°ä¸ºåŒ–å­¦åˆ†å­çš„é‰´å®šå’Œæ€§è´¨é¢„æµ‹æä¾›äº†å…³é”®ä¿¡æ¯ã€‚ç„¶è€Œï¼Œå®éªŒè°±å›¾çš„ç¼ºä¹é˜»ç¢äº†åˆ†å­é‰´å®šï¼Œå› æ­¤è¿«åˆ‡éœ€è¦å»ºç«‹è®¡ç®—æ¨¡å‹æ¥é¢„æµ‹åˆ†å­ç»“æ„è°±å›¾ã€‚æ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨æ­¤ä»»åŠ¡ä¸Šè¡¨ç°å‡ºæ½œåŠ›ï¼Œä½†ç”±äºæ–¹æ³•å¼‚è´¨æ€§å’Œç¼ºä¹æ˜ç¡®å®šä¹‰çš„åŸºå‡†ï¼Œæ•´ä½“è¯„ä¼°ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚FlexMSæ—¨åœ¨è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæ”¯æŒåŠ¨æ€æ„å»ºå¤šç§ä¸åŒçš„æ¨¡å‹æ¶æ„ç»„åˆï¼Œå¹¶åœ¨é¢„å¤„ç†è¿‡çš„å…¬å…±æ•°æ®é›†ä¸Šä½¿ç”¨ä¸åŒæŒ‡æ ‡è¯„ä¼°å…¶æ€§èƒ½ã€‚è®ºæ–‡æ·±å…¥æ¢è®¨äº†å½±å“æ€§èƒ½çš„å› ç´ ï¼ŒåŒ…æ‹¬æ•°æ®é›†çš„ç»“æ„å¤šæ ·æ€§ã€å­¦ä¹ ç‡ç­‰è¶…å‚æ•°ã€æ•°æ®ç¨€ç–æ€§ã€é¢„è®­ç»ƒæ•ˆæœã€å…ƒæ•°æ®æ¶ˆèè®¾ç½®ä»¥åŠè·¨é¢†åŸŸè¿ç§»å­¦ä¹ åˆ†æã€‚æ­¤å¤–ï¼Œæ£€ç´¢åŸºå‡†æ¨¡æ‹Ÿäº†å®é™…çš„é‰´å®šåœºæ™¯ï¼Œæ ¹æ®é¢„æµ‹è°±å›¾å¯¹æ½œåœ¨åŒ¹é…è¿›è¡Œè¯„åˆ†ã€‚è¯¥æ¡†æ¶ä¸ºå¼€å‘å’Œè¯„ä¼°ç”¨äºè´¨è°±é¢„æµ‹çš„æ·±åº¦å­¦ä¹ æ¨¡å‹æä¾›äº†æ ‡å‡†åŒ–å·¥å…·å’Œæ·±å…¥è§è§£ã€‚

<details>
<summary><b>ğŸ” æŸ¥çœ‹åŸæ–‡æ‘˜è¦</b></summary>

The identification and property prediction of chemical molecules is of central importance in the advancement of drug discovery and material science, where the tandem mass spectrometry technology gives valuable fragmentation cues in the form of mass-to-charge ratio peaks. However, the lack of experimental spectra hinders the attachment of each molecular identification, and thus urges the establishment of prediction approaches for computational models. Deep learning models appear promising for predicting molecular structure spectra, but overall assessment remains challenging as a result of the heterogeneity in methods and the lack of well-defined benchmarks. To address this, our contribution is the creation of benchmark framework FlexMS for constructing and evaluating diverse model architectures in mass spectrum prediction. With its easy-to-use flexibility, FlexMS supports the dynamic construction of numerous distinct combinations of model architectures, while assessing their performance on preprocessed public datasets using different metrics. In this paper, we provide insights into factors influencing performance, including the structural diversity of datasets, hyperparameters like learning rate and data sparsity, pretraining effects, metadata ablation settings and cross-domain transfer learning analysis. This provides practical guidance in choosing suitable models. Moreover, retrieval benchmarks simulate practical identification scenarios and score potential matches based on predicted spectra.

</details>

---

### 15. [MM-NeuroOnco: A Multimodal Benchmark and Instruction Dataset for MRI-Based Brain Tumor Diagnosis](https://arxiv.org/abs/2602.22955)

**åŸºæœ¬ä¿¡æ¯**

- ğŸ”— arXiv: [`2602.22955`](https://arxiv.org/abs/2602.22955)
- ğŸ‘¥ ä½œè€…: Feng Guo, Jiaxiang Liu, Yang Li ç­‰5äºº
- ğŸ“„ PDF: [ä¸‹è½½](https://arxiv.org/pdf/2602.22955.pdf)

**ğŸ’¡ ç›¸å…³æ€§åˆ†æ**

æ»¡è¶³æ ‡å‡†2ï¼šè®ºæ–‡è¯¦ç»†æè¿°äº†ä¸€ä¸ªå¤§è§„æ¨¡ã€è¯­ä¹‰ä¸°å¯Œçš„å¤šæ¨¡æ€æ•°æ®é›†ï¼ˆMM-NeuroOncoï¼‰å’Œè¯„ä¼°åŸºå‡†ï¼ˆMM-NeuroOnco-Benchï¼‰çš„æ„å»ºæ–¹æ³•è®ºã€‚è¿™ç§ä¸ºå¤æ‚AIä»»åŠ¡åˆ›å»ºé«˜è´¨é‡æ•°æ®èµ„æºçš„æ–¹æ³•ï¼Œå¯ç›´æ¥å€Ÿé‰´ç”¨äºä¸ºâ€œåŒ–å­¦å¤§æ¨¡å‹â€å’Œâ€œè´¨è°±ç»“æ„æ¨ç†â€ä¸»é¢˜æ„å»ºæ•°æ®é›†ã€‚

**ğŸ“– ä¸­æ–‡æ‘˜è¦**

æœ¬æ–‡ä»‹ç»äº†MM-NeuroOncoï¼Œä¸€ä¸ªç”¨äºåŸºäºMRIçš„è„‘è‚¿ç˜¤è¯Šæ–­çš„å¤§è§„æ¨¡å¤šæ¨¡æ€åŸºå‡†å’ŒæŒ‡ä»¤è°ƒä¼˜æ•°æ®é›†ã€‚å°½ç®¡è¯¥è®ºæ–‡ä¸»è¦å…³æ³¨åŒ»å­¦å½±åƒï¼ˆMRIï¼‰ï¼Œä½†å…¶æ ¸å¿ƒæ–¹æ³•è®ºâ€”â€”æ„å»ºä¸€ä¸ªåŒ…å«ä¸°å¯Œè¯Šæ–­è¯­ä¹‰æ³¨é‡Šçš„å¤§è§„æ¨¡å¤šæ¨¡æ€æ•°æ®é›†ï¼Œå¹¶åˆ©ç”¨å¤šæ¨¡å‹åä½œç®¡é“è‡ªåŠ¨å®ŒæˆåŒ»å­¦ä¿¡æ¯â€”â€”å±•ç¤ºäº†å¦‚ä½•ä¸ºå¤æ‚æ¨ç†ä»»åŠ¡ï¼ˆå¦‚è¯Šæ–­ï¼‰åˆ›å»ºé«˜è´¨é‡çš„æ•°æ®èµ„æºã€‚è¿™ç§æ„å»ºå¯Œå«è¯­ä¹‰ã€å¯ç”¨äºè®­ç»ƒå’Œè¯„ä¼°AIæ¨¡å‹çš„æ•°æ®é›†çš„æ–¹æ³•è®ºï¼Œä¸åŒ–å­¦ä¿¡æ¯å­¦å’Œè´¨è°±åˆ†æé¢†åŸŸåˆ›å»ºç”¨äºâ€œåŒ–å­¦å¤§æ¨¡å‹â€æˆ–â€œè´¨è°±ç»“æ„æ¨ç†â€çš„æ ‡æ³¨æ•°æ®é›†çš„åŠªåŠ›æ˜¯å¹³è¡Œçš„ã€‚è®ºæ–‡ä¸­å¼€å‘çš„æ•°æ®é›†æ„å»ºç®¡é“ï¼ˆè‡ªåŠ¨åŒ–ä¿¡æ¯è¡¥å…¨å’Œè´¨é‡æ§åˆ¶ï¼‰ä»¥åŠè¯„ä¼°åŸºå‡†ï¼ˆMM-NeuroOnco-Benchï¼‰çš„è®¾è®¡æ€è·¯ï¼Œå¯ä¸ºåŒ–å­¦é¢†åŸŸç±»ä¼¼æ•°æ®èµ„æºçš„åˆ›å»ºæä¾›å‚è€ƒã€‚

<details>
<summary><b>ğŸ” æŸ¥çœ‹åŸæ–‡æ‘˜è¦</b></summary>

Accurate brain tumor diagnosis requires models to not only detect lesions but also generate clinically interpretable reasoning grounded in imaging manifestations, yet existing public datasets remain limited in annotation richness and diagnostic semantics. To bridge this gap, we introduce MM-NeuroOnco, a large-scale multimodal benchmark and instruction-tuning dataset for brain tumor MRI understanding, consisting of 24,726 MRI slices from 20 data sources paired with approximately 200,000 semantically enriched multimodal instructions spanning diverse tumor subtypes and imaging modalities. To mitigate the scarcity and high cost of diagnostic semantic annotations, we develop a multi-model collaborative pipeline for automated medical information completion and quality control, enabling the generation of diagnosis-related semantics beyond mask-only annotations. Building upon this dataset, we further construct MM-NeuroOnco-Bench, a manually annotated evaluation benchmark with a rejection-aware setting to reduce biases inherent in closed-ended question formats. Evaluation across ten representative models shows that even the strongest baseline, Gemini 3 Flash, achieves only 41.88% accuracy on diagnosis-related questions, highlighting the substantial challenges of multimodal brain tumor diagnostic understanding. Leveraging MM-NeuroOnco, we further propose NeuroOnco-GPT, which achieves a 27% absolute accuracy improvement on diagnostic questions following fine-tuning. This result demonstrates the effectiveness of our dataset and benchmark in advancing clinically grounded multimodal diagnostic reasoning. Code and dataset are publicly available at: this https URL

</details>

---

### 16. [SPM-Bench: Benchmarking Large Language Models for Scanning Probe Microscopy](https://arxiv.org/abs/2602.22971)

**åŸºæœ¬ä¿¡æ¯**

- ğŸ”— arXiv: [`2602.22971`](https://arxiv.org/abs/2602.22971)
- ğŸ‘¥ ä½œè€…: Peiyao Xiao, Xiaogang Li, Chengliang Xu ç­‰13äºº
- ğŸ“„ PDF: [ä¸‹è½½](https://arxiv.org/pdf/2602.22971.pdf)

**ğŸ’¡ ç›¸å…³æ€§åˆ†æ**

æ»¡è¶³æ ‡å‡†2ï¼šè®ºæ–‡æå‡ºäº†ä¸€ç§è‡ªåŠ¨åŒ–æ•°æ®åˆæˆæµæ°´çº¿å’Œæ–¹æ³•ï¼ˆAGSæŠ€æœ¯ã€æ··åˆæ¶æ„ï¼‰ï¼Œç”¨äºä»ç§‘å­¦æ–‡çŒ®ä¸­é«˜æ•ˆç”Ÿæˆé«˜è´¨é‡ã€é¢†åŸŸç‰¹å®šçš„å¤šæ¨¡æ€æ•°æ®é›†ã€‚è¿™ç§æ„å»ºæ•°æ®é›†å’Œèµ„æºçš„æ–¹æ³•è®ºï¼Œå¯ç›´æ¥åº”ç”¨äºä¸ºâ€œåŒ–å­¦å¤§æ¨¡å‹â€åˆ›å»ºè®­ç»ƒå’Œè¯„ä¼°æ•°æ®ã€‚

**ğŸ“– ä¸­æ–‡æ‘˜è¦**

æœ¬æ–‡ä»‹ç»äº†SPM-Benchï¼Œä¸€ä¸ªä¸“é—¨ä¸ºæ‰«ææ¢é’ˆæ˜¾å¾®é•œï¼ˆSPMï¼‰è®¾è®¡çš„ã€åšå£«çº§åˆ«çš„å¤šæ¨¡æ€åŸºå‡†æµ‹è¯•ã€‚è®ºæ–‡çš„æ ¸å¿ƒè´¡çŒ®åœ¨äºæå‡ºäº†ä¸€ç§å…¨è‡ªåŠ¨çš„æ•°æ®åˆæˆæµæ°´çº¿ï¼Œç”¨äºé«˜æ•ˆåœ°ä»arXivå’ŒæœŸåˆŠè®ºæ–‡ä¸­æå–é«˜è´¨é‡çš„å›¾åƒ-æ–‡æœ¬å¯¹ã€‚è¯¥æµæ°´çº¿é‡‡ç”¨Anchor-Gated Sieve (AGS)æŠ€æœ¯å’Œæ··åˆäº‘-æœ¬åœ°æ¶æ„ï¼Œåœ¨ä¿æŒæ•°æ®é›†é«˜çº¯åº¦çš„åŒæ—¶å®ç°äº†æè‡´çš„tokenèŠ‚çœã€‚è™½ç„¶è®ºæ–‡ä¸»é¢˜æ˜¯SPMï¼Œä½†å…¶æå‡ºçš„è‡ªåŠ¨åŒ–ç§‘å­¦æ•°æ®åˆæˆèŒƒå¼ã€ä»ç§‘å­¦æ–‡çŒ®ä¸­é«˜æ•ˆæå–ç»“æ„åŒ–å¤šæ¨¡æ€æ•°æ®çš„æ–¹æ³•ï¼Œä»¥åŠæ„å»ºé¢†åŸŸç‰¹å®šåŸºå‡†æµ‹è¯•çš„æ¡†æ¶ï¼Œä¸â€œåŒ–å­¦å¤§æ¨¡å‹â€ä¸»é¢˜é«˜åº¦ç›¸å…³ã€‚è¯¥æ–¹æ³•å±•ç¤ºäº†å¦‚ä½•ä¸ºç‰¹å®šç§‘å­¦é¢†åŸŸï¼ˆå¦‚åŒ–å­¦ï¼‰æ„å»ºé«˜è´¨é‡ã€ä½æˆæœ¬çš„è®­ç»ƒå’Œè¯„ä¼°æ•°æ®èµ„æºï¼Œè¿™å¯¹äºè®­ç»ƒé¢†åŸŸä¸“ç”¨çš„åŒ–å­¦å¤§æ¨¡å‹è‡³å…³é‡è¦ã€‚

<details>
<summary><b>ğŸ” æŸ¥çœ‹åŸæ–‡æ‘˜è¦</b></summary>

As LLMs achieved breakthroughs in general reasoning, their proficiency in specialized scientific domains reveals pronounced gaps in existing benchmarks due to data contamination, insufficient complexity, and prohibitive human labor costs. Here we present SPM-Bench, an original, PhD-level multimodal benchmark specifically designed for scanning probe microscopy (SPM). We propose a fully automated data synthesis pipeline that ensures both high authority and low-cost. By employing Anchor-Gated Sieve (AGS) technology, we efficiently extract high-value image-text pairs from arXiv and journal papers published between 2023 and 2025. Through a hybrid cloud-local architecture where VLMs return only spatial coordinates "llbox" for local high-fidelity cropping, our pipeline achieves extreme token savings while maintaining high dataset purity. To accurately and objectively evaluate the performance of the LLMs, we introduce the Strict Imperfection Penalty F1 (SIP-F1) score. This metric not only establishes a rigorous capability hierarchy but also, for the first time, quantifies model "personalities" (Conservative, Aggressive, Gambler, or Wise). By correlating these results with model-reported confidence and perceived difficulty, we expose the true reasoning boundaries of current AI in complex physical scenarios. These insights establish SPM-Bench as a generalizable paradigm for automated scientific data synthesis.

</details>

---

### 17. [Induction Meets Biology: Mechanisms of Repeat Detection in Protein Language Models](https://arxiv.org/abs/2602.23179)

**åŸºæœ¬ä¿¡æ¯**

- ğŸ”— arXiv: [`2602.23179`](https://arxiv.org/abs/2602.23179)
- ğŸ‘¥ ä½œè€…: Gal Kesten-Pomeranz, Yaniv Nikankin, Anja Reusch ç­‰6äºº
- ğŸ“„ PDF: [ä¸‹è½½](https://arxiv.org/pdf/2602.23179.pdf)

**ğŸ’¡ ç›¸å…³æ€§åˆ†æ**

æ»¡è¶³æ ‡å‡†1ï¼šè®ºæ–‡çš„æ ¸å¿ƒç ”ç©¶å†…å®¹ç›´æ¥å›´ç»•â€œåŒ–å­¦å¤§æ¨¡å‹â€çš„ä¸€ä¸ªå…·ä½“å®ä¾‹â€”â€”è›‹ç™½è´¨è¯­è¨€æ¨¡å‹ï¼ˆPLMsï¼‰ã€‚å®ƒæ·±å…¥åˆ†æäº†è¿™ç±»å¤§è¯­è¨€æ¨¡å‹åœ¨ç”Ÿç‰©åŒ–å­¦åºåˆ—ï¼ˆè›‹ç™½è´¨ï¼‰ä¸­è¯†åˆ«å¤æ‚æ¨¡å¼ï¼ˆé‡å¤ç‰‡æ®µï¼‰çš„å†…éƒ¨æœºåˆ¶ï¼Œè¿™ä¸ç†è§£åŒ–å­¦é¢†åŸŸå¤§æ¨¡å‹çš„èƒ½åŠ›å’ŒåŸç†é«˜åº¦ç›¸å…³ã€‚

**ğŸ“– ä¸­æ–‡æ‘˜è¦**

æœ¬æ–‡ç ”ç©¶äº†è›‹ç™½è´¨è¯­è¨€æ¨¡å‹ï¼ˆPLMsï¼‰å†…éƒ¨æ£€æµ‹è›‹ç™½è´¨åºåˆ—ä¸­é‡å¤ç‰‡æ®µï¼ˆåŒ…æ‹¬ç²¾ç¡®é‡å¤å’Œè¿‘ä¼¼é‡å¤ï¼‰çš„æœºåˆ¶ã€‚è¿™äº›é‡å¤å¯¹è›‹ç™½è´¨ç»“æ„å’ŒåŠŸèƒ½è‡³å…³é‡è¦ã€‚è®ºæ–‡å‘ç°PLMsèƒ½å¤Ÿè¯†åˆ«è¿™ä¸¤ç§é‡å¤ï¼Œå¹¶æ­ç¤ºäº†å…¶å†…éƒ¨å·¥ä½œæœºåˆ¶åŒ…å«ä¸¤ä¸ªä¸»è¦é˜¶æ®µï¼šé¦–å…ˆï¼ŒPLMsé€šè¿‡é€šç”¨çš„ä½ç½®æ³¨æ„åŠ›å¤´å’Œç”Ÿç‰©å­¦ç‰¹åŒ–çš„ç»„ä»¶ï¼ˆå¦‚ç¼–ç æ°¨åŸºé…¸ç›¸ä¼¼æ€§çš„ç¥ç»å…ƒï¼‰æ„å»ºç‰¹å¾è¡¨ç¤ºï¼›ç„¶åï¼Œå½’çº³å¤´ï¼ˆinduction headsï¼‰å…³æ³¨é‡å¤ç‰‡æ®µé—´å¯¹é½çš„tokenï¼Œä»è€Œä¿ƒè¿›æ­£ç¡®ç­”æ¡ˆçš„é¢„æµ‹ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒPLMsé€šè¿‡ç»“åˆåŸºäºè¯­è¨€çš„æ¨¡å¼åŒ¹é…å’Œä¸“é—¨çš„ç”Ÿç‰©å­¦çŸ¥è¯†æ¥è§£å†³è¿™ä¸€ç”Ÿç‰©ä¿¡æ¯å­¦ä»»åŠ¡ã€‚

<details>
<summary><b>ğŸ” æŸ¥çœ‹åŸæ–‡æ‘˜è¦</b></summary>

Protein sequences are abundant in repeating segments, both as exact copies and as approximate segments with mutations. These repeats are important for protein structure and function, motivating decades of algorithmic work on repeat identification. Recent work has shown that protein language models (PLMs) identify repeats, by examining their behavior in masked-token prediction. To elucidate their internal mechanisms, we investigate how PLMs detect both exact and approximate repeats. We find that the mechanism for approximate repeats functionally subsumes that of exact repeats. We then characterize this mechanism, revealing two main stages: PLMs first build feature representations using both general positional attention heads and biologically specialized components, such as neurons that encode amino-acid similarity. Then, induction heads attend to aligned tokens across repeated segments, promoting the correct answer. Our results reveal how PLMs solve this biological task by combining language-based pattern matching with specialized biological knowledge, thereby establishing a basis for studying more complex evolutionary processes in PLMs.

</details>

---

### 18. [Inferential Mechanics Part 1: Causal Mechanistic Theories of Machine Learning in Chemical Biology with Implications](https://arxiv.org/abs/2602.23303)

**åŸºæœ¬ä¿¡æ¯**

- ğŸ”— arXiv: [`2602.23303`](https://arxiv.org/abs/2602.23303)
- ğŸ‘¥ ä½œè€…: Ilya Balabin, Thomas M. Kaiser
- ğŸ“„ PDF: [ä¸‹è½½](https://arxiv.org/pdf/2602.23303.pdf)

**ğŸ’¡ ç›¸å…³æ€§åˆ†æ**

æ»¡è¶³æ ‡å‡†1ï¼šè®ºæ–‡çš„æ ¸å¿ƒç ”ç©¶å†…å®¹ç›´æ¥å›´ç»•åŒ–å­¦ä¿¡æ¯å­¦é¢†åŸŸçš„æœºå™¨å­¦ä¹ æ¨¡å‹ï¼ˆåŒ–å­¦å¤§æ¨¡å‹ï¼‰çš„ç†è®ºåŸºç¡€æ„å»ºï¼Œæ—¨åœ¨è§£å†³è¯¥é¢†åŸŸæ¨¡å‹å­˜åœ¨çš„å› æœç¼ºé™·é—®é¢˜ã€‚

**ğŸ“– ä¸­æ–‡æ‘˜è¦**

è¿™ç¯‡è®ºæ–‡é¢˜ä¸ºã€ŠInferential Mechanics Part 1: Causal Mechanistic Theories of Machine Learning in Chemical Biology with Implicationsã€‹ã€‚å®ƒç›´æ¥é’ˆå¯¹åŒ–å­¦ä¿¡æ¯å­¦é¢†åŸŸï¼Œæå‡ºäº†ä¸€ä¸ªæ–°é¢–çš„ç†è®ºæ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å½“å‰åŒ–å­¦å’Œç”Ÿç‰©å­¦ä¸­æœºå™¨å­¦ä¹ æ¨¡å‹ï¼ˆå¦‚ç”¨äºè¯ç‰©å‘ç°çš„æ¨¡å‹ï¼‰æ™®éå­˜åœ¨çš„å› æœç»“æ„ç¼ºé™·é—®é¢˜ã€‚ä½œè€…å°†åŒ–å­¦ç†è®ºã€ç”Ÿç‰©ç†è®ºã€æ¦‚ç‡è®ºå’Œå› æœæ¨ç†ç›¸ç»“åˆï¼Œä¸ºåŒ–å­¦ç”Ÿç‰©å­¦ä¸­çš„æœºå™¨å­¦ä¹ å»ºç«‹äº†ä¸€ä¸ªæ–°çš„æ•°å­¦æ¡†æ¶ï¼Œç§°ä¸ºâ€œæ¨ç†åŠ›å­¦â€ã€‚è¯¥æ¡†æ¶çš„æ ¸å¿ƒæ˜¯å¼•å…¥äº†â€œç„¦ç‚¹â€è¿™ä¸€æ–°æ¦‚å¿µï¼Œå³æœºå™¨å­¦ä¹ ç®—æ³•ä»å¤§å‹æ•°æ®é›†ä¸­èšç„¦äºæ½œåœ¨æœºåˆ¶çš„èƒ½åŠ›ã€‚è®ºæ–‡è¿˜æä¾›äº†åœ¨AktæŠ‘åˆ¶å‰‚å®¶æ—ä¸Šçš„åˆæ­¥åŸç†è¯æ˜ã€‚è¿™é¡¹å·¥ä½œç›´æ¥å›´ç»•â€œåŒ–å­¦å¤§æ¨¡å‹â€çš„ä¸»é¢˜ï¼Œå› ä¸ºå®ƒæ—¨åœ¨ä¸ºåŒ–å­¦å’Œç”Ÿç‰©å­¦ä¸­çš„æœºå™¨å­¦ä¹ ï¼ˆå¯è§†ä¸ºç‰¹å®šé¢†åŸŸçš„â€œåŒ–å­¦å¤§æ¨¡å‹â€ï¼‰æä¾›ä¸€ä¸ªæ›´ä¸¥è°¨ã€åŸºäºå› æœæœºåˆ¶çš„ç†è®ºåŸºç¡€ï¼Œä»¥çº æ­£å½“å‰é»‘ç®±æ¨¡å‹çš„ç¼ºé™·ã€‚

<details>
<summary><b>ğŸ” æŸ¥çœ‹åŸæ–‡æ‘˜è¦</b></summary>

Machine learning techniques are now routinely encountered in research laboratories across the globe. Impressive progress has been made through ML and AI techniques with regards to large data set processing. This progress has increased the ability of the experimenter to digest data and make novel predictions regarding phenomena of interest. However, machine learning predictors generated from data sets taken from the natural sciences are often treated as black boxes which are used broadly and generally without detailed consideration of the causal structure of the data set of interest. Work has been attempted to bring causality into discussions of machine learning models of natural phenomena; however, a firm and unified theoretical treatment is lacking. This series of three papers explores the union of chemical theory, biological theory, probability theory and causality that will correct current causal flaws of machine learning in the natural sciences. This paper, Part 1 of the series, provides the formal framework of the foundational causal structure of phenomena in chemical biology and is extended to machine learning through the novel concept of focus, defined here as the ability of a machine learning algorithm to narrow down to a hidden underpinning mechanism in large data sets. Initial proof of these principles on a family of Akt inhibitors is also provided. The second paper containing Part 2 will provide a formal exploration of chemical similarity, and Part 3 will present extensive experimental evidence of how hidden causal structures weaken all machine learning in chemical biology. This series serves to establish for chemical biology a new kind of mathematical framework for modeling mechanisms in Nature without the need for the tools of reductionism: inferential mechanics.

</details>

---

### 19. [CrossLLM-Mamba: Multimodal State Space Fusion of LLMs for RNA Interaction Prediction](https://arxiv.org/abs/2602.22236)

**åŸºæœ¬ä¿¡æ¯**

- ğŸ”— arXiv: [`2602.22236`](https://arxiv.org/abs/2602.22236)
- ğŸ‘¥ ä½œè€…: Rabeya Tus Sadia, Qiang Ye, Qiang Cheng
- ğŸ“„ PDF: [ä¸‹è½½](https://arxiv.org/pdf/2602.22236.pdf)

**ğŸ’¡ ç›¸å…³æ€§åˆ†æ**

æ»¡è¶³æ ‡å‡†1ï¼šè®ºæ–‡çš„ä¸»è¦ç ”ç©¶å†…å®¹æ˜¯åˆ©ç”¨å’Œèåˆç”Ÿç‰©å¤§è¯­è¨€æ¨¡å‹ï¼ˆBioLLMsï¼‰è¿›è¡ŒRNAç›¸äº’ä½œç”¨é¢„æµ‹ï¼Œè¿™ç›´æ¥å±äºâ€œåŒ–å­¦å¤§æ¨¡å‹â€åœ¨åŒ–å­¦ç”Ÿç‰©å­¦é¢†åŸŸçš„åº”ç”¨å’Œç ”ç©¶ã€‚

**ğŸ“– ä¸­æ–‡æ‘˜è¦**

è¿™ç¯‡è®ºæ–‡é¢˜ä¸ºã€ŠCrossLLM-Mamba: Multimodal State Space Fusion of LLMs for RNA Interaction Predictionã€‹ã€‚å®ƒæå‡ºäº†ä¸€ç§åä¸ºCrossLLM-Mambaçš„æ–°å‹æ¡†æ¶ï¼Œç”¨äºé¢„æµ‹RNAç›¸å…³çš„ç›¸äº’ä½œç”¨ï¼ˆå¦‚RNA-è›‹ç™½è´¨ã€RNA-å°åˆ†å­ã€RNA-RNAï¼‰ã€‚è¯¥æ¡†æ¶çš„æ ¸å¿ƒæ˜¯åˆ©ç”¨ç”Ÿç‰©å¤§è¯­è¨€æ¨¡å‹ï¼ˆBioLLMsï¼Œå¦‚ESM-2, RiNALMoï¼‰æä¾›çš„å¼ºå¤§åºåˆ—è¡¨ç¤ºï¼Œå¹¶é€šè¿‡åŒå‘Mambaç¼–ç å™¨å®ç°è·¨æ¨¡æ€çš„æ·±åº¦çŠ¶æ€ç©ºé—´å¯¹é½ã€‚è¿™é¡¹å·¥ä½œå°†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åº”ç”¨äºç”Ÿç‰©åˆ†å­ï¼ˆRNAï¼‰çš„è¡¨ç¤ºå­¦ä¹ å’Œç›¸äº’ä½œç”¨é¢„æµ‹ï¼Œå±äºâ€œåŒ–å­¦å¤§æ¨¡å‹â€åœ¨ç”Ÿç‰©åŒ–å­¦å’Œè®¡ç®—ç”Ÿç‰©å­¦é¢†åŸŸçš„åº”ç”¨ã€‚è®ºæ–‡å±•ç¤ºäº†è¯¥æ¡†æ¶åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œè¯æ˜äº†åŸºäºçŠ¶æ€ç©ºé—´å»ºæ¨¡çš„LLMèåˆèŒƒå¼åœ¨ç”Ÿç‰©å¤šæ¨¡æ€äº¤äº’é¢„æµ‹ä¸­çš„æœ‰æ•ˆæ€§ã€‚

<details>
<summary><b>ğŸ” æŸ¥çœ‹åŸæ–‡æ‘˜è¦</b></summary>

Accurate prediction of RNA-associated interactions is essential for understanding cellular regulation and advancing drug discovery. While Biological Large Language Models (BioLLMs) such as ESM-2 and RiNALMo provide powerful sequence representations, existing methods rely on static fusion strategies that fail to capture the dynamic, context-dependent nature of molecular binding. We introduce CrossLLM-Mamba, a novel framework that reformulates interaction prediction as a state-space alignment problem. By leveraging bidirectional Mamba encoders, our approach enables deep ``crosstalk'' between modality-specific embeddings through hidden state propagation, modeling interactions as dynamic sequence transitions rather than static feature overlaps. The framework maintains linear computational complexity, making it scalable to high-dimensional BioLLM embeddings. We further incorporate Gaussian noise injection and Focal Loss to enhance robustness against hard-negative samples. Comprehensive experiments across three interaction categories, RNA-protein, RNA-small molecule, and RNA-RNA demonstrate that CrossLLM-Mamba achieves state-of-the-art performance. On the RPI1460 benchmark, our model attains an MCC of 0.892, surpassing the previous best by 5.2\%. For binding affinity prediction, we achieve Pearson correlations exceeding 0.95 on riboswitch and repeat RNA subtypes. These results establish state-space modeling as a powerful paradigm for multi-modal biological interaction prediction.

</details>

---

### 20. [VAE-MS: An Asymmetric Variational Autoencoder for Mutational Signature Extraction](https://arxiv.org/abs/2602.22239)

**åŸºæœ¬ä¿¡æ¯**

- ğŸ”— arXiv: [`2602.22239`](https://arxiv.org/abs/2602.22239)
- ğŸ‘¥ ä½œè€…: Ida Egendal, Rasmus Froberg BrÃ¸ndum, Dan J Woodcock ç­‰5äºº
- ğŸ“„ PDF: [ä¸‹è½½](https://arxiv.org/pdf/2602.22239.pdf)

**ğŸ’¡ ç›¸å…³æ€§åˆ†æ**

æ»¡è¶³æ ‡å‡†1ï¼šè®ºæ–‡çš„ä¸»è¦ç ”ç©¶å†…å®¹æ˜¯å¼€å‘ä¸€ç§åŸºäºå˜åˆ†è‡ªç¼–ç å™¨çš„æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œç”¨äºåˆ†æç™Œç—‡çªå˜æ•°æ®ï¼ˆä¸€ç§åŒ–å­¦ç”Ÿç‰©å­¦æ•°æ®ï¼‰ï¼Œå±äºåŒ–å­¦ä¿¡æ¯å­¦ä¸­æœºå™¨å­¦ä¹ æ¨¡å‹çš„åº”ç”¨ã€‚

**ğŸ“– ä¸­æ–‡æ‘˜è¦**

è¿™ç¯‡è®ºæ–‡é¢˜ä¸ºã€ŠVAE-MS: An Asymmetric Variational Autoencoder for Mutational Signature Extractionã€‹ã€‚å®ƒæå‡ºäº†ä¸€ç§ç”¨äºç™Œç—‡çªå˜ç‰¹å¾æå–çš„æ–°å‹å˜åˆ†è‡ªç¼–ç å™¨æ¨¡å‹ã€‚çªå˜ç‰¹å¾åˆ†ææ˜¯ç™Œç—‡åŸºå› ç»„å­¦å’Œç”Ÿç‰©ä¿¡æ¯å­¦ä¸­çš„å…³é”®æ–¹æ³•ã€‚VAE-MSæ¨¡å‹ç»“åˆäº†éå¯¹ç§°æ¶æ„å’Œæ¦‚ç‡æ–¹æ³•ï¼Œæ—¨åœ¨ä»çªå˜æ•°æ®ä¸­æ›´å¯é åœ°æå–ç‰¹å¾ã€‚è®ºæ–‡å°†VAE-MSä¸ç°æœ‰çš„é‡‘æ ‡å‡†æ–¹æ³•ï¼ˆå¦‚SigProfilerExtractorï¼‰ä»¥åŠå…¶ä»–å…ˆè¿›æ¨¡å‹ï¼ˆå¦‚MUSE-XAE, SigneRï¼‰è¿›è¡Œäº†æ¯”è¾ƒã€‚è¿™é¡¹å·¥ä½œå±äºè®¡ç®—åŒ–å­¦å’Œç”Ÿç‰©ä¿¡æ¯å­¦é¢†åŸŸï¼Œåˆ©ç”¨æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼ˆVAEï¼‰å¤„ç†å’Œåˆ†æåŒ–å­¦ç”Ÿç‰©å­¦æ•°æ®ï¼ˆç™Œç—‡çªå˜è°±ï¼‰ï¼Œæ˜¯æœºå™¨å­¦ä¹ åœ¨åŒ–å­¦å’Œç”Ÿç‰©å­¦æ•°æ®å»ºæ¨¡ä¸­çš„ä¸€ä¸ªå…·ä½“åº”ç”¨ã€‚

<details>
<summary><b>ğŸ” æŸ¥çœ‹åŸæ–‡æ‘˜è¦</b></summary>

Mutational signature analysis has emerged as a powerful method for uncovering the underlying biological processes driving cancer development. However, the signature extraction process, typically performed using non-negative matrix factorization (NMF), often lacks reliability and clinical applicability. To address these limitations, several solutions have been introduced, including the use of neural networks to achieve more accurate estimates and probabilistic methods to better capture natural variation in the data. In this work, we introduce a Variational Autoencoder for Mutational Signatures (VAE-MS), a novel model that leverages both an asymmetric architecture and probabilistic methods for the extraction of mutational signatures. VAE-MS is compared to with three state-of-the-art models for mutational signature extraction: SigProfilerExtractor, the NMF-based gold standard; MUSE-XAE, an autoencoder that employs an asymmetric design without probabilistic components; and SigneR, a Bayesian NMF model, to illustrate the strength in combining a nonlinear extraction with a probabilistic model. In the ability to reconstruct input data and generalize to unseen data, models with probabilistic components (VAE-MS, SigneR) dramatically outperformed models without (SigProfilerExtractor, MUSE-XAE). The NMF-baed models (SigneR, SigProfilerExtractor) had the most accurate reconstructions in simulated data, while VAE-MS reconstructed more accurately on real cancer data. Upon evaluating the ability to extract signatures consistently, no model exhibited a clear advantage over the others. Software for VAE-MS is available at this https URL .

</details>

---

### 21. [Stochastic Neural Networks for Quantum Devices](https://arxiv.org/abs/2602.22241)

**åŸºæœ¬ä¿¡æ¯**

- ğŸ”— arXiv: [`2602.22241`](https://arxiv.org/abs/2602.22241)
- ğŸ‘¥ ä½œè€…: Bodo Rosenhahn, Tobias J. Osborne, Christoph Hirche
- ğŸ“„ PDF: [ä¸‹è½½](https://arxiv.org/pdf/2602.22241.pdf)

**ğŸ’¡ ç›¸å…³æ€§åˆ†æ**

æ»¡è¶³æ ‡å‡†1ï¼šè®ºæ–‡çš„æ ¸å¿ƒç ”ç©¶å†…å®¹æ˜¯åœ¨é‡å­è®¾å¤‡ä¸Šå®ç°å’Œä¼˜åŒ–éšæœºç¥ç»ç½‘ç»œï¼Œæ¢ç´¢é‡å­è®¡ç®—ä¸æœºå™¨å­¦ä¹ çš„äº¤å‰ï¼Œè¿™ä¸æœªæ¥åŒ–å­¦ä¿¡æ¯å­¦ä¸­æ–°å‹è®¡ç®—æ¨¡å‹çš„å‘å±•æ–¹å‘ç›¸å…³ã€‚

**ğŸ“– ä¸­æ–‡æ‘˜è¦**

è¿™ç¯‡è®ºæ–‡é¢˜ä¸ºã€ŠStochastic Neural Networks for Quantum Devicesã€‹ã€‚å®ƒæå‡ºäº†ä¸€ç§åœ¨åŸºäºé—¨çš„é‡å­è®¡ç®—ä¸­ï¼Œå°†éšæœºç¥ç»ç½‘ç»œè¡¨è¾¾å’Œä¼˜åŒ–ä¸ºé‡å­ç”µè·¯çš„è¡¨è¿°ã€‚è®ºæ–‡å—ç»å…¸æ„ŸçŸ¥å™¨å¯å‘ï¼Œå¼•å…¥äº†éšæœºç¥ç»å…ƒå¹¶å°†å…¶ç»„åˆæˆé‡å­ç¥ç»ç½‘ç»œã€‚æ¨¡å‹ä½¿ç”¨Kiefer-Wolfowitzç®—æ³•ç»“åˆæ¨¡æ‹Ÿé€€ç«è¿›è¡Œè®­ç»ƒã€‚å±•ç¤ºäº†å¤šç§æ‹“æ‰‘å’Œæ¨¡å‹ï¼ŒåŒ…æ‹¬æµ…å±‚å…¨è¿æ¥ç½‘ç»œã€Hopfieldç½‘ç»œã€å—é™ç»å°”å…¹æ›¼æœºã€è‡ªç¼–ç å™¨å’Œå·ç§¯ç¥ç»ç½‘ç»œã€‚æ­¤å¤–ï¼Œè¿˜æ¼”ç¤ºäº†å°†ä¼˜åŒ–åçš„ç¥ç»ç½‘ç»œä½œä¸ºGroverç®—æ³•çš„é¢„è¨€æœºï¼Œä»¥å®ç°é‡å­ç”Ÿæˆå¼AIæ¨¡å‹ã€‚è¿™é¡¹å·¥ä½œå¤„äºé‡å­è®¡ç®—å’Œæœºå™¨å­¦ä¹ çš„äº¤å‰é¢†åŸŸï¼Œæ¢ç´¢äº†åœ¨é‡å­è®¾å¤‡ä¸Šå®ç°ç¥ç»ç½‘ç»œçš„å¯èƒ½æ€§ï¼Œè™½ç„¶ä¸ç›´æ¥é’ˆå¯¹è´¨è°±ï¼Œä½†å…¶å…³äºâ€œæ¨¡å‹â€å’Œâ€œè®¡ç®—â€çš„æ ¸å¿ƒä¸å¹¿ä¹‰çš„â€œåŒ–å­¦ä¿¡æ¯å­¦æ¨¡å‹â€æœ‰ä¸€å®šå…³è”ï¼Œä¸”é‡å­è®¡ç®—åœ¨æœªæ¥çš„åŒ–å­¦æ¨¡æ‹Ÿå’Œåˆ†å­è®¾è®¡ä¸­å…·æœ‰æ½œåœ¨é‡è¦æ€§ã€‚è€ƒè™‘åˆ°å…¶ä¸è®¡ç®—åŒ–å­¦å’Œæ–°å‹è®¡ç®—æ¨¡å‹çš„å…³è”ï¼Œä»¥åŒ…å®¹æ€§åŸåˆ™çº³å…¥ã€‚

<details>
<summary><b>ğŸ” æŸ¥çœ‹åŸæ–‡æ‘˜è¦</b></summary>

This work presents a formulation to express and optimize stochastic neural networks as quantum circuits in gate-based quantum computing. Motivated by a classical perceptron, stochastic neurons are introduced and combined into a quantum neural network. The Kiefer-Wolfowitz algorithm in combination with simulated annealing is used for training the network weights. Several topologies and models are presented, including shallow fully connected networks, Hopfield Networks, Restricted Boltzmann Machines, Autoencoders and convolutional neural networks. We also demonstrate the combination of our optimized neural networks as an oracle for the Grover algorithm to realize a quantum generative AI model.

</details>

---

### 22. [Multi-Dimensional Spectral Geometry of Biological Knowledge in Single-Cell Transformer Representations](https://arxiv.org/abs/2602.22247)

**åŸºæœ¬ä¿¡æ¯**

- ğŸ”— arXiv: [`2602.22247`](https://arxiv.org/abs/2602.22247)
- ğŸ‘¥ ä½œè€…: Ihor Kendiukhov
- ğŸ“„ PDF: [ä¸‹è½½](https://arxiv.org/pdf/2602.22247.pdf)

**ğŸ’¡ ç›¸å…³æ€§åˆ†æ**

æ»¡è¶³æ ‡å‡†1ï¼šè®ºæ–‡çš„æ ¸å¿ƒç ”ç©¶å†…å®¹æ˜¯æ·±å…¥åˆ†æå’Œè§£é‡Šä¸€ä¸ªç”¨äºå•ç»†èƒç”Ÿç‰©å­¦çš„Transformeræ¨¡å‹ï¼ˆå¯è§†ä¸ºä¸€ç§ç”Ÿç‰©åŒ–å­¦é¢†åŸŸçš„å¤§æ¨¡å‹ï¼‰çš„å†…éƒ¨è¡¨ç¤ºå’ŒçŸ¥è¯†ç¼–ç æœºåˆ¶ã€‚

**ğŸ“– ä¸­æ–‡æ‘˜è¦**

è¿™ç¯‡è®ºæ–‡é¢˜ä¸ºã€ŠMulti-Dimensional Spectral Geometry of Biological Knowledge in Single-Cell Transformer Representationsã€‹ã€‚å®ƒç³»ç»Ÿåœ°ç ”ç©¶äº†å•ç»†èƒåŸºç¡€æ¨¡å‹scGPTï¼ˆä¸€ç§åŸºäºTransformerçš„æ¨¡å‹ï¼‰å†…éƒ¨é«˜ç»´åŸºå› è¡¨ç¤ºæ‰€ç¼–ç çš„ç”Ÿç‰©å­¦çŸ¥è¯†ã€‚é€šè¿‡è‡ªåŠ¨åŒ–çš„å‡è®¾ç­›é€‰ï¼Œè®ºæ–‡æ­ç¤ºäº†scGPTæ¨¡å‹å°†åŸºå› ç»„ç»‡æˆä¸€ä¸ªç»“æ„åŒ–çš„ç”Ÿç‰©åæ ‡ç³»ï¼Œå…¶ä¸­ä¸»å¯¼çš„å…‰è°±è½´æ ¹æ®äºšç»†èƒå®šä½åˆ†ç¦»åŸºå› ï¼Œæ­£äº¤è½´ç¼–ç è›‹ç™½è´¨-è›‹ç™½è´¨ç›¸äº’ä½œç”¨ç½‘ç»œï¼Œå¹¶åœ¨ä¸€ä¸ªç´§å‡‘çš„å­ç©ºé—´ä¸­åŒºåˆ†è½¬å½•å› å­ä¸å…¶é¶åŸºå› ã€‚è¿™é¡¹å·¥ä½œæ·±å…¥åˆ†æäº†Transformeræ¨¡å‹åœ¨å•ç»†èƒç”Ÿç‰©å­¦æ•°æ®ä¸Šå­¦ä¹ åˆ°çš„å¯è§£é‡Šçš„å†…éƒ¨è¡¨ç¤ºï¼Œæ­ç¤ºäº†æ¨¡å‹å¦‚ä½•å†…åŒ–ç»†èƒç»„ç»‡çš„çŸ¥è¯†ã€‚è¿™ç›´æ¥å…³è”åˆ°â€œåŒ–å­¦å¤§æ¨¡å‹â€ï¼ˆæ­¤å¤„ä¸ºç”Ÿç‰©é¢†åŸŸçš„Transformeræ¨¡å‹ï¼‰çš„å¯è§£é‡Šæ€§å’Œå†…éƒ¨å·¥ä½œæœºåˆ¶ç ”ç©¶ã€‚

<details>
<summary><b>ğŸ” æŸ¥çœ‹åŸæ–‡æ‘˜è¦</b></summary>

Single-cell foundation models such as scGPT learn high-dimensional gene representations, but what biological knowledge these representations encode remains unclear. We systematically decode the geometric structure of scGPT internal representations through 63 iterations of automated hypothesis screening (183 hypotheses tested), revealing that the model organizes genes into a structured biological coordinate system rather than an opaque feature space. The dominant spectral axis separates genes by subcellular localization, with secreted proteins at one pole and cytosolic proteins at the other. Intermediate transformer layers transiently encode mitochondrial and ER compartments in a sequence that mirrors the cellular secretory pathway. Orthogonal axes encode protein-protein interaction networks with graded fidelity to experimentally measured interaction strength (Spearman rho = 1.000 across n = 5 STRING confidence quintiles, p = 0.017). In a compact six-dimensional spectral subspace, the model distinguishes transcription factors from their target genes (AUROC = 0.744, all 12 layers significant). Early layers preserve which specific genes regulate which targets, while deeper layers compress this into a coarser regulator versus regulated distinction. Repression edges are geometrically more prominent than activation edges, and B-cell master regulators BATF and BACH2 show convergence toward the B-cell identity anchor PAX5 across transformer depth. Cell-type marker genes cluster with high fidelity (AUROC = 0.851). Residual-stream geometry encodes biological structure complementary to attention patterns. These results indicate that biological transformers learn an interpretable internal model of cellular organization, with implications for regulatory network inference, drug target prioritization, and model auditing.

</details>

---

### 23. [Machine Learning on Heterogeneous, Edge, and Quantum Hardware for Particle Physics (ML-HEQUPP)](https://arxiv.org/abs/2602.22248)

**åŸºæœ¬ä¿¡æ¯**

- ğŸ”— arXiv: [`2602.22248`](https://arxiv.org/abs/2602.22248)
- ğŸ‘¥ ä½œè€…: Julia Gonski, Jenni Ott, Shiva Abbaszadeh ç­‰100äºº
- ğŸ“„ PDF: [ä¸‹è½½](https://arxiv.org/pdf/2602.22248.pdf)

**ğŸ’¡ ç›¸å…³æ€§åˆ†æ**

æ»¡è¶³æ ‡å‡†3ï¼šè®ºæ–‡æ˜¯ä¸€ä»½å‰ç»æ€§çš„ç™½çš®ä¹¦/ç»¼è¿°ï¼Œå¹¿æ³›è®¨è®ºäº†æœºå™¨å­¦ä¹ åœ¨ç§‘å­¦è®¡ç®—ï¼ˆåŒ…æ‹¬å¯èƒ½æ¶‰åŠåŒ–å­¦æ•°æ®çš„é¢†åŸŸï¼‰ä¸æ–°å‹ç¡¬ä»¶ï¼ˆè¾¹ç¼˜ã€é‡å­ï¼‰ç»“åˆçš„åº”ç”¨ã€æŒ‘æˆ˜å’Œæœºé‡ï¼ŒåŒ…å«äº†ä¸æ„å»ºå’Œå¤„ç†å¤§å‹ç§‘å­¦æ¨¡å‹åŠæ•°æ®ç›¸å…³çš„é‡è¦è®¨è®ºã€‚

**ğŸ“– ä¸­æ–‡æ‘˜è¦**

è¿™ç¯‡è®ºæ–‡é¢˜ä¸ºã€ŠMachine Learning on Heterogeneous, Edge, and Quantum Hardware for Particle Physics (ML-HEQUPP)ã€‹ã€‚è¿™æ˜¯ä¸€ä»½å…³äºåœ¨é«˜èƒ½ç‰©ç†å®éªŒä¸­åº”ç”¨æœºå™¨å­¦ä¹ ã€è¾¹ç¼˜è®¡ç®—å’Œé‡å­ç¡¬ä»¶ç­‰æ–°å…´æŠ€æœ¯çš„ç™½çš®ä¹¦ã€‚å®ƒæ¦‚è¿°äº†ç¤¾åŒºé©±åŠ¨çš„æ„¿æ™¯ï¼Œæ—¨åœ¨è¯†åˆ«å’Œä¼˜å…ˆè€ƒè™‘åŸºäºç¡¬ä»¶çš„MLç³»ç»ŸåŠå…¶ç‰©ç†åº”ç”¨çš„ç ”ç©¶å’Œå¼€å‘æœºä¼šã€‚è™½ç„¶ä¸»è¦é’ˆå¯¹ç²’å­ç‰©ç†ï¼Œä½†å…¶ä¸­è®¨è®ºçš„è®¸å¤šæŒ‘æˆ˜å’ŒæŠ€æœ¯ï¼ˆå¦‚ä½åŠŸè€—è¾¹ç¼˜AIã€å¼‚æ„åŠ é€Ÿå™¨ã€é‡å­ç®—æ³•ã€æ¨¡æ‹Ÿè®¡ç®—ç­‰ï¼‰ä¸å¤„ç†å¤§å‹ç§‘å­¦æ•°æ®ï¼ˆåŒ…æ‹¬å¯èƒ½çš„åŒ–å­¦æˆ–è´¨è°±æ•°æ®ï¼‰çš„é€šç”¨é«˜æ€§èƒ½è®¡ç®—å’Œæ™ºèƒ½ä¿¡æ¯å­¦æ¡†æ¶é«˜åº¦ç›¸å…³ã€‚è®ºæ–‡å¼ºè°ƒäº†AI/MLä¸æ–°å‹ç¡¬ä»¶ååŒè®¾è®¡å¯¹äºåº”å¯¹æœªæ¥ç§‘å­¦å®éªŒæ•°æ®æŒ‘æˆ˜çš„é‡è¦æ€§ã€‚

<details>
<summary><b>ğŸ” æŸ¥çœ‹åŸæ–‡æ‘˜è¦</b></summary>

The next generation of particle physics experiments will face a new era of challenges in data acquisition, due to unprecedented data rates and volumes along with extreme environments and operational constraints. Harnessing this data for scientific discovery demands real-time inference and decision-making, intelligent data reduction, and efficient processing architectures beyond current capabilities. Crucial to the success of this experimental paradigm are several emerging technologies, such as artificial intelligence and machine learning (AI/ML) and silicon microelectronics, and the advent of quantum algorithms and processing. Their intersection includes areas of research such as low-power and low-latency devices for edge computing, heterogeneous accelerator systems, reconfigurable hardware, novel codesign and synthesis strategies, readout for cryogenic or high-radiation environments, and analog computing. This white paper presents a community-driven vision to identify and prioritize research and development opportunities in hardware-based ML systems and corresponding physics applications, contributing towards a successful transition to the new data frontier of fundamental science.

</details>

---

### 24. [Flow Matching is Adaptive to Manifold Structures](https://arxiv.org/abs/2602.22486)

**åŸºæœ¬ä¿¡æ¯**

- ğŸ”— arXiv: [`2602.22486`](https://arxiv.org/abs/2602.22486)
- ğŸ‘¥ ä½œè€…: Shivam Kumar, Yixin Wang, Lizhen Lin
- ğŸ“„ PDF: [ä¸‹è½½](https://arxiv.org/pdf/2602.22486.pdf)

**ğŸ’¡ ç›¸å…³æ€§åˆ†æ**

æ»¡è¶³æ ‡å‡†1ï¼šè®ºæ–‡çš„æ ¸å¿ƒç ”ç©¶å†…å®¹å›´ç»•ç”Ÿæˆæ¨¡å‹ï¼ˆåŒ–å­¦å¤§æ¨¡å‹çš„ä¸€ä¸ªå…³é”®åº”ç”¨æ–¹å‘ï¼‰çš„ç†è®ºåˆ†æå±•å¼€ï¼Œç‰¹åˆ«æ˜¯é’ˆå¯¹åˆ†å­ç»“æ„ç”Ÿæˆç­‰åœºæ™¯ï¼Œæ¢è®¨äº†æ¨¡å‹åœ¨ä½ç»´æµå½¢æ•°æ®ä¸Šçš„é€‚åº”æ€§ï¼Œä¸'åŒ–å­¦å¤§æ¨¡å‹'ä¸»é¢˜ç›´æ¥ç›¸å…³ã€‚

**ğŸ“– ä¸­æ–‡æ‘˜è¦**

æœ¬æ–‡ä»ç†è®ºè§’åº¦åˆ†æäº†æµåŒ¹é…ï¼ˆFlow Matchingï¼‰æ–¹æ³•åœ¨ç›®æ ‡åˆ†å¸ƒæ”¯æ’‘äºä½ç»´æµå½¢æ—¶çš„æ€§è´¨ã€‚æµåŒ¹é…æ˜¯ä¸€ç§å…æ¨¡æ‹Ÿçš„ç”Ÿæˆå»ºæ¨¡æ–¹æ³•ï¼Œé€šè¿‡å­¦ä¹ æºåˆ†å¸ƒï¼ˆå¦‚æ ‡å‡†æ­£æ€åˆ†å¸ƒï¼‰ä¸ç›®æ ‡æ•°æ®åˆ†å¸ƒä¹‹é—´çš„æ’å€¼è·¯å¾„ä¸Šçš„é€Ÿåº¦åœºæ¥ç”Ÿæˆæ ·æœ¬ã€‚è®ºæ–‡æŒ‡å‡ºï¼Œå°½ç®¡æµåŒ¹é…æ–¹æ³•åœ¨æ–‡æœ¬åˆ°å›¾åƒåˆæˆã€è§†é¢‘ç”Ÿæˆå’Œåˆ†å­ç»“æ„ç”Ÿæˆç­‰é«˜ç»´æ•°æ®é›†ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†ç°æœ‰ç†è®ºåˆ†æé€šå¸¸å‡è®¾ç›®æ ‡åˆ†å¸ƒå…·æœ‰å¹³æ»‘çš„å…¨ç»´å¯†åº¦ï¼Œæœªèƒ½è§£é‡Šå…¶åœ¨æµå½¢æ”¯æ’‘æ•°æ®ä¸Šçš„æœ‰æ•ˆæ€§ã€‚ä¸ºæ­¤ï¼Œä½œè€…å»ºç«‹äº†å½“ç›®æ ‡åˆ†å¸ƒæ”¯æ’‘äºå…‰æ»‘æµå½¢æ—¶ï¼ŒæµåŒ¹é…æ–¹æ³•ä¸­å­¦ä¹ åˆ°çš„é€Ÿåº¦åœºçš„éæ¸è¿‘æ”¶æ•›ä¿è¯ï¼Œå¹¶å°†æ­¤ä¼°è®¡è¯¯å·®é€šè¿‡å¸¸å¾®åˆ†æ–¹ç¨‹ä¼ æ’­ï¼Œå¾—åˆ°äº†ç”±æµåŒ¹é…ç›®æ ‡è¯±å¯¼çš„éšå¼å¯†åº¦ä¼°è®¡å™¨çš„ç»Ÿè®¡ä¸€è‡´æ€§ã€‚æœ€ç»ˆè¯æ˜å…¶æ”¶æ•›é€Ÿç‡æ¥è¿‘æå°æå¤§æœ€ä¼˜ï¼Œä¸”ä»…ä¾èµ–äºå†…åœ¨ç»´åº¦ï¼Œåæ˜ äº†æµå½¢å’Œç›®æ ‡åˆ†å¸ƒçš„å…‰æ»‘æ€§ã€‚è¿™äº›ç»“æœä¸ºæµåŒ¹é…å¦‚ä½•é€‚åº”æ•°æ®çš„å†…åœ¨å‡ ä½•ç»“æ„å¹¶è§„é¿ç»´åº¦è¯…å’’æä¾›äº†åŸç†æ€§è§£é‡Šã€‚

<details>
<summary><b>ğŸ” æŸ¥çœ‹åŸæ–‡æ‘˜è¦</b></summary>

Flow matching has emerged as a simulation-free alternative to diffusion-based generative modeling, producing samples by solving an ODE whose time-dependent velocity field is learned along an interpolation between a simple source distribution (e.g., a standard normal) and a target data distribution. Flow-based methods often exhibit greater training stability and have achieved strong empirical performance in high-dimensional settings where data concentrate near a low-dimensional manifold, such as text-to-image synthesis, video generation, and molecular structure generation. Despite this success, existing theoretical analyses of flow matching assume target distributions with smooth, full-dimensional densities, leaving its effectiveness in manifold-supported settings largely unexplained. To this end, we theoretically analyze flow matching with linear interpolation when the target distribution is supported on a smooth manifold. We establish a non-asymptotic convergence guarantee for the learned velocity field, and then propagate this estimation error through the ODE to obtain statistical consistency of the implicit density estimator induced by the flow-matching objective. The resulting convergence rate is near minimax-optimal, depends only on the intrinsic dimension, and reflects the smoothness of both the manifold and the target distribution. Together, these results provide a principled explanation for how flow matching adapts to intrinsic data geometry and circumvents the curse of dimensionality.

</details>

---

### 25. [Discovery of Interpretable Physical Laws in Materials via Language-Model-Guided Symbolic Regression](https://arxiv.org/abs/2602.22967)

**åŸºæœ¬ä¿¡æ¯**

- ğŸ”— arXiv: [`2602.22967`](https://arxiv.org/abs/2602.22967)
- ğŸ‘¥ ä½œè€…: Yifeng Guan, Chuyi Liu, Dongzhan Zhou ç­‰7äºº
- ğŸ“„ PDF: [ä¸‹è½½](https://arxiv.org/pdf/2602.22967.pdf)

**ğŸ’¡ ç›¸å…³æ€§åˆ†æ**

æ»¡è¶³æ ‡å‡†1ï¼šè®ºæ–‡çš„æ ¸å¿ƒç ”ç©¶å†…å®¹æ˜¯å¼€å‘ä¸€ä¸ªç”±å¤§è¯­è¨€æ¨¡å‹å¼•å¯¼çš„ã€ç”¨äºä»ææ–™ç§‘å­¦æ•°æ®ä¸­å‘ç°ç‰©ç†å®šå¾‹çš„æ¡†æ¶ã€‚è¿™ç›´æ¥æ¶‰åŠåˆ©ç”¨'åŒ–å­¦å¤§æ¨¡å‹'ï¼ˆæ­¤å¤„æŒ‡å¤§å‹è¯­è¨€æ¨¡å‹ä½œä¸ºç§‘å­¦å‘ç°çš„å·¥å…·ï¼‰æ¥è§£å†³åŒ–å­¦ä¿¡æ¯å­¦ä¸­çš„å…³é”®é—®é¢˜ï¼Œå³ä»æ•°æ®ä¸­æ¨å¯¼å¯è§£é‡Šçš„æ¨¡å‹ã€‚

**ğŸ“– ä¸­æ–‡æ‘˜è¦**

æœ¬æ–‡æå‡ºäº†ä¸€ç§åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¼•å¯¼ç¬¦å·å›å½’ï¼Œä»é«˜ç»´æ•°æ®ä¸­å‘ç°å¯è§£é‡Šç‰©ç†å®šå¾‹çš„æ¡†æ¶ã€‚ä¼ ç»Ÿç¬¦å·å›å½’æ–¹æ³•åœ¨æœç´¢å·¨å¤§çš„å¯èƒ½å½¢å¼ç©ºé—´æ—¶ï¼Œå¸¸å¸¸äº§ç”Ÿå¤æ‚ä¸”ä¸å…·ç‰©ç†æ„ä¹‰çš„å…¬å¼ã€‚è¯¥æ¡†æ¶é€šè¿‡åˆ©ç”¨LLMä¸­åµŒå…¥çš„ç§‘å­¦çŸ¥è¯†æ¥å¼•å¯¼æœç´¢è¿‡ç¨‹ï¼Œä»è€Œé«˜æ•ˆåœ°ä»æ•°æ®ä¸­è¯†åˆ«ç‰©ç†å®šå¾‹ã€‚ä½œè€…é€šè¿‡åœ¨é’™é’›çŸ¿ææ–™çš„å…³é”®å±æ€§å»ºæ¨¡ä¸ŠéªŒè¯äº†è¯¥æ–¹æ³•ã€‚è¯¥æ–¹æ³•ç¼“è§£äº†ä¼ ç»Ÿç¬¦å·å›å½’ä¸­å¸¸è§çš„ç»„åˆçˆ†ç‚¸é—®é¢˜ï¼Œå°†æœ‰æ•ˆæœç´¢ç©ºé—´å‡å°‘äº†çº¦10^5å€ã€‚ç ”ç©¶è¯†åˆ«å‡ºäº†ä¸€ç»„å…³äºä½“æ¨¡é‡ã€å¸¦éš™å’Œææ°§ååº”æ´»æ€§çš„æ–°å…¬å¼ï¼Œè¿™äº›å…¬å¼ä¸ä»…æä¾›äº†æœ‰æ„ä¹‰çš„ç‰©ç†è§è§£ï¼Œè€Œä¸”åœ¨å‡†ç¡®æ€§å’Œç®€æ´æ€§ä¸Šè¶…è¶Šäº†å…ˆå‰çš„å…¬å¼ã€‚

<details>
<summary><b>ğŸ” æŸ¥çœ‹åŸæ–‡æ‘˜è¦</b></summary>

Discovering interpretable physical laws from high-dimensional data is a fundamental challenge in scientific research. Traditional methods, such as symbolic regression, often produce complex, unphysical formulas when searching a vast space of possible forms. We introduce a framework that guides the search process by leveraging the embedded scientific knowledge of large language models, enabling efficient identification of physical laws in the data. We validate our approach by modeling key properties of perovskite materials. Our method mitigates the combinatorial explosion commonly encountered in traditional symbolic regression, reducing the effective search space by a factor of approximately $10^5$. A set of novel formulas for bulk modulus, band gap, and oxygen evolution reaction activity are identified, which not only provide meaningful physical insights but also outperform previous formulas in accuracy and simplicity.

</details>

---

### 26. [Efficient Graph Coloring with Neural Networks: A Physics-Inspired Approach for Large Graphs](https://arxiv.org/abs/2408.01503)

**åŸºæœ¬ä¿¡æ¯**

- ğŸ”— arXiv: [`2408.01503`](https://arxiv.org/abs/2408.01503)
- ğŸ‘¥ ä½œè€…: Lorenzo Colantonio, Andrea Cacioppo, Federico Scarpati ç­‰6äºº
- ğŸ“„ PDF: [ä¸‹è½½](https://arxiv.org/pdf/2408.01503.pdf)

**ğŸ’¡ ç›¸å…³æ€§åˆ†æ**

æ»¡è¶³æ ‡å‡†1ï¼šè®ºæ–‡æå‡ºäº†ä¸€ç§ç»“åˆå›¾ç¥ç»ç½‘ç»œå’Œç‰©ç†åŸç†çš„æ¡†æ¶æ¥è§£å†³å›¾ç€è‰²é—®é¢˜ã€‚è™½ç„¶åº”ç”¨é¢†åŸŸæ˜¯ç»„åˆä¼˜åŒ–ï¼Œä½†å…¶æ ¸å¿ƒæ–¹æ³•ï¼ˆå›¾ç¥ç»ç½‘ç»œä¸ç”Ÿæˆå¼/æ¨æ–­æ¡†æ¶çš„ç»“åˆï¼‰ä¸æ„å»ºç”¨äºå¤æ‚åŒ–å­¦ç³»ç»Ÿï¼ˆå¦‚åˆ†å­å›¾ï¼‰æ¨ç†å’Œç”Ÿæˆçš„'åŒ–å­¦å¤§æ¨¡å‹'åœ¨æ–¹æ³•è®ºä¸Šé«˜åº¦ç›¸å…³ã€‚

**ğŸ“– ä¸­æ–‡æ‘˜è¦**

æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªå—ç‰©ç†å­¦å¯å‘çš„ç¥ç»æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ç»“åˆå›¾ç¥ç»ç½‘ç»œå’Œç»Ÿè®¡åŠ›å­¦åŸç†ï¼Œå­¦ä¹ è§£å†³å¤§è§„æ¨¡å›¾ç€è‰²é—®é¢˜ã€‚å›¾ç€è‰²æ˜¯çº¦æŸæ»¡è¶³é—®é¢˜çš„å…¸å‹ä»£è¡¨ï¼Œè¡¨ç°å‡ºå°–é”çš„åŠ¨æ€å’Œå¯æ»¡è¶³æ€§é˜ˆå€¼ã€‚è¯¥æ¡†æ¶æ•´åˆäº†åŸºäºç§æ¤çš„ç›‘ç£ä¿¡å·ã€å¯¹ç§°ç ´ç¼ºæ­£åˆ™åŒ–å’Œè¿­ä»£å™ªå£°é€€ç«ç¥ç»åŠ¨åŠ›å­¦ï¼Œä»¥å¯¼èˆªèšé›†çš„è§£ç©ºé—´ã€‚å½“è¿­ä»£æ¬¡æ•°ä¸å›¾è§„æ¨¡å‘ˆäºŒæ¬¡æ–¹å¢é•¿æ—¶ï¼Œå­¦ä¹ åˆ°çš„æ±‚è§£å™¨åœ¨éšæœºå›¾ä¸­èƒ½è¾¾åˆ°æ¥è¿‘ç†è®ºåŠ¨æ€é˜ˆå€¼çš„æ€§èƒ½ï¼Œå¹¶åœ¨ç§æ¤æ¨æ–­æœºåˆ¶ä¸­å®ç°æ¥è¿‘æœ€ä¼˜çš„æ£€æµ‹æ€§èƒ½ã€‚è¯¥æ¨¡å‹èƒ½å¤Ÿä»å°å‹è®­ç»ƒå›¾æ³›åŒ–åˆ°è§„æ¨¡å¤§å‡ ä¸ªæ•°é‡çº§çš„å®ä¾‹ï¼Œè¯æ˜äº†ç¥ç»æ¶æ„å¯ä»¥å­¦ä¹ åˆ°åœ¨ç»„åˆä¼˜åŒ–å’Œæ¨æ–­çš„ç¡¬è¿é€šåŒºåŸŸä»ç„¶æœ‰æ•ˆçš„å¯æ‰©å±•ç®—æ³•ç­–ç•¥ã€‚

<details>
<summary><b>ğŸ” æŸ¥çœ‹åŸæ–‡æ‘˜è¦</b></summary>

Combinatorial optimization problems near algorithmic phase transitions represent a fundamental challenge for both classical algorithms and machine learning approaches. Among them, graph coloring stands as a prototypical constraint satisfaction problem exhibiting sharp dynamical and satisfiability thresholds. Here we introduce a physics-inspired neural framework that learns to solve large-scale graph coloring instances by combining graph neural networks with statistical-mechanics principles. Our approach integrates a planting-based supervised signal, symmetry-breaking regularization, and iterative noise-annealed neural dynamics to navigate clustered solution landscapes. When the number of iterations scales quadratically with graph size, the learned solver reaches algorithmic thresholds close to the theoretical dynamical transition in random graphs and achieves near-optimal detection performance in the planted inference regime. The model generalizes from small training graphs to instances orders of magnitude larger, demonstrating that neural architectures can learn scalable algorithmic strategies that remain effective in hard connectivity regions. These results establish a general paradigm for learning neural solvers that operate near fundamental phase boundaries in combinatorial optimization and inference.

</details>

---

### 27. [Neuro-Symbolic AI for Analytical Solutions of Differential Equations](https://arxiv.org/abs/2502.01476)

**åŸºæœ¬ä¿¡æ¯**

- ğŸ”— arXiv: [`2502.01476`](https://arxiv.org/abs/2502.01476)
- ğŸ‘¥ ä½œè€…: Orestis Oikonomou, Levi Lingsch, Dana Grund ç­‰5äºº
- ğŸ“„ PDF: [ä¸‹è½½](https://arxiv.org/pdf/2502.01476.pdf)

**ğŸ’¡ ç›¸å…³æ€§åˆ†æ**

æ»¡è¶³æ ‡å‡†1ï¼šè®ºæ–‡çš„æ ¸å¿ƒç ”ç©¶å†…å®¹æ˜¯å¼€å‘ä¸€ä¸ªç¥ç»ç¬¦å·æ¡†æ¶ï¼ˆSIGSï¼‰ï¼Œç”¨äºè‡ªåŠ¨å‘ç°å¾®åˆ†æ–¹ç¨‹çš„è§£æè§£ã€‚è¯¥æ–¹æ³•ç»“åˆäº†å½¢å¼è¯­æ³•ï¼ˆç¬¦å·ï¼‰å’Œè¿ç»­ç©ºé—´æœç´¢ï¼ˆç¥ç»ï¼‰ï¼Œæ˜¯æ„å»ºç”¨äºç§‘å­¦å‘ç°ï¼ˆåŒ…æ‹¬åŒ–å­¦åŠ¨åŠ›å­¦ã€åˆ†å­æ¨¡æ‹Ÿç­‰é¢†åŸŸï¼‰çš„'åŒ–å­¦å¤§æ¨¡å‹'çš„ä¸€ç§å‰æ²¿æ–¹æ³•å­¦æ¢ç´¢ã€‚

**ğŸ“– ä¸­æ–‡æ‘˜è¦**

æœ¬æ–‡ä»‹ç»äº†SIGSï¼Œä¸€ä¸ªç”¨äºè‡ªåŠ¨æ±‚è§£å¾®åˆ†æ–¹ç¨‹è§£æè§£çš„ç¥ç»ç¬¦å·æ¡†æ¶ã€‚å¾®åˆ†æ–¹ç¨‹çš„è§£æè§£èƒ½æä¾›ç²¾ç¡®ã€å¯è§£é‡Šçš„æ´å¯Ÿï¼Œä½†å¾ˆå°‘å¯ç”¨ï¼Œå› ä¸ºå‘ç°å®ƒä»¬éœ€è¦ä¸“å®¶ç›´è§‰æˆ–åœ¨ç»„åˆç©ºé—´ä¸­è¿›è¡Œç©·ä¸¾æœç´¢ã€‚SIGSä½¿ç”¨å½¢å¼è¯­æ³•ä»…ç”Ÿæˆè¯­æ³•æœ‰æ•ˆçš„æ„å»ºå—ï¼Œå°†è¿™äº›è¡¨è¾¾å¼åµŒå…¥è¿ç»­ç©ºé—´ï¼Œç„¶åé€šè¿‡æœ€å°åŒ–åŸºäºç‰©ç†çš„æ®‹å·®ï¼Œåœ¨è¯¥ç©ºé—´ä¸­æœç´¢ã€è¯„åˆ†å’Œç»†åŒ–å€™é€‰çš„é—­å¼è§£ã€‚è¯¥è®¾è®¡å°†ç¬¦å·æ¨ç†ä¸æ•°å€¼ä¼˜åŒ–ç›¸ç»Ÿä¸€ï¼›è¯­æ³•ç¡®ä¿å€™é€‰è§£å—åœ¨æ„é€ ä¸Šæ˜¯æ­£ç¡®çš„ï¼Œè€Œæ½œåœ¨æœç´¢ä½¿æ¢ç´¢æ˜“äºå¤„ç†ä¸”æ— éœ€æ•°æ®ã€‚SIGSæ˜¯ç¬¬ä¸€ä¸ªèƒ½å¤Ÿï¼ˆiï¼‰è§£ææ±‚è§£éçº¿æ€§åå¾®åˆ†æ–¹ç¨‹è€¦åˆç³»ç»Ÿï¼Œï¼ˆiiï¼‰åœ¨è¯­æ³•æœªå®Œå…¨æŒ‡å®šçš„æƒ…å†µä¸‹å‘ç°è§£ï¼Œä»¥åŠï¼ˆiiiï¼‰ä¸ºç¼ºä¹å·²çŸ¥é—­å¼è§£çš„åå¾®åˆ†æ–¹ç¨‹äº§ç”Ÿç²¾ç¡®ç¬¦å·è¿‘ä¼¼çš„ç¥ç»ç¬¦å·æ–¹æ³•ã€‚æ€»ä½“è€Œè¨€ï¼ŒSIGSåœ¨æ ‡å‡†åŸºå‡†æµ‹è¯•ä¸Šï¼Œç›¸æ¯”ç°æœ‰çš„ç¬¦å·æ–¹æ³•ï¼Œåœ¨å‡†ç¡®æ€§å’Œæ•ˆç‡ä¸Šå®ç°äº†æ•°é‡çº§çš„æå‡ã€‚

<details>
<summary><b>ğŸ” æŸ¥çœ‹åŸæ–‡æ‘˜è¦</b></summary>

Analytical solutions to differential equations offer exact, interpretable insight but are rarely available because discovering them requires expert intuition or exhaustive search in combinatorial spaces. We introduce SIGS, a neuro-symbolic framework that automates this process. SIGS uses a formal grammar to generate only syntactically valid building blocks, embeds these expressions into a continuous space, and then searches this space to assemble, score, and refine candidate closed-form solutions by minimizing a physics-based residual. This design unifies symbolic reasoning with numerical optimization; the grammar constrains candidate solution blocks to be proper by construction, while the latent search makes exploration tractable and data-free. SIGS is the first neuro-symbolic method to (i) analytically solve coupled systems of nonlinear PDEs, (ii) discover solutions under grammar misspecification, and (iii) produce accurate symbolic approximations for PDEs lacking known closed-form solutions. Overall, SIGS achieves orders-of-magnitude improvements in accuracy and efficiency over existing symbolic methods on standard benchmarks.

</details>

---

### 28. [CLIP-Free, Label Free, Unsupervised Concept Bottleneck Models](https://arxiv.org/abs/2503.10981)

**åŸºæœ¬ä¿¡æ¯**

- ğŸ”— arXiv: [`2503.10981`](https://arxiv.org/abs/2503.10981)
- ğŸ‘¥ ä½œè€…: Fawaz Sammani, Jonas Fischer, Nikos Deligiannis
- ğŸ“„ PDF: [ä¸‹è½½](https://arxiv.org/pdf/2503.10981.pdf)

**ğŸ’¡ ç›¸å…³æ€§åˆ†æ**

æ»¡è¶³æ ‡å‡†1ï¼šè®ºæ–‡çš„æ ¸å¿ƒç ”ç©¶å†…å®¹æ˜¯å¼€å‘ä¸€ç§æ–°å‹çš„ã€æ— éœ€å¤–éƒ¨æ¨¡å‹ï¼ˆCLIPï¼‰å’Œæ ‡æ³¨çš„æ¦‚å¿µç“¶é¢ˆæ¨¡å‹ã€‚æ¦‚å¿µç“¶é¢ˆæ¨¡å‹æ—¨åœ¨æé«˜æ¨¡å‹çš„å¯è§£é‡Šæ€§ï¼Œè¿™æ˜¯æ„å»ºå¯é ã€å¯è§£é‡Šçš„'åŒ–å­¦å¤§æ¨¡å‹'ï¼ˆä¾‹å¦‚ç”¨äºæ€§è´¨é¢„æµ‹æˆ–ååº”æ¨ç†ï¼‰çš„ä¸€ä¸ªé‡è¦ç ”ç©¶æ–¹å‘ã€‚è¯¥æ–¹æ³•åœ¨æå‡æ¨¡å‹å¯è§£é‡Šæ€§æ–¹é¢çš„åˆ›æ–°ä¸åŒ–å­¦ä¿¡æ¯å­¦ä¸­æ¨¡å‹é€æ˜åŒ–çš„éœ€æ±‚ç›´æ¥ç›¸å…³ã€‚

**ğŸ“– ä¸­æ–‡æ‘˜è¦**

æœ¬æ–‡æå‡ºäº†ä¸€ç§æ— éœ€CLIPæ¨¡å‹ã€æ— éœ€å›¾åƒ-æ¦‚å¿µæ ‡æ³¨ã€ä¸”èƒ½ä»¥æ— ç›‘ç£æ–¹å¼æ¨å¯¼çº¿æ€§åˆ†ç±»å™¨çš„æ¦‚å¿µç“¶é¢ˆæ¨¡å‹ï¼ˆCBMï¼‰æ„å»ºæ–¹æ³•ã€‚æ¦‚å¿µç“¶é¢ˆæ¨¡å‹å°†å¯†é›†ç‰¹å¾è¡¨ç¤ºæ˜ å°„åˆ°äººç±»å¯è§£é‡Šçš„æ¦‚å¿µï¼Œç„¶åçº¿æ€§ç»„åˆè¿™äº›æ¦‚å¿µè¿›è¡Œé¢„æµ‹ã€‚ç°æœ‰CBMä¾èµ–CLIPæ¨¡å‹è·å–å›¾åƒ-æ¦‚å¿µæ ‡æ³¨ï¼Œæˆ–éœ€è¦äººå·¥æ ‡æ³¨ã€‚æœ¬æ–‡æ–¹æ³•é€šè¿‡å°†ä»»ä½•å†»ç»“çš„è§†è§‰åˆ†ç±»å™¨çš„åˆ†å¸ƒï¼ˆåœ¨ç¦»æ•£ç±»åˆ«ç´¢å¼•ä¸Šï¼‰ä¸å…¶å¯¹åº”çš„ã€ä»æ–‡æœ¬ç±»åˆ«åç§°å¯¼å‡ºçš„è§†è§‰-è¯­è¨€å¯¹åº”åˆ†å¸ƒå¯¹é½ï¼ŒåŒæ—¶ä¿æŒåˆ†ç±»å™¨çš„æ€§èƒ½ï¼Œä»è€Œå°†å…¶è½¬æ¢ä¸ºCBMã€‚è¯¥æ–¹æ³•ä¸éœ€è¦çœŸå®å›¾åƒ-ç±»åˆ«æ ‡æ³¨ï¼Œæ•°æ®æ•ˆç‡é«˜ï¼Œå¹¶ä¿ç•™äº†åˆ†ç±»å™¨çš„æ¨ç†è¿‡ç¨‹ã€‚åœ¨è¶…è¿‡40ä¸ªè§†è§‰åˆ†ç±»å™¨ä¸Šçš„åº”ç”¨å’Œæµ‹è¯•è¡¨æ˜ï¼Œæ‰€å¾—åˆ°çš„æ— ç›‘ç£ã€æ— æ ‡ç­¾ã€æ— CLIPçš„CBMï¼ˆU-F^2-CBMï¼‰è¾¾åˆ°äº†æ–°çš„æœ€å…ˆè¿›æ°´å¹³ï¼Œç”šè‡³è¶…è¿‡äº†æœ‰ç›‘ç£çš„åŸºäºCLIPçš„CBMã€‚ä½œè€…è¿˜å±•ç¤ºäº†è¯¥æ–¹æ³•å¯ç”¨äºé›¶æ ·æœ¬å›¾åƒæè¿°ï¼Œæ€§èƒ½ä¼˜äºåŸºäºCLIPçš„ç°æœ‰æ–¹æ³•ï¼Œè¾¾åˆ°æœ€å…ˆè¿›æ°´å¹³ã€‚

<details>
<summary><b>ğŸ” æŸ¥çœ‹åŸæ–‡æ‘˜è¦</b></summary>

Concept Bottleneck Models (CBMs) map dense feature representations into human-interpretable concepts which are then combined linearly to make a prediction. However, modern CBMs rely on the CLIP model to obtain image-concept annotations, and it remains unclear how to design CBMs without the CLIP bottleneck. Methods that do not use CLIP instead require manual, labor intensive annotation to associate feature representations with concepts. Furthermore, all CBMs necessitate training a linear classifier to map the extracted concepts to class labels. In this work, we lift all three limitations simultaneously by proposing a method that converts any frozen visual classifier into a CBM without requiring image-concept labels (label-free), without relying on the CLIP model (CLIP-free), and by deriving the linear classifier in an unsupervised manner. Our method is formulated by aligning the original classifier's distribution (over discrete class indices) with its corresponding vision-language counterpart distribution derived from textual class names, while preserving the classifier's performance. The approach requires no ground-truth image-class annotations, and is highly data-efficient and preserves the classifier's reasoning process. Applied and tested on over 40 visual classifiers, our resulting unsupervised, label-free and CLIP-free CBM (U-F$^2$-CBM) sets a new state of the art, surpassing even supervised CLIP-based CBMs. We also show that our method can be used for zero-shot image captioning, outperforming existing methods based on CLIP, and achieving state-of-art.

</details>

---

### 29. [The Spacetime of Diffusion Models: An Information Geometry Perspective](https://arxiv.org/abs/2505.17517)

**åŸºæœ¬ä¿¡æ¯**

- ğŸ”— arXiv: [`2505.17517`](https://arxiv.org/abs/2505.17517)
- ğŸ‘¥ ä½œè€…: RafaÅ‚ Karczewski, Markus Heinonen, Alison Pouplin ç­‰5äºº
- ğŸ“„ PDF: [ä¸‹è½½](https://arxiv.org/pdf/2505.17517.pdf)

**ğŸ’¡ ç›¸å…³æ€§åˆ†æ**

æ»¡è¶³æ ‡å‡†1ï¼šè®ºæ–‡çš„æ ¸å¿ƒç ”ç©¶å†…å®¹å›´ç»•æ‰©æ•£æ¨¡å‹çš„å‡ ä½•ç»“æ„å±•å¼€ï¼Œè€Œæ‰©æ•£æ¨¡å‹æ˜¯æ„å»ºåŒ–å­¦å¤§æ¨¡å‹ï¼ˆç”¨äºåˆ†å­ç”Ÿæˆã€æ€§è´¨é¢„æµ‹ç­‰ï¼‰å’Œè¿›è¡Œè´¨è°±ç»“æ„æ¨ç†ï¼ˆå¦‚é€šè¿‡ç”Ÿæˆæ¨¡å‹æ¨æ–­åˆ†å­ç»“æ„ï¼‰çš„å…³é”®åº•å±‚æŠ€æœ¯ä¹‹ä¸€ã€‚è®ºæ–‡å¯¹æ‰©æ•£æ¨¡å‹æ½œåœ¨ç©ºé—´å‡ ä½•æ€§è´¨çš„æ·±å…¥åˆ†æï¼Œç›´æ¥å…³è”åˆ°å¦‚ä½•æ›´å¥½åœ°åˆ©ç”¨è¿™ç±»æ¨¡å‹è¿›è¡ŒåŒ–å­¦æ•°æ®çš„è¡¨ç¤ºä¸ç”Ÿæˆã€‚

**ğŸ“– ä¸­æ–‡æ‘˜è¦**

æœ¬æ–‡ä»ä¿¡æ¯å‡ ä½•çš„è§’åº¦ä¸ºæ‰©æ•£æ¨¡å‹çš„æ½œåœ¨ç©ºé—´æä¾›äº†ä¸€ä¸ªæ–°é¢–çš„å‡ ä½•è§†è§’ã€‚ä½œè€…æŒ‡å‡ºï¼Œä¼ ç»Ÿçš„åŸºäºç¡®å®šæ€§æ¦‚ç‡æµODEè§£ç å™¨çš„å›æ‹‰æ–¹æ³•å­˜åœ¨æ ¹æœ¬æ€§ç¼ºé™·ï¼Œå› ä¸ºå®ƒå¼ºåˆ¶è¦æ±‚æµ‹åœ°çº¿åœ¨æ•°æ®ç©ºé—´ä¸­è§£ç ä¸ºç›´çº¿æ®µï¼Œä»è€Œå¿½ç•¥äº†æ•°æ®æœ¬èº«çš„å†…åœ¨å‡ ä½•ç»“æ„ã€‚ä½œä¸ºè¡¥å……ï¼Œæ‰©æ•£æ¨¡å‹ä¹Ÿå…è®¸é€šè¿‡åå‘SDEè¿›è¡Œéšæœºè§£ç ï¼Œè¿™ä½¿å¾—å¯ä»¥ä½¿ç”¨Fisher-Raoåº¦é‡è¿›è¡Œä¿¡æ¯å‡ ä½•å¤„ç†ã€‚ç„¶è€Œï¼Œé€‰æ‹©x_Tä½œä¸ºæ½œåœ¨è¡¨ç¤ºä¼šå¯¼è‡´è¯¥åº¦é‡åç¼©ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œä½œè€…å¼•å…¥äº†ä¸€ä¸ªæ½œåœ¨æ—¶ç©ºz=(x_t, t)ï¼Œè¯¥æ—¶ç©ºç´¢å¼•äº†æ‰€æœ‰å™ªå£°å°ºåº¦ä¸‹çš„å»å™ªåˆ†å¸ƒæ—p(x_0 | x_t)ï¼Œä»è€Œäº§ç”Ÿäº†ä¸€ä¸ªéå¹³å‡¡çš„å‡ ä½•ç»“æ„ã€‚ä½œè€…è¯æ˜äº†è¿™äº›åˆ†å¸ƒå½¢æˆäº†ä¸€ä¸ªæŒ‡æ•°æ—ï¼Œå¹¶æ¨å¯¼äº†æ›²çº¿é•¿åº¦çš„æ— æ¨¡æ‹Ÿä¼°è®¡å™¨ï¼Œä»è€Œå®ç°äº†é«˜æ•ˆçš„æµ‹åœ°çº¿è®¡ç®—ã€‚ç”±æ­¤äº§ç”Ÿçš„ç»“æ„å¼•å…¥äº†ä¸€ç§åŸåˆ™æ€§çš„æ‰©æ•£ç¼–è¾‘è·ç¦»ï¼Œå…¶ä¸­æµ‹åœ°çº¿è¿½è¸ªæ•°æ®ä¹‹é—´å™ªå£°å’Œå»å™ªç¼–è¾‘çš„æœ€å°åºåˆ—ã€‚è¿™é¡¹å·¥ä½œä¸ºç†è§£æ‰©æ•£æ¨¡å‹çš„å‡ ä½•ç»“æ„æä¾›äº†ç†è®ºåŸºç¡€ï¼Œè¿™å¯¹äºåŒ–å­¦ä¿¡æ¯å­¦é¢†åŸŸï¼ˆç‰¹åˆ«æ˜¯è´¨è°±åˆ†æï¼‰ä¸­åˆ©ç”¨æ‰©æ•£æ¨¡å‹è¿›è¡Œåˆ†å­ç”Ÿæˆã€ç»“æ„æ¨ç†å’Œè·¯å¾„é‡‡æ ·ç­‰ä»»åŠ¡å…·æœ‰é‡è¦å¯ç¤ºã€‚

<details>
<summary><b>ğŸ” æŸ¥çœ‹åŸæ–‡æ‘˜è¦</b></summary>

We present a novel geometric perspective on the latent space of diffusion models. We first show that the standard pullback approach, utilizing the deterministic probability flow ODE decoder, is fundamentally flawed. It provably forces geodesics to decode as straight segments in data space, effectively ignoring any intrinsic data geometry beyond the ambient Euclidean space. Complementing this view, diffusion also admits a stochastic decoder via the reverse SDE, which enables an information geometric treatment with the Fisher-Rao metric. However, a choice of $x_T$ as the latent representation collapses this metric due to memorylessness. We address this by introducing a latent spacetime $z=(x_t,t)$ that indexes the family of denoising distributions $p(x_0 | x_t)$ across all noise scales, yielding a nontrivial geometric structure. We prove these distributions form an exponential family and derive simulation-free estimators for curve lengths, enabling efficient geodesic computation. The resulting structure induces a principled Diffusion Edit Distance, where geodesics trace minimal sequences of noise and denoise edits between data. We also demonstrate benefits for transition path sampling in molecular systems, including constrained variants such as low-variance transitions and region avoidance. Code is available at: this https URL .

</details>

---

### 30. [Knowledge Fusion of Large Language Models Via Modular SkillPacks](https://arxiv.org/abs/2505.18502)

**åŸºæœ¬ä¿¡æ¯**

- ğŸ”— arXiv: [`2505.18502`](https://arxiv.org/abs/2505.18502)
- ğŸ‘¥ ä½œè€…: Guodong Du, Zhuo Li, Xuanning Zhou ç­‰12äºº
- ğŸ“„ PDF: [ä¸‹è½½](https://arxiv.org/pdf/2505.18502.pdf)

**ğŸ’¡ ç›¸å…³æ€§åˆ†æ**

æ»¡è¶³æ ‡å‡†1ï¼šè®ºæ–‡çš„æ ¸å¿ƒç ”ç©¶å†…å®¹æ˜¯å¤§å‹è¯­è¨€æ¨¡å‹çš„è·¨èƒ½åŠ›è¿ç§»ä¸çŸ¥è¯†èåˆæ–¹æ³•ã€‚è¿™ç›´æ¥å…³ç³»åˆ°å¦‚ä½•æ„å»ºå’Œä¼˜åŒ–é¢å‘ç‰¹å®šé¢†åŸŸï¼ˆå¦‚åŒ–å­¦ä¿¡æ¯å­¦ï¼‰çš„â€œåŒ–å­¦å¤§æ¨¡å‹â€ï¼Œé€šè¿‡æ¨¡å—åŒ–åœ°é›†æˆä¸åŒæ¥æºçš„çŸ¥è¯†å’ŒæŠ€èƒ½ï¼ˆä¾‹å¦‚ï¼Œç»“æ„é¢„æµ‹ã€è°±å›¾è§£æï¼‰ï¼Œæå‡æ¨¡å‹çš„æ€§èƒ½å’Œé€‚åº”æ€§ã€‚

**ğŸ“– ä¸­æ–‡æ‘˜è¦**

æœ¬æ–‡æå‡ºäº†GraftLLMï¼Œä¸€ç§æ–°é¢–çš„æ–¹æ³•ï¼Œç”¨äºè§£å†³å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­çš„è·¨èƒ½åŠ›è¿ç§»æŒ‘æˆ˜ï¼Œè¯¥æŒ‘æˆ˜åœ¨å¤šä»»åŠ¡é›†æˆã€æ¨¡å‹å‹ç¼©å’ŒæŒç»­å­¦ä¹ ç­‰åº”ç”¨ä¸­è‡³å…³é‡è¦ã€‚GraftLLMå°†æºæ¨¡å‹çš„èƒ½åŠ›ä»¥SkillPackæ ¼å¼å­˜å‚¨åœ¨ç›®æ ‡æ¨¡å‹ä¸­ã€‚è¿™ç§æ–¹æ³•ä¿ç•™äº†æ¨¡å‹çš„é€šç”¨èƒ½åŠ›ï¼Œå‡å°‘äº†å‚æ•°å†²çªï¼Œå¹¶æ”¯æŒæ— é—å¿˜çš„æŒç»­å­¦ä¹ å’Œæ¨¡å‹èåˆã€‚ä½œè€…é‡‡ç”¨äº†ä¸€ç§æ¨¡å—æ„ŸçŸ¥çš„è‡ªé€‚åº”å‹ç¼©ç­–ç•¥æ¥å‹ç¼©å‚æ•°æ›´æ–°ï¼Œåœ¨ç¡®ä¿é«˜æ•ˆå­˜å‚¨çš„åŒæ—¶ä¿æŒä»»åŠ¡ç‰¹å®šçŸ¥è¯†ã€‚ç”Ÿæˆçš„SkillPackä½œä¸ºä¸€ç§ç´§å‡‘ä¸”å¯è¿ç§»çš„çŸ¥è¯†è½½ä½“ï¼Œéå¸¸é€‚åˆå¼‚æ„æ¨¡å‹èåˆå’ŒæŒç»­å­¦ä¹ ã€‚å®éªŒè¡¨æ˜ï¼ŒGraftLLMåœ¨çŸ¥è¯†è¿ç§»ã€çŸ¥è¯†èåˆå’Œæ— é—å¿˜å­¦ä¹ æ–¹é¢ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚è¿™é¡¹å·¥ä½œä¸ºå¤§æ¨¡å‹çš„æ¨¡å—åŒ–ã€å¯ç»„åˆèƒ½åŠ›æ„å»ºå’Œè¿ç§»æä¾›äº†æ–°æ€è·¯ï¼Œè¿™å¯¹äºæ„å»ºé¢å‘åŒ–å­¦æˆ–è´¨è°±é¢†åŸŸçš„ä¸“ç”¨å¤§æ¨¡å‹ï¼ˆä¾‹å¦‚ï¼Œå°†é€šç”¨åŒ–å­¦çŸ¥è¯†ã€è°±å›¾è§£æèƒ½åŠ›ç­‰æ¨¡å—åŒ–å¹¶è¿ç§»åˆ°æ–°æ¨¡å‹ï¼‰å…·æœ‰ç›´æ¥çš„å‚è€ƒä»·å€¼ã€‚

<details>
<summary><b>ğŸ” æŸ¥çœ‹åŸæ–‡æ‘˜è¦</b></summary>

Cross-capability transfer is a key challenge in large language model (LLM) research, with applications in multi-task integration, model compression, and continual learning. Recent works like FuseLLM and FuseChat have demonstrated the potential of transferring multiple model capabilities to lightweight models, enhancing adaptability and efficiency, which motivates our investigation into more efficient cross-capability transfer methods. However, existing approaches primarily focus on small, homogeneous models, limiting their applicability. For large, heterogeneous models, knowledge distillation with full-parameter fine-tuning often overlooks the student model's intrinsic capacity and risks catastrophic forgetting, while PEFT methods struggle to effectively absorb knowledge from source LLMs. To address these issues, we introduce GraftLLM, a novel method that stores source model capabilities in a target model with SkillPack format. This approach preserves general capabilities, reduces parameter conflicts, and supports forget-free continual learning and model fusion. We employ a module-aware adaptive compression strategy to compress parameter updates, ensuring efficient storage while maintaining task-specific knowledge. The resulting SkillPack serves as a compact and transferable knowledge carrier, ideal for heterogeneous model fusion and continual learning. Experiments across various scenarios demonstrate that GraftLLM outperforms existing techniques in knowledge transfer, knowledge fusion, and forget-free learning, providing a scalable and efficient solution for cross-capability transfer. The code is publicly available at: this https URL .

</details>

---

### 31. [Random Matrix Theory-guided sparse PCA for single-cell RNA-seq data](https://arxiv.org/abs/2509.15429)

**åŸºæœ¬ä¿¡æ¯**

- ğŸ”— arXiv: [`2509.15429`](https://arxiv.org/abs/2509.15429)
- ğŸ‘¥ ä½œè€…: Victor ChardÃ¨s
- ğŸ“„ PDF: [ä¸‹è½½](https://arxiv.org/pdf/2509.15429.pdf)

**ğŸ’¡ ç›¸å…³æ€§åˆ†æ**

æ»¡è¶³æ ‡å‡†2ï¼šè®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ•°æ®å¤„ç†å’Œç‰¹å¾æå–æ–¹æ³•ï¼ˆåŸºäºRMTçš„ç¨€ç–PCAï¼‰ï¼Œå¹¶å±•ç¤ºäº†å…¶åœ¨ç”Ÿç‰©åŒ»å­¦é«˜ç»´æ•°æ®ï¼ˆå•ç»†èƒRNA-seqï¼‰ä¸Šçš„æœ‰æ•ˆæ€§ã€‚è¿™ç§æ–¹æ³•ä½œä¸ºä¸€ç§é€šç”¨çš„æ•°æ®é¢„å¤„ç†å’Œç‰¹å¾å·¥ç¨‹å·¥å…·ï¼Œå¯ä»¥åº”ç”¨äºåŒ–å­¦ä¿¡æ¯å­¦å’Œè´¨è°±åˆ†æé¢†åŸŸï¼Œç”¨äºå¤„ç†é«˜ç»´è´¨è°±æ•°æ®æˆ–åˆ†å­æè¿°ç¬¦æ•°æ®ï¼Œæå–å…³é”®ç‰¹å¾ä»¥ç”¨äºåç»­çš„åŒ–å­¦å¤§æ¨¡å‹è®­ç»ƒæˆ–è´¨è°±ç»“æ„æ¨ç†ã€‚

**ğŸ“– ä¸­æ–‡æ‘˜è¦**

æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºéšæœºçŸ©é˜µç†è®ºï¼ˆRMTï¼‰çš„ç¨€ç–ä¸»æˆåˆ†åˆ†æï¼ˆPCAï¼‰æ–¹æ³•ï¼Œç”¨äºå¤„ç†å•ç»†èƒRNA-seqæ•°æ®ã€‚å•ç»†èƒRNA-seqæ•°æ®å™ªå£°é«˜ã€ç»´åº¦é«˜ï¼Œä¼ ç»Ÿçš„PCAåœ¨é«˜ç»´æƒ…å†µä¸‹å­˜åœ¨åå·®ã€‚ä½œè€…çš„æ–¹æ³•é¦–å…ˆå¼•å…¥äº†ä¸€ç§æ–°é¢–çš„åŒç™½åŒ–ç®—æ³•ï¼Œè¯¥ç®—æ³•èƒ½å¤Ÿè‡ªæ´½åœ°ä¼°è®¡æ¯ä¸ªåŸºå› åœ¨æ¯ä¸ªç»†èƒä¸­çš„è½¬å½•ç»„å™ªå£°å¤§å°ï¼Œè€Œæ— éœ€å‡è®¾ç‰¹å®šçš„å™ªå£°åˆ†å¸ƒã€‚è¿™ä½¿å¾—èƒ½å¤Ÿä½¿ç”¨åŸºäºRMTçš„æ ‡å‡†è‡ªåŠ¨é€‰æ‹©ç¨€ç–åº¦æ°´å¹³ï¼Œä»è€Œä½¿ç¨€ç–PCAå‡ ä¹æ— éœ€å‚æ•°è°ƒæ•´ã€‚è¿™ç§åŸºäºæ•°å­¦çš„æ–¹æ³•ä¿ç•™äº†PCAçš„å¯è§£é‡Šæ€§ï¼ŒåŒæ—¶èƒ½å¤Ÿç¨³å¥ã€è‡ªåŠ¨åœ°æ¨æ–­ç¨€ç–ä¸»æˆåˆ†ã€‚åœ¨ä¸ƒç§å•ç»†èƒRNA-seqæŠ€æœ¯å’Œå››ç§ç¨€ç–PCAç®—æ³•ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•ç³»ç»Ÿåœ°æ”¹å–„äº†ä¸»æˆåˆ†å­ç©ºé—´çš„é‡å»ºï¼Œå¹¶åœ¨ç»†èƒç±»å‹åˆ†ç±»ä»»åŠ¡ä¸­ consistently ä¼˜äºåŸºäºPCAã€è‡ªç¼–ç å™¨å’Œæ‰©æ•£çš„æ–¹æ³•ã€‚è¯¥æ–¹æ³•ä¸ºé«˜ç»´ã€å™ªå£°æ•°æ®çš„é™ç»´å’Œç‰¹å¾æå–æä¾›äº†å¼ºå¤§çš„å·¥å…·ã€‚

<details>
<summary><b>ğŸ” æŸ¥çœ‹åŸæ–‡æ‘˜è¦</b></summary>

Single-cell RNA-seq provides detailed molecular snapshots of individual cells but is notoriously noisy. Variability stems from biological differences and technical factors, such as amplification bias and limited RNA capture efficiency, making it challenging to adapt computational pipelines to heterogeneous datasets or evolving technologies. As a result, most studies still rely on principal component analysis (PCA) for dimensionality reduction, valued for its interpretability and robustness, in spite of its known bias in high dimensions. Here, we improve upon PCA with a Random Matrix Theory (RMT)-based approach that guides the inference of sparse principal components using existing sparse PCA algorithms. We first introduce a novel biwhitening algorithm which self-consistently estimates the magnitude of transcriptomic noise affecting each gene in individual cells, without assuming a specific noise distribution. This enables the use of an RMT-based criterion to automatically select the sparsity level, rendering sparse PCA nearly parameter-free. Our mathematically grounded approach retains the interpretability of PCA while enabling robust, hands-off inference of sparse principal components. Across seven single-cell RNA-seq technologies and four sparse PCA algorithms, we show that this method systematically improves the reconstruction of the principal subspace and consistently outperforms PCA-, autoencoder-, and diffusion-based methods in cell-type classification tasks.

</details>

---

### 32. [G-reasoner: Foundation Models for Unified Reasoning over Graph-structured Knowledge](https://arxiv.org/abs/2509.24276)

**åŸºæœ¬ä¿¡æ¯**

- ğŸ”— arXiv: [`2509.24276`](https://arxiv.org/abs/2509.24276)
- ğŸ‘¥ ä½œè€…: Linhao Luo, Zicheng Zhao, Junnan Liu ç­‰12äºº
- ğŸ“„ PDF: [ä¸‹è½½](https://arxiv.org/pdf/2509.24276.pdf)

**ğŸ’¡ ç›¸å…³æ€§åˆ†æ**

æ»¡è¶³æ ‡å‡†1ï¼šè®ºæ–‡çš„æ ¸å¿ƒç ”ç©¶å†…å®¹æ˜¯æ„å»ºèƒ½å¤Ÿå¯¹å›¾ç»“æ„çŸ¥è¯†è¿›è¡Œæ¨ç†çš„åŸºç¡€æ¨¡å‹æ¡†æ¶ã€‚åœ¨åŒ–å­¦ä¿¡æ¯å­¦ä¸­ï¼Œåˆ†å­ç»“æ„ã€ååº”ç½‘ç»œã€åŒ–åˆç‰©-é¶ç‚¹ç›¸äº’ä½œç”¨ç­‰å¤©ç„¶åœ°ä»¥å›¾çš„å½¢å¼å­˜åœ¨ã€‚G-reasonerè¿™ç±»æ¡†æ¶ä¸ºæ„å»ºèƒ½å¤Ÿç†è§£å’Œæ¨ç†åŒ–å­¦å›¾çŸ¥è¯†çš„â€œåŒ–å­¦å¤§æ¨¡å‹â€æä¾›äº†é‡è¦çš„æŠ€æœ¯è·¯å¾„å’Œæ¶æ„å‚è€ƒï¼Œç›´æ¥æœåŠ¡äºåŸºäºå›¾è¡¨ç¤ºçš„åˆ†å­æ€§è´¨é¢„æµ‹ã€ååº”ç»“æœæ¨ç†å’Œè´¨è°±è°±å›¾ä¸åˆ†å­ç»“æ„å…³è”ç­‰ä»»åŠ¡ã€‚

**ğŸ“– ä¸­æ–‡æ‘˜è¦**

æœ¬æ–‡æå‡ºäº†G-reasonerï¼Œä¸€ä¸ªç»Ÿä¸€çš„æ¡†æ¶ï¼Œå°†å›¾åŸºç¡€æ¨¡å‹å’Œè¯­è¨€åŸºç¡€æ¨¡å‹é›†æˆèµ·æ¥ï¼Œç”¨äºå¯¹å¤šæ ·åŒ–å›¾ç»“æ„çŸ¥è¯†è¿›è¡Œå¯æ‰©å±•çš„æ¨ç†ã€‚å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤æ‚æ¨ç†æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†å—é™äºé™æ€å’Œä¸å®Œæ•´çš„å‚æ•°åŒ–çŸ¥è¯†ã€‚æ£€ç´¢å¢å¼ºç”Ÿæˆé€šè¿‡æ•´åˆå¤–éƒ¨çŸ¥è¯†æ¥ç¼“è§£è¿™ä¸€é—®é¢˜ï¼Œä½†ç°æœ‰çš„æ–¹æ³•ç”±äºä¿¡æ¯ç¢ç‰‡åŒ–å’ŒçŸ¥è¯†ç»“æ„å»ºæ¨¡è–„å¼±ï¼Œåœ¨å¤„ç†çŸ¥è¯†å¯†é›†å‹ä»»åŠ¡æ—¶ä»å­˜åœ¨å›°éš¾ã€‚å›¾æä¾›äº†ä¸€ç§å¯¹çŸ¥è¯†å†…éƒ¨å…³ç³»è¿›è¡Œå»ºæ¨¡çš„è‡ªç„¶æ–¹å¼ã€‚G-reasonerçš„æ ¸å¿ƒæ˜¯QuadGraphï¼Œä¸€ä¸ªæ ‡å‡†åŒ–çš„å››å±‚æŠ½è±¡ï¼Œå°†å¼‚æ„çŸ¥è¯†æºç»Ÿä¸€ä¸ºé€šç”¨çš„å›¾è¡¨ç¤ºã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œä½œè€…å¼•å…¥äº†ä¸€ä¸ª3400ä¸‡å‚æ•°çš„å›¾åŸºç¡€æ¨¡å‹ï¼Œè¯¥æ¨¡å‹è”åˆæ•è·å›¾æ‹“æ‰‘å’Œæ–‡æœ¬è¯­ä¹‰ï¼Œå¹¶ä¸LLMsé›†æˆä»¥å¢å¼ºä¸‹æ¸¸åº”ç”¨ä¸­çš„æ¨ç†èƒ½åŠ›ã€‚åœ¨å…­ä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒG-reasoner consistently ä¼˜äºæœ€å…ˆè¿›çš„åŸºçº¿æ–¹æ³•ï¼Œæ˜¾è‘—å¢å¼ºäº†LLMçš„æ¨ç†èƒ½åŠ›ï¼Œå¹¶å®ç°äº†å¼ºå¤§çš„æ•ˆç‡å’Œè·¨å›¾æ³›åŒ–èƒ½åŠ›ã€‚

<details>
<summary><b>ğŸ” æŸ¥çœ‹åŸæ–‡æ‘˜è¦</b></summary>

Large language models (LLMs) excel at complex reasoning but remain limited by static and incomplete parametric knowledge. Retrieval-augmented generation (RAG) mitigates this by incorporating external knowledge, yet existing RAGs struggle with knowledge-intensive tasks due to fragmented information and weak modeling of knowledge structure. Graphs offer a natural way to model relationships within knowledge, but LLMs are inherently unstructured and cannot effectively reason over graph-structured data. Recent graph-enhanced RAG (GraphRAG) attempts to bridge this gap by constructing tailored graphs and enabling LLMs to reason on them. However, these methods often depend on ad-hoc graph designs, heuristic search, or costly agent pipelines, which hinder scalability and generalization. To address these challenges, we present G-reasoner, a unified framework that integrates graph and language foundation models for scalable reasoning over diverse graph-structured knowledge. Central to our approach is QuadGraph, a standardized four-layer abstraction that unifies heterogeneous knowledge sources into a common graph representation. Building on this, we introduce a 34M-parameter graph foundation model (GFM) that jointly captures graph topology and textual semantics, and is integrated with LLMs to enhance reasoning in downstream applications. To ensure scalability and efficiency, mixed-precision training and distributed message-passing are implemented to scale GFM with more GPUs. Extensive experiments on six benchmarks show that G-reasoner consistently outperforms state-of-the-art baselines, significantly enhances LLM reasoning, and achieves strong efficiency and cross-graph generalization.

</details>

---

### 33. [Object-Centric Representation Learning for Enhanced 3D Semantic Scene Graph Prediction](https://arxiv.org/abs/2510.04714)

**åŸºæœ¬ä¿¡æ¯**

- ğŸ”— arXiv: [`2510.04714`](https://arxiv.org/abs/2510.04714)
- ğŸ‘¥ ä½œè€…: KunHo Heo, GiHyun Kim, SuYeon Kim ç­‰4äºº
- ğŸ“„ PDF: [ä¸‹è½½](https://arxiv.org/pdf/2510.04714.pdf)

**ğŸ’¡ ç›¸å…³æ€§åˆ†æ**

æ»¡è¶³æ ‡å‡†2ï¼šè®ºæ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„å¯¹è±¡ç‰¹å¾ç¼–ç å™¨å’Œå¯¹æ¯”é¢„è®­ç»ƒç­–ç•¥ï¼Œç”¨äºæå‡3Dåœºæ™¯ç†è§£ä»»åŠ¡çš„æ€§èƒ½ã€‚è¿™ç§ä¸“æ³¨äºå­¦ä¹ é«˜è´¨é‡ã€å¯åŒºåˆ†å¯¹è±¡è¡¨ç¤ºçš„æ–¹æ³•ï¼Œå¯ä»¥è¿ç§»åˆ°åŒ–å­¦ä¿¡æ¯å­¦é¢†åŸŸã€‚ä¾‹å¦‚ï¼Œåœ¨è´¨è°±ç»“æ„æ¨ç†ä¸­ï¼Œå¯ä»¥å°†è´¨è°±å³°æˆ–ç¢ç‰‡ç¦»å­è§†ä¸ºâ€œå¯¹è±¡â€ï¼Œå­¦ä¹ å…¶è¡¨ç¤ºä»¥æ¨æ–­å®ƒä»¬ä¹‹é—´çš„ç»“æ„å…³ç³»ï¼ˆå¦‚è¿æ¥æ€§ï¼‰ï¼Œä»è€Œæ„å»ºåˆ†å­çš„â€œåœºæ™¯å›¾â€ã€‚è®ºæ–‡æä¾›çš„æ–¹æ³•è®ºå¯¹æ”¹è¿›è´¨è°±æ•°æ®çš„è¡¨ç¤ºå­¦ä¹ å…·æœ‰å‚è€ƒä»·å€¼ã€‚

**ğŸ“– ä¸­æ–‡æ‘˜è¦**

æœ¬æ–‡ä¸“æ³¨äº3Dè¯­ä¹‰åœºæ™¯å›¾é¢„æµ‹ä»»åŠ¡ï¼Œæ—¨åœ¨æ£€æµ‹3Dåœºæ™¯ä¸­çš„å¯¹è±¡åŠå…¶è¯­ä¹‰å…³ç³»ã€‚ä½œè€…é€šè¿‡å¹¿æ³›åˆ†æå‘ç°ï¼Œå¯¹è±¡ç‰¹å¾çš„è´¨é‡åœ¨å†³å®šæ•´ä½“åœºæ™¯å›¾å‡†ç¡®æ€§æ–¹é¢èµ·ç€å…³é”®ä½œç”¨ã€‚ä¸ºäº†è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œä½œè€…è®¾è®¡äº†ä¸€ä¸ªé«˜åº¦åŒºåˆ†æ€§çš„å¯¹è±¡ç‰¹å¾ç¼–ç å™¨ï¼Œå¹¶é‡‡ç”¨äº†ä¸€ç§å¯¹æ¯”é¢„è®­ç»ƒç­–ç•¥ï¼Œå°†å¯¹è±¡è¡¨ç¤ºå­¦ä¹ ä¸åœºæ™¯å›¾é¢„æµ‹è§£è€¦ã€‚è¿™ç§è®¾è®¡ä¸ä»…æé«˜äº†å¯¹è±¡åˆ†ç±»çš„å‡†ç¡®æ€§ï¼Œè¿˜ç›´æ¥æ”¹å–„äº†å…³ç³»é¢„æµ‹ã€‚å½“å°†é¢„è®­ç»ƒçš„ç¼–ç å™¨æ’å…¥ç°æœ‰æ¡†æ¶æ—¶ï¼Œåœ¨æ‰€æœ‰è¯„ä¼°æŒ‡æ ‡ä¸Šéƒ½è§‚å¯Ÿåˆ°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚æ­¤å¤–ï¼Œä½œè€…æœ‰æ•ˆåœ°ç»“åˆäº†å‡ ä½•å’Œè¯­ä¹‰ç‰¹å¾æ¥å®ç°æ›´ä¼˜çš„å…³ç³»é¢„æµ‹ã€‚åœ¨3DSSGæ•°æ®é›†ä¸Šçš„ç»¼åˆå®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—ä¼˜äºå…ˆå‰æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚

<details>
<summary><b>ğŸ” æŸ¥çœ‹åŸæ–‡æ‘˜è¦</b></summary>

3D Semantic Scene Graph Prediction aims to detect objects and their semantic relationships in 3D scenes, and has emerged as a crucial technology for robotics and AR/VR applications. While previous research has addressed dataset limitations and explored various approaches including Open-Vocabulary settings, they frequently fail to optimize the representational capacity of object and relationship features, showing excessive reliance on Graph Neural Networks despite insufficient discriminative capability. In this work, we demonstrate through extensive analysis that the quality of object features plays a critical role in determining overall scene graph accuracy. To address this challenge, we design a highly discriminative object feature encoder and employ a contrastive pretraining strategy that decouples object representation learning from the scene graph prediction. This design not only enhances object classification accuracy but also yields direct improvements in relationship prediction. Notably, when plugging in our pretrained encoder into existing frameworks, we observe substantial performance improvements across all evaluation metrics. Additionally, whereas existing approaches have not fully exploited the integration of relationship information, we effectively combine both geometric and semantic features to achieve superior relationship prediction. Comprehensive experiments on the 3DSSG dataset demonstrate that our approach significantly outperforms previous state-of-the-art methods. Our code is publicly available at this https URL .

</details>

---

### 34. [Learning Hamiltonian Flow Maps: Mean Flow Consistency for Large-Timestep Molecular Dynamics](https://arxiv.org/abs/2601.22123)

**åŸºæœ¬ä¿¡æ¯**

- ğŸ”— arXiv: [`2601.22123`](https://arxiv.org/abs/2601.22123)
- ğŸ‘¥ ä½œè€…: Winfried Ripken, Michael Plainer, Gregor Lied ç­‰8äºº
- ğŸ“„ PDF: [ä¸‹è½½](https://arxiv.org/pdf/2601.22123.pdf)

**ğŸ’¡ ç›¸å…³æ€§åˆ†æ**

æ»¡è¶³æ ‡å‡†1ï¼šè®ºæ–‡çš„æ ¸å¿ƒç ”ç©¶å†…å®¹å›´ç»•å¼€å‘ç”¨äºåˆ†å­ç³»ç»Ÿï¼ˆåŒ–å­¦ä¿¡æ¯å­¦çš„æ ¸å¿ƒé¢†åŸŸï¼‰çš„æ–°å‹æœºå™¨å­¦ä¹ æ¡†æ¶ï¼Œè¿™ä¸â€œåŒ–å­¦å¤§æ¨¡å‹â€ä¸»é¢˜ç›´æ¥ç›¸å…³ï¼Œæ—¨åœ¨å®ç°é«˜æ•ˆã€ç¨³å®šçš„æ¨¡æ‹Ÿã€‚

**ğŸ“– ä¸­æ–‡æ‘˜è¦**

æœ¬æ–‡æå‡ºäº†ä¸€ç§å­¦ä¹ å“ˆå¯†é¡¿æµæ˜ å°„çš„æ¡†æ¶ï¼Œç”¨äºåˆ†å­åŠ¨åŠ›å­¦ç­‰å“ˆå¯†é¡¿ç³»ç»Ÿçš„é•¿æ—¶é—´æ¼”åŒ–æ¨¡æ‹Ÿã€‚è¯¥æ–¹æ³•é€šè¿‡é¢„æµ‹é€‰å®šæ—¶é—´è·¨åº¦å†…çš„å¹³å‡ç›¸ç©ºé—´æ¼”åŒ–ï¼Œå®ç°äº†è¿œè¶…ç»å…¸ç§¯åˆ†å™¨ç¨³å®šæ€§é™åˆ¶çš„å¤§æ—¶é—´æ­¥é•¿æ›´æ–°ã€‚å…¶æ ¸å¿ƒæ˜¯æ–½åŠ äº†ä¸€ä¸ªâ€œå¹³å‡æµä¸€è‡´æ€§â€æ¡ä»¶ã€‚ä¸å…ˆå‰æ–¹æ³•ä¸åŒï¼Œè¯¥æ¡†æ¶å…è®¸åœ¨ç‹¬ç«‹ç›¸ç©ºé—´æ ·æœ¬ä¸Šè¿›è¡Œè®­ç»ƒï¼Œè€Œæ— éœ€è®¿é—®æœªæ¥çŠ¶æ€ï¼Œä»è€Œé¿å…äº†æ˜‚è´µçš„è½¨è¿¹ç”Ÿæˆã€‚è¯¥æ–¹æ³•ç‰¹åˆ«æ”¹è¿›äº†ä½¿ç”¨æœºå™¨å­¦ä¹ åŠ›åœºçš„åˆ†å­åŠ¨åŠ›å­¦æ¨¡æ‹Ÿï¼Œåœ¨ä¿æŒå¯æ¯”è¾ƒçš„è®­ç»ƒå’Œæ¨ç†æˆæœ¬çš„åŒæ—¶ï¼Œæ”¯æŒæ˜¾è‘—æ›´å¤§çš„ç§¯åˆ†æ—¶é—´æ­¥é•¿ã€‚è¿™é¡¹å·¥ä½œä¸â€œåŒ–å­¦å¤§æ¨¡å‹â€ä¸»é¢˜ç›¸å…³ï¼Œå› ä¸ºå®ƒæå‡ºäº†ä¸€ç§ç”¨äºåˆ†å­ç³»ç»Ÿï¼ˆåŒ–å­¦ä¿¡æ¯å­¦çš„æ ¸å¿ƒï¼‰çš„æ–°å‹æœºå™¨å­¦ä¹ æ¡†æ¶ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿå®ç°é«˜æ•ˆã€ç¨³å®šçš„æ¨¡æ‹Ÿï¼Œè¿™æ˜¯å¼€å‘ç”¨äºåŒ–å­¦å’Œææ–™ç§‘å­¦çš„â€œå¤§æ¨¡å‹â€çš„å…³é”®èƒ½åŠ›ã€‚

<details>
<summary><b>ğŸ” æŸ¥çœ‹åŸæ–‡æ‘˜è¦</b></summary>

Simulating the long-time evolution of Hamiltonian systems is limited by the small timesteps required for stable numerical integration. To overcome this constraint, we introduce a framework to learn Hamiltonian Flow Maps by predicting the mean phase-space evolution over a chosen time span, enabling stable large-timestep updates far beyond the stability limits of classical integrators. To this end, we impose a Mean Flow consistency condition for time-averaged Hamiltonian dynamics. Unlike prior approaches, this allows training on independent phase-space samples without access to future states, avoiding expensive trajectory generation. Validated across diverse Hamiltonian systems, our method in particular improves upon molecular dynamics simulations using machine-learned force fields (MLFF). Our models maintain comparable training and inference cost, but support significantly larger integration timesteps while trained directly on widely-available trajectory-free MLFF datasets.

</details>

---

### 35. [A Minimum Variance Path Principle for Accurate and Stable Score-Based Density Ratio Estimation](https://arxiv.org/abs/2602.00834)

**åŸºæœ¬ä¿¡æ¯**

- ğŸ”— arXiv: [`2602.00834`](https://arxiv.org/abs/2602.00834)
- ğŸ‘¥ ä½œè€…: Wei Chen, Jiacheng Li, Shigui Li ç­‰7äºº
- ğŸ“„ PDF: [ä¸‹è½½](https://arxiv.org/pdf/2602.00834.pdf)

**ğŸ’¡ ç›¸å…³æ€§åˆ†æ**

æ»¡è¶³æ ‡å‡†1ï¼šè®ºæ–‡çš„æ ¸å¿ƒç ”ç©¶å†…å®¹æ˜¯æ”¹è¿›åŸºäºåˆ†æ•°çš„ç”Ÿæˆæ¨¡å‹å’Œå¯†åº¦ä¼°è®¡æ–¹æ³•ï¼Œè¿™æ˜¯æ„å»ºåŒ–å­¦é¢†åŸŸç”Ÿæˆæ¨¡å‹ï¼ˆåŒ–å­¦å¤§æ¨¡å‹ï¼‰å’Œè¿›è¡Œæ¦‚ç‡æ¨ç†ï¼ˆå¦‚è´¨è°±ç»“æ„æ¨ç†ï¼‰çš„åŸºç¡€æŠ€æœ¯ã€‚

**ğŸ“– ä¸­æ–‡æ‘˜è¦**

æœ¬æ–‡è§£å†³äº†åŸºäºåˆ†æ•°çš„å¯†åº¦æ¯”ä¼°è®¡æ–¹æ³•ä¸­çš„ä¸€ä¸ªæ ¸å¿ƒæ‚–è®ºï¼šç†è®ºä¸Šè·¯å¾„ç‹¬ç«‹ï¼Œä½†å®è·µä¸­è·¯å¾„ä¾èµ–ã€‚ä½œè€…é€šè¿‡è¯æ˜å®é™…è®­ç»ƒç›®æ ‡ä¸ç†æƒ³ç›®æ ‡ä¹‹é—´ç›¸å·®ä¸€ä¸ªå…³é”®é¡¹â€”â€”åˆ†æ•°å‡½æ•°çš„è·¯å¾„æ–¹å·®ï¼Œä»è€Œè§£å†³äº†è¿™ä¸€é—®é¢˜ã€‚ä»–ä»¬æå‡ºäº†â€œæœ€å°æ–¹å·®è·¯å¾„â€åŸåˆ™æ¥æœ€å°åŒ–æ­¤æ–¹å·®ï¼Œå¹¶æ¨å¯¼äº†æ–¹å·®çš„é—­å¼è¡¨è¾¾å¼ï¼Œä½¿ä¼˜åŒ–å˜å¾—å¯è¡Œã€‚é€šè¿‡ä½¿ç”¨çµæ´»çš„Kumaraswamyæ··åˆæ¨¡å‹å¯¹è·¯å¾„è¿›è¡Œå‚æ•°åŒ–ï¼Œè¯¥æ–¹æ³•å¯ä»¥å­¦ä¹ æ•°æ®è‡ªé€‚åº”çš„ä½æ–¹å·®è·¯å¾„ã€‚è¿™ç§å¯¹å®Œæ•´ç›®æ ‡çš„ä¼˜åŒ–äº§ç”Ÿäº†æ›´å‡†ç¡®å’Œç¨³å®šçš„ä¼°è®¡å™¨ã€‚è¿™é¡¹å·¥ä½œä¸â€œåŒ–å­¦å¤§æ¨¡å‹â€å’Œâ€œè´¨è°±ç»“æ„æ¨ç†â€éƒ½ç›¸å…³ï¼Œå› ä¸ºåŸºäºåˆ†æ•°çš„ç”Ÿæˆæ¨¡å‹å’Œå¯†åº¦ä¼°è®¡æ˜¯æ„å»ºåŒ–å­¦é¢†åŸŸç”Ÿæˆæ¨¡å‹ï¼ˆå¦‚åˆ†å­ç”Ÿæˆï¼‰å’Œè¿›è¡Œæ¦‚ç‡æ¨ç†ï¼ˆå¦‚ä»è´¨è°±æ•°æ®æ¨æ–­ç»“æ„ï¼‰çš„æ ¸å¿ƒæŠ€æœ¯ã€‚æœ¬æ–‡æå‡ºçš„æ”¹è¿›æ–¹æ³•å¯ä»¥æå‡è¿™äº›åŒ–å­¦ä¿¡æ¯å­¦ä»»åŠ¡çš„æ¨¡å‹æ€§èƒ½ã€‚

<details>
<summary><b>ğŸ” æŸ¥çœ‹åŸæ–‡æ‘˜è¦</b></summary>

Score-based methods are powerful across machine learning, but they face a paradox: theoretically path-independent, yet practically path-dependent. We resolve this by proving that practical training objectives differ from the ideal, ground-truth objective by a crucial, overlooked term: the path variance of the score function. We propose the MVP (**M**imum **V**ariance **P**ath) Principle to minimize this path variance. Our key contribution is deriving a closed-form expression for the variance, making optimization tractable. By parameterizing the path with a flexible Kumaraswamy Mixture Model, our method learns data-adaptive, low-variance paths without heuristic manual selection. This principled optimization of the complete objective yields more accurate and stable estimators, establishing new state-of-the-art results on challenging benchmarks and providing a general framework for optimizing score-based interpolation.

</details>

---

### 36. [VQ-Style: Disentangling Style and Content in Motion with Residual Quantized Representations](https://arxiv.org/abs/2602.02334)

**åŸºæœ¬ä¿¡æ¯**

- ğŸ”— arXiv: [`2602.02334`](https://arxiv.org/abs/2602.02334)
- ğŸ‘¥ ä½œè€…: Fatemeh Zargarbashi, Dhruv Agrawal, Jakob Buhmann ç­‰6äºº
- ğŸ“„ PDF: [ä¸‹è½½](https://arxiv.org/pdf/2602.02334.pdf)

**ğŸ’¡ ç›¸å…³æ€§åˆ†æ**

æ»¡è¶³æ ‡å‡†1ï¼šè®ºæ–‡æå‡ºçš„åŸºäºRVQ-VAEçš„å±‚æ¬¡åŒ–è§£è€¦è¡¨ç¤ºå­¦ä¹ æ–¹æ³•ï¼Œå…¶æ ¸å¿ƒæ€æƒ³ä¸åŒ–å­¦ä¿¡æ¯å­¦ä¸­è§£è€¦åˆ†å­æ ¸å¿ƒç»“æ„ä¸åŠŸèƒ½åŸºå›¢çš„éœ€æ±‚é«˜åº¦ç›¸å…³ï¼Œæ˜¯æ„å»ºå’Œç†è§£â€œåŒ–å­¦å¤§æ¨¡å‹â€å†…éƒ¨è¡¨ç¤ºçš„é‡è¦æŠ€æœ¯è·¯å¾„ã€‚

**ğŸ“– ä¸­æ–‡æ‘˜è¦**

æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ–¹æ³•ï¼Œç”¨äºæœ‰æ•ˆè§£è€¦äººä½“è¿åŠ¨æ•°æ®ä¸­çš„é£æ ¼å’Œå†…å®¹ï¼Œä»¥ä¿ƒè¿›é£æ ¼è¿ç§»ã€‚è¯¥æ–¹æ³•åˆ©ç”¨æ®‹å·®å‘é‡é‡åŒ–å˜åˆ†è‡ªç¼–ç å™¨å­¦ä¹ ä»ç²—åˆ°ç»†çš„è¿åŠ¨è¡¨ç¤ºï¼Œå¹¶é€šè¿‡ç»“åˆç æœ¬å­¦ä¹ ã€å¯¹æ¯”å­¦ä¹ å’Œæ–°é¢–çš„ä¿¡æ¯æ³„æ¼æŸå¤±æ¥å¢å¼ºè§£è€¦ã€‚ä½œè€…åˆ©ç”¨è¿™ç§è§£è€¦è¡¨ç¤ºï¼Œæå‡ºäº†ä¸€ç§ç®€å•æœ‰æ•ˆçš„æ¨ç†æ—¶æŠ€æœ¯â€œé‡åŒ–ç äº¤æ¢â€ï¼Œæ— éœ€å¯¹æœªè§é£æ ¼è¿›è¡Œå¾®è°ƒå³å¯å®ç°è¿åŠ¨é£æ ¼è¿ç§»ã€‚è™½ç„¶è®ºæ–‡åº”ç”¨åœ¨äººä½“è¿åŠ¨é¢†åŸŸï¼Œä½†å…¶æ ¸å¿ƒæ–¹æ³•è®ºâ€”â€”ä½¿ç”¨RVQ-VAEè¿›è¡Œå±‚æ¬¡åŒ–è¡¨ç¤ºå­¦ä¹ ä»¥è§£è€¦é«˜çº§è¯­ä¹‰ï¼ˆå†…å®¹ï¼‰å’Œä½çº§ç»†èŠ‚ï¼ˆé£æ ¼ï¼‰â€”â€”ä¸åŒ–å­¦ä¿¡æ¯å­¦ä¸­åˆ†å­è¡¨ç¤ºå­¦ä¹ çš„ç›®æ ‡é«˜åº¦ç›¸ä¼¼ã€‚åœ¨åŒ–å­¦é¢†åŸŸï¼Œç±»ä¼¼æŠ€æœ¯å¯ç”¨äºè§£è€¦åˆ†å­çš„æ ¸å¿ƒéª¨æ¶ï¼ˆå†…å®¹ï¼‰å’Œå®˜èƒ½å›¢/å–ä»£åŸºï¼ˆé£æ ¼ï¼‰ï¼Œè¿™å¯¹äºåˆ†å­ç”Ÿæˆã€ä¼˜åŒ–å’Œæ€§è´¨é¢„æµ‹è‡³å…³é‡è¦ï¼Œæ˜¯â€œåŒ–å­¦å¤§æ¨¡å‹â€ç ”ç©¶çš„å‰æ²¿æ–¹å‘ã€‚

<details>
<summary><b>ğŸ” æŸ¥çœ‹åŸæ–‡æ‘˜è¦</b></summary>

Human motion data is inherently rich and complex, containing both semantic content and subtle stylistic features that are challenging to model. We propose a novel method for effective disentanglement of the style and content in human motion data to facilitate style transfer. Our approach is guided by the insight that content corresponds to coarse motion attributes while style captures the finer, expressive details. To model this hierarchy, we employ Residual Vector Quantized Variational Autoencoders (RVQ-VAEs) to learn a coarse-to-fine representation of motion. We further enhance the disentanglement by integrating codebook learning with contrastive learning and a novel information leakage loss to organize the content and the style across different codebooks. We harness this disentangled representation using our simple and effective inference-time technique Quantized Code Swapping, which enables motion style transfer without requiring any fine-tuning for unseen styles. Our framework demonstrates strong versatility across multiple inference applications, including style transfer, style removal, and motion blending.

</details>

---

### 37. [Enabling Large-Scale Channel Sounding for 6G: A Framework for Sparse Sampling and Multipath Component Extraction](https://arxiv.org/abs/2602.05405)

**åŸºæœ¬ä¿¡æ¯**

- ğŸ”— arXiv: [`2602.05405`](https://arxiv.org/abs/2602.05405)
- ğŸ‘¥ ä½œè€…: Yi Chen, Ming Li, Chong Han
- ğŸ“„ PDF: [ä¸‹è½½](https://arxiv.org/pdf/2602.05405.pdf)

**ğŸ’¡ ç›¸å…³æ€§åˆ†æ**

æ»¡è¶³æ ‡å‡†1ï¼šè®ºæ–‡æå‡ºçš„ç”¨äºä»éå‡åŒ€é‡‡æ ·æ•°æ®ä¸­æå–å¤šå¾„åˆ†é‡çš„LR-SAGEç®—æ³•ï¼Œå…¶æ ¸å¿ƒæ–¹æ³•è®ºä¸â€œè´¨è°±ç»“æ„æ¨ç†â€ä¸­ä»å¤æ‚è´¨è°±ä¿¡å·ä¸­è§£æç¢ç‰‡ç¦»å­æ¨¡å¼çš„ä»»åŠ¡ç›´æ¥ç›¸å…³ï¼Œä¸ºè§£å†³è´¨è°±æ•°æ®åˆ†æä¸­çš„ç±»ä¼¼é—®é¢˜æä¾›äº†æ–°çš„æŠ€æœ¯æ€è·¯ã€‚

**ğŸ“– ä¸­æ–‡æ‘˜è¦**

æœ¬æ–‡æå‡ºäº†ä¸€ç§ç”¨äº6Gå¤§è§„æ¨¡ä¿¡é“æµ‹é‡çš„æ–°æ¡†æ¶ï¼Œæ¶‰åŠç¨€ç–éå‡åŒ€é‡‡æ ·å’Œä¸€ç§ç”¨äºå¤šå¾„åˆ†é‡æå–çš„ä¼¼ç„¶æ ¡æ­£ç©ºé—´äº¤æ›¿å¹¿ä¹‰æœŸæœ›æœ€å¤§åŒ–ç®—æ³•ã€‚è¯¥æ¡†æ¶èƒ½å¤Ÿåœ¨ç›¸åŒæµ‹é‡æ—¶é—´å†…è·å–å¤§æ•°åç”šè‡³æ•°ç™¾å€çš„ä¿¡é“æ•°æ®é›†ï¼Œä¸ºåˆ©ç”¨AIç¼©æ”¾å®šå¾‹æä¾›æ‰€éœ€çš„æµ·é‡æ•°æ®ã€‚å…·ä½“è€Œè¨€ï¼Œä½œè€…æå‡ºäº†æŠ›ç‰©çº¿é¢‘ç‡é‡‡æ ·ç­–ç•¥å’Œéå‡åŒ€é‡‡æ ·ä¸‹çš„LR-SAGEç®—æ³•ã€‚è¿™é¡¹å·¥ä½œä¸â€œè´¨è°±ç»“æ„æ¨ç†â€åœ¨æ–¹æ³•è®ºä¸Šé«˜åº¦ç›¸å…³ã€‚è´¨è°±åˆ†æï¼Œå°¤å…¶æ˜¯ä¸²è”è´¨è°±ï¼Œå…¶æ ¸å¿ƒä»»åŠ¡ä¹‹ä¸€å°±æ˜¯ä»å¤æ‚çš„è°±å›¾ä¸­æå–å’Œè§£æç¢ç‰‡ç¦»å­ä¿¡å·ï¼ˆç±»ä¼¼äºé€šä¿¡ä¸­çš„å¤šå¾„åˆ†é‡ï¼‰ï¼Œä»¥æ¨æ–­åˆ†å­ç»“æ„ã€‚æœ¬æ–‡æå‡ºçš„é’ˆå¯¹éå‡åŒ€é‡‡æ ·æ•°æ®çš„ã€é«˜æ•ˆçš„MPCæå–ç®—æ³•ï¼ˆLR-SAGEï¼‰ï¼Œå…¶æ€æƒ³å¯ä»¥è¿ç§»åˆ°è´¨è°±æ•°æ®åˆ†æä¸­ï¼Œç”¨äºä»é«˜åˆ†è¾¨ç‡è´¨è°±æ•°æ®ä¸­æ›´é²æ£’ã€æ›´é«˜æ•ˆåœ°è§£æå‡ºç¢ç‰‡ç¦»å­å³°åŠå…¶å…³ç³»ï¼Œä»è€Œè¾…åŠ©â€œè´¨è°±ç»“æ„æ¨ç†â€ã€‚

<details>
<summary><b>ğŸ” æŸ¥çœ‹åŸæ–‡æ‘˜è¦</b></summary>

Realizing the 6G vision of artificial intelligence (AI) and integrated sensing and communication (ISAC) critically requires large-scale real-world channel datasets for channel modeling and data-driven AI models. However, traditional frequency-domain channel sounding methods suffer from low efficiency due to a prohibitive number of frequency points to avoid delay ambiguity. This paper proposes a novel channel sounding framework involving sparse nonuniform sampling along with a likelihood-rectified space-alternating generalized expectation-maximization (LR-SAGE) algorithm for multipath component extraction. This framework enables the acquisition of channel datasets that are tens or even hundreds of times larger within the same channel measurement duration, thereby providing the massive data required to harness the full potential of AI scaling laws. Specifically, we propose a Parabolic Frequency Sampling (PFS) strategy that non-uniformly distributes frequency points, effectively eliminating delay ambiguity while reducing sampling overhead by orders of magnitude. To efficiently extract multipath components (MPCs) from the channel data measured by PFS, we develop a LR-SAGE algorithm, rectifying the likelihood distortion caused by nonuniform sampling and molecular absorption effect. Simulation results and experimental validation at 280--300~GHz confirm that the proposed PFS and LR-SAGE algorithm not only achieve 50$\times$ faster measurement, a 98\% reduction in data volume and a 99.96\% reduction in post-processing computational complexity, but also successfully captures MPCs and channel characteristics consistent with traditional exhaustive measurements, demonstrating its potential as a fundamental enabler for constructing the massive ISAC datasets required by AI-native 6G systems.

</details>

---

### 38. [Document Reconstruction Unlocks Scalable Long-Context RLVR](https://arxiv.org/abs/2602.08237)

**åŸºæœ¬ä¿¡æ¯**

- ğŸ”— arXiv: [`2602.08237`](https://arxiv.org/abs/2602.08237)
- ğŸ‘¥ ä½œè€…: Yao Xiao, Lei Wang, Yue Deng ç­‰9äºº
- ğŸ“„ PDF: [ä¸‹è½½](https://arxiv.org/pdf/2602.08237.pdf)

**ğŸ’¡ ç›¸å…³æ€§åˆ†æ**

æ»¡è¶³æ ‡å‡†1ï¼šè®ºæ–‡æå‡ºçš„æ— ç›‘ç£é•¿ä¸Šä¸‹æ–‡è®­ç»ƒæ–¹æ³•ï¼ˆé€šè¿‡æ–‡æ¡£é‡å»ºï¼‰ï¼Œä¸ºè§£å†³â€œåŒ–å­¦å¤§æ¨¡å‹â€ä¸­å¤„ç†é•¿åºåˆ—åŒ–å­¦æ•°æ®ï¼ˆå¦‚è›‹ç™½è´¨åºåˆ—ã€æ–‡çŒ®ï¼‰çš„æ ¸å¿ƒæŒ‘æˆ˜æä¾›äº†åˆ›æ–°çš„è®­ç»ƒèŒƒå¼å’ŒæŠ€æœ¯æ€è·¯ã€‚

**ğŸ“– ä¸­æ–‡æ‘˜è¦**

æœ¬æ–‡ç ”ç©¶äº†ä¸€ç§æ— ç›‘ç£æ–¹æ³•æ¥å¢å¼ºå¤§è¯­è¨€æ¨¡å‹çš„é•¿ä¸Šä¸‹æ–‡èƒ½åŠ›ï¼Œæ— éœ€æ˜‚è´µçš„äººå·¥æ ‡æ³¨æˆ–æ•™å¸ˆæ¨¡å‹ç›‘ç£ã€‚å…·ä½“æ–¹æ³•æ˜¯ï¼šåœ¨ä¸€ç¯‡é•¿æ–‡æ¡£ä¸­ï¼Œç”¨ç‰¹æ®Šå ä½ç¬¦æ›¿æ¢å°‘æ•°æ®µè½ï¼Œç„¶åé€šè¿‡å¼ºåŒ–å­¦ä¹ è®­ç»ƒLLMï¼Œé€šè¿‡ä»ä¸€ç»„å€™é€‰é€‰é¡¹ä¸­æ­£ç¡®è¯†åˆ«å’Œæ’åºç¼ºå¤±æ®µè½æ¥é‡å»ºæ–‡æ¡£ã€‚è¿™ç§è®­ç»ƒèŒƒå¼ä½¿æ¨¡å‹èƒ½å¤Ÿæ•æ‰å…¨å±€å™äº‹è¿è´¯æ€§ï¼Œä»è€Œæ˜¾è‘—æå‡é•¿ä¸Šä¸‹æ–‡æ€§èƒ½ã€‚è¯¥æ–¹æ³•åœ¨RULERå’ŒLongBench v2åŸºå‡†æµ‹è¯•ä¸ŠéªŒè¯äº†æœ‰æ•ˆæ€§ã€‚è¿™é¡¹å·¥ä½œä¸â€œåŒ–å­¦å¤§æ¨¡å‹â€ç›¸å…³ï¼Œå› ä¸ºå¤„ç†é•¿åºåˆ—åŒ–å­¦æ•°æ®ï¼ˆå¦‚é•¿è›‹ç™½è´¨åºåˆ—ã€é«˜åˆ†å­èšåˆç‰©ç»“æ„ã€å¤æ‚çš„ååº”è·¯å¾„æè¿°ï¼‰æ˜¯åŒ–å­¦é¢†åŸŸå¤§æ¨¡å‹é¢ä¸´çš„å…³é”®æŒ‘æˆ˜ä¹‹ä¸€ã€‚æœ¬æ–‡æå‡ºçš„é€šè¿‡æ–‡æ¡£é‡å»ºä»»åŠ¡è¿›è¡Œæ— ç›‘ç£é•¿ä¸Šä¸‹æ–‡è®­ç»ƒçš„æ–¹æ³•ï¼Œä¸ºè®­ç»ƒåŒ–å­¦é¢†åŸŸå¤§æ¨¡å‹å¤„ç†é•¿åºåˆ—åŒ–å­¦ä¿¡æ¯ï¼ˆå¦‚ä»é•¿æ–‡çŒ®ä¸­æå–ååº”è§„åˆ™æˆ–è§£æé•¿ç”Ÿç‰©åˆ†å­åºåˆ—ï¼‰æä¾›äº†æ–°çš„ã€å¯æ‰©å±•çš„è®­ç»ƒèŒƒå¼ã€‚

<details>
<summary><b>ğŸ” æŸ¥çœ‹åŸæ–‡æ‘˜è¦</b></summary>

Reinforcement Learning with Verifiable Rewards~(RLVR) has become a prominent paradigm to enhance the capabilities (i.e.\ long-context) of Large Language Models~(LLMs). However, it often relies on gold-standard answers or explicit evaluation rubrics provided by powerful teacher models or human experts, which are costly and time-consuming. In this work, we investigate unsupervised approaches to enhance the long-context capabilities of LLMs, eliminating the need for heavy human annotations or teacher models' supervision. Specifically, we first replace a few paragraphs with special placeholders in a long document. LLMs are trained through reinforcement learning to reconstruct the document by correctly identifying and sequencing missing paragraphs from a set of candidate options. This training paradigm enables the model to capture global narrative coherence, significantly boosting long-context performance. We validate the effectiveness of our method on two widely used benchmarks, RULER and LongBench~v2. While acquiring noticeable gains on RULER, it can also achieve a reasonable improvement on LongBench~v2 without any manually curated long-context QA data. Furthermore, we conduct extensive ablation studies to analyze the impact of reward design, data curation strategies, training schemes, and data scaling effects on model performance. We publicly release our code, data, and models.

</details>

---

### 39. [When Less is More: The LLM Scaling Paradox in Context Compression](https://arxiv.org/abs/2602.09789)

**åŸºæœ¬ä¿¡æ¯**

- ğŸ”— arXiv: [`2602.09789`](https://arxiv.org/abs/2602.09789)
- ğŸ‘¥ ä½œè€…: Ruishan Guo, Yibing Liu, Guoxin Ma ç­‰9äºº
- ğŸ“„ PDF: [ä¸‹è½½](https://arxiv.org/pdf/2602.09789.pdf)

**ğŸ’¡ ç›¸å…³æ€§åˆ†æ**

æ»¡è¶³æ ‡å‡†1ï¼šè®ºæ–‡æ·±å…¥ç ”ç©¶äº†å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ä¸Šä¸‹æ–‡å‹ç¼©ä»»åŠ¡ä¸­çš„â€œå°ºå¯¸-ä¿çœŸåº¦æ‚–è®ºâ€ï¼Œå…¶å‘ç°å’Œæœºç†åˆ†æï¼ˆçŸ¥è¯†è¦†ç›–ã€è¯­ä¹‰æ¼‚ç§»ï¼‰å¯¹äºç†è§£å’Œè¯„ä¼°â€œåŒ–å­¦å¤§æ¨¡å‹â€åœ¨å‹ç¼©åŒ–å­¦çŸ¥è¯†å¹¶ä¿æŒæ¨ç†å¿ å®æ€§æ–¹é¢çš„èƒ½åŠ›å…·æœ‰ç›´æ¥çš„ç›¸å…³æ€§å’Œé‡è¦çš„æŒ‡å¯¼æ„ä¹‰ã€‚

**ğŸ“– ä¸­æ–‡æ‘˜è¦**

æœ¬æ–‡ç ”ç©¶äº†åœ¨å‹ç¼©å™¨-è§£ç å™¨è®¾ç½®ä¸‹çš„æœ‰æŸä¸Šä¸‹æ–‡å‹ç¼©ä¸­å‡ºç°çš„â€œå°ºå¯¸-ä¿çœŸåº¦æ‚–è®ºâ€ï¼šå¢åŠ å‹ç¼©å™¨çš„å¤§å°å¯èƒ½ä¼šé™ä½é‡å»ºä¸Šä¸‹æ–‡çš„å¿ å®åº¦ï¼Œå°½ç®¡è®­ç»ƒæŸå¤±åœ¨ä¸‹é™ã€‚é€šè¿‡å¯¹ä»0.6Båˆ°90Bçš„æ¨¡å‹è¿›è¡Œå¹¿æ³›å®éªŒï¼Œä½œè€…å°†è¿™ä¸€æ‚–è®ºå½’å› äºä¸¤ä¸ªä¸»è¦å› ç´ ï¼š1ï¼‰çŸ¥è¯†è¦†ç›–ï¼šæ›´å¤§çš„æ¨¡å‹è¶Šæ¥è¶Šå¤šåœ°ç”¨å…¶å…ˆéªŒä¿¡å¿µæ›¿æ¢æºäº‹å®ï¼›2ï¼‰è¯­ä¹‰æ¼‚ç§»ï¼šæ›´å¤§çš„æ¨¡å‹å€¾å‘äºæ„è¯‘æˆ–é‡ç»„å†…å®¹ï¼Œè€Œä¸æ˜¯é€å­—å¤ç°ã€‚é€šè¿‡å›ºå®šæ¨¡å‹å¤§å°ï¼Œä½œè€…åæ€äº†å‹ç¼©ä¸Šä¸‹æ–‡è¡¨ç¤ºçš„æ¶Œç°ç‰¹æ€§ã€‚è¿™é¡¹å·¥ä½œä¸â€œåŒ–å­¦å¤§æ¨¡å‹â€é«˜åº¦ç›¸å…³ã€‚åœ¨åŒ–å­¦é¢†åŸŸï¼Œå¤§æ¨¡å‹ç»å¸¸è¢«ç”¨äºå‹ç¼©å’Œè¡¨ç¤ºå¤æ‚çš„åŒ–å­¦çŸ¥è¯†ï¼ˆå¦‚åˆ†å­ç»“æ„ã€ååº”è§„åˆ™ï¼‰ï¼Œç„¶åç”¨äºä¸‹æ¸¸æ¨ç†ï¼ˆå¦‚é€†åˆæˆè§„åˆ’ã€æ€§è´¨é¢„æµ‹ï¼‰ã€‚æœ¬æ–‡æ­ç¤ºçš„â€œå°ºå¯¸-ä¿çœŸåº¦æ‚–è®ºâ€åŠå…¶æœºç†ï¼ˆçŸ¥è¯†è¦†ç›–ã€è¯­ä¹‰æ¼‚ç§»ï¼‰ï¼Œå¯¹äºç†è§£å’Œè¯„ä¼°åŒ–å­¦å¤§æ¨¡å‹åœ¨çŸ¥è¯†å‹ç¼©ä¸å¿ å®é‡å»ºæ–¹é¢çš„èƒ½åŠ›å…·æœ‰é‡è¦å¯ç¤ºï¼Œæœ‰åŠ©äºè®¾è®¡æ›´å¯é ã€æ›´å¿ å®çš„åŒ–å­¦çŸ¥è¯†è¡¨ç¤ºä¸æ¨ç†æ¨¡å‹ã€‚

<details>
<summary><b>ğŸ” æŸ¥çœ‹åŸæ–‡æ‘˜è¦</b></summary>

Scaling up model parameters has long been a prevalent training paradigm driven by the assumption that larger models yield superior generation capabilities. However, under lossy context compression in a compressor-decoder setup, we observe a Size-Fidelity Paradox: increasing the compressor size can lessen the faithfulness of reconstructed contexts though training loss decreases. Through extensive experiments across models from 0.6B to 90B, we coin this paradox arising from two dominant factors: 1) knowledge overwriting: larger models increasingly replace source facts with their own prior beliefs, e.g., ``the white strawberry'' $\to$ ``the red strawberry''; and 2) semantic drift: larger models tend to paraphrase or restructure content instead of reproducing it verbatim, e.g., ``Alice hit Bob'' $\to$ ``Bob hit Alice''. By holding model size fixed, we reflect on the emergent properties of compressed context representations. We show that the culprit is not parameter count itself, but the excessive semantic capacity and amplified generative uncertainty that accompany scaling. Specifically, the increased rank of context embeddings facilitates prior knowledge intrusion, whereas higher entropy over token prediction distributions promotes rewriting. Our results complement existing evaluations over context compression paradigm, underpinning a breakdown in scaling laws for faithful preservation in open-ended generation.

</details>

---

### 40. [Versor: A Geometric Sequence Architecture](https://arxiv.org/abs/2602.10195)

**åŸºæœ¬ä¿¡æ¯**

- ğŸ”— arXiv: [`2602.10195`](https://arxiv.org/abs/2602.10195)
- ğŸ‘¥ ä½œè€…: Truong Minh Huy, Edward Hirst
- ğŸ“„ PDF: [ä¸‹è½½](https://arxiv.org/pdf/2602.10195.pdf)

**ğŸ’¡ ç›¸å…³æ€§åˆ†æ**

æ»¡è¶³æ ‡å‡†1ï¼šè®ºæ–‡æå‡ºçš„Versoræ¶æ„åŸç”Ÿæ”¯æŒSE(3)ç­‰å˜æ€§ï¼Œå¹¶æä¾›äº†å¼ºå¤§çš„å‡ ä½•å…³ç³»å»ºæ¨¡å·¥å…·ï¼Œè¿™ä¸â€œåŒ–å­¦å¤§æ¨¡å‹â€ï¼ˆç”¨äºåˆ†å­ä¸‰ç»´ç»“æ„å»ºæ¨¡ï¼‰å’Œâ€œè´¨è°±ç»“æ„æ¨ç†â€ï¼ˆæ¶‰åŠä¸‰ç»´ç©ºé—´ä¸­çš„åŒ–å­¦é”®æ–­è£‚ï¼‰çš„æ ¸å¿ƒéœ€æ±‚ç›´æ¥ç›¸å…³ï¼Œä¸ºè¿™äº›é¢†åŸŸæä¾›äº†åˆ›æ–°çš„æ¨¡å‹æ¶æ„é€‰æ‹©ã€‚

**ğŸ“– ä¸­æ–‡æ‘˜è¦**

æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°é¢–çš„åºåˆ—æ¶æ„Versorï¼Œå®ƒä½¿ç”¨å…±å½¢å‡ ä½•ä»£æ•°ä»£æ›¿ä¼ ç»Ÿçš„çº¿æ€§æ“ä½œï¼Œä»¥å®ç°ç»“æ„æ³›åŒ–å¹¶åœ¨å¤šç§ä»»åŠ¡ä¸Šè·å¾—æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚Versorå°†çŠ¶æ€åµŒå…¥Cl_{4,1}æµå½¢å¹¶é€šè¿‡å‡ ä½•å˜æ¢æ¼”åŒ–å®ƒä»¬ï¼ŒåŸç”Ÿè¡¨ç¤ºSE(3)ç­‰å˜å…³ç³»ã€‚è¯¥æ¨¡å‹åœ¨æ··æ²ŒNä½“åŠ¨åŠ›å­¦ã€æ‹“æ‰‘æ¨ç†å’Œå¤šæ¨¡æ€åŸºå‡†æµ‹è¯•ä¸Šè¿›è¡Œäº†éªŒè¯ã€‚è¿™é¡¹å·¥ä½œä¸â€œåŒ–å­¦å¤§æ¨¡å‹â€å’Œâ€œè´¨è°±ç»“æ„æ¨ç†â€éƒ½é«˜åº¦ç›¸å…³ã€‚é¦–å…ˆï¼ŒVersoråŸç”Ÿæ”¯æŒSE(3)ç­‰å˜æ€§ï¼Œè¿™æ˜¯åˆ†å­å»ºæ¨¡ï¼ˆå¦‚åˆ†å­æ„è±¡ã€è›‹ç™½è´¨ç»“æ„ï¼‰å’Œè´¨è°±æ¨ç†ï¼ˆæ¶‰åŠä¸‰ç»´ç©ºé—´ä¸­çš„ç¢ç‰‡åŒ–è¿‡ç¨‹ï¼‰çš„å…³é”®å±æ€§ã€‚å…¶æ¬¡ï¼Œå…¶å‡ ä½•ç§¯æ³¨æ„åŠ›æœºåˆ¶å’Œé€’å½’è½¬å­ç´¯åŠ å™¨ä¸ºå»ºæ¨¡åˆ†å­å†…åŸå­é—´çš„å¤æ‚ç©ºé—´å…³ç³»å’Œè´¨è°±ç¢ç‰‡ç¦»å­çš„ç”Ÿæˆè·¯å¾„æä¾›äº†å¼ºå¤§çš„æ–°å·¥å…·ã€‚Versorå±•ç¤ºçš„å“è¶Šçš„OODæ³›åŒ–èƒ½åŠ›å’Œå¯è§£é‡Šæ€§ï¼Œä½¿å…¶æˆä¸ºå¼€å‘ä¸‹ä¸€ä»£åŒ–å­¦å’Œè´¨è°±åˆ†æåŸºç¡€æ¨¡å‹çš„æå…·æ½œåŠ›çš„æ¶æ„ã€‚

<details>
<summary><b>ğŸ” æŸ¥çœ‹åŸæ–‡æ‘˜è¦</b></summary>

A novel sequence architecture is introduced, Versor, which uses Conformal Geometric Algebra (CGA) in place of traditional linear operations to achieve structural generalization and significant performance improvements on a variety of tasks, while offering improved interpretability and efficiency. By embedding states in the $Cl_{4,1}$ manifold and evolving them via geometric transformations (rotors), Versor natively represents $SE(3)$-equivariant relationships without requiring explicit structural encoding. Versor is validated on chaotic N-body dynamics, topological reasoning, and standard multimodal benchmarks (CIFAR-10, WikiText-103), consistently outperforming Transformers, Graph Networks, and geometric baselines (GATr, EGNN). Key results include: orders-of-magnitude fewer parameters ($200\times$ vs. Transformers); interpretable attention decomposing into proximity and orientational components; zero-shot scale generalization (0.993 vs. 0.070 MCC for ViT); and featuring a Recursive Rotor Accumulator (RRA) for $O(L)$ linear temporal complexity in dynamical systems, and a Geometric Product Attention (GPA) mechanism for $O(L^{2})$ global relational modeling, allowing for task-specific architectural pruning or hybridization depending on the required scale. In out-of-distribution tests, Versor maintains stable predictions while Transformers fail catastrophically. Custom Clifford kernels achieve a cumulative over $100\times$ speedup via bit-masked contraction and specialized Matrix Isomorphism kernels, reducing per-step latency to 1.05 ms and outperforming highly-optimized Transformer baselines.

</details>

---

### 41. [Symmetry in language statistics shapes the geometry of model representations](https://arxiv.org/abs/2602.15029)

**åŸºæœ¬ä¿¡æ¯**

- ğŸ”— arXiv: [`2602.15029`](https://arxiv.org/abs/2602.15029)
- ğŸ‘¥ ä½œè€…: Dhruva Karkada, Daniel J. Korchinski, Andres Nava ç­‰5äºº
- ğŸ“„ PDF: [ä¸‹è½½](https://arxiv.org/pdf/2602.15029.pdf)

**ğŸ’¡ ç›¸å…³æ€§åˆ†æ**

æ»¡è¶³æ ‡å‡†1ï¼šè®ºæ–‡ä»æ•°æ®ç»Ÿè®¡å¯¹ç§°æ€§çš„è§’åº¦ï¼Œä¸ºè¯­è¨€æ¨¡å‹å†…éƒ¨è¡¨ç¤ºçš„å‡ ä½•ç»“æ„æä¾›äº†ç†è®ºè§£é‡Šã€‚è¿™ä¸€ç†è®ºæ¡†æ¶å¯ç›´æ¥åº”ç”¨äºåˆ†æå’Œè®¾è®¡â€œåŒ–å­¦å¤§æ¨¡å‹â€çš„å†…éƒ¨è¡¨ç¤ºï¼Œå› ä¸ºåŒ–å­¦æ•°æ®æœ¬èº«å…·æœ‰ä¸°å¯Œçš„å¯¹ç§°æ€§å’Œå‡ ä½•ç»“æ„ï¼Œå¯¹äºæå‡æ¨¡å‹çš„å¯è§£é‡Šæ€§å’Œæ³›åŒ–èƒ½åŠ›è‡³å…³é‡è¦ã€‚

**ğŸ“– ä¸­æ–‡æ‘˜è¦**

æœ¬æ–‡è§£é‡Šäº†è¯­è¨€æ¨¡å‹å†…éƒ¨è¡¨ç¤ºä¸­å‡ºç°çš„å‡ ä½•ç»“æ„ï¼ˆå¦‚æœˆä»½æ’åˆ—æˆåœ†ã€å¹´ä»½å½¢æˆæµå½¢ï¼‰çš„èµ·æºã€‚ä½œè€…é¦–å…ˆè¯æ˜è¯­è¨€ç»Ÿè®¡å…·æœ‰å¹³ç§»å¯¹ç§°æ€§ï¼ˆä¾‹å¦‚ï¼Œä»»æ„ä¸¤ä¸ªæœˆåœ¨æ–‡æœ¬ä¸­å…±åŒå‡ºç°çš„é¢‘ç‡ä»…å–å†³äºå®ƒä»¬ä¹‹é—´çš„æ—¶é—´é—´éš”ï¼‰ã€‚ä»–ä»¬è¯æ˜äº†è¿™ç§å¯¹ç§°æ€§æ”¯é…ç€é«˜ç»´è¯åµŒå…¥æ¨¡å‹ä¸­çš„è¿™äº›å‡ ä½•ç»“æ„ï¼Œå¹¶è§£æåœ°æ¨å¯¼äº†è¯è¡¨ç¤ºçš„æµå½¢å‡ ä½•ã€‚è¿™äº›é¢„æµ‹ä¸å¤§å‹æ–‡æœ¬åµŒå…¥æ¨¡å‹å’Œå¤§å‹è¯­è¨€æ¨¡å‹çš„å®è¯ç»“æœç›¸åŒ¹é…ã€‚æ­¤å¤–ï¼Œå³ä½¿ç›¸å…³ç»Ÿè®¡å—åˆ°æ‰°åŠ¨ï¼Œè¡¨ç¤ºå‡ ä½•åœ¨ä¸­ç­‰åµŒå…¥ç»´åº¦ä¸‹ä»ç„¶ä¿æŒç¨³å¥ã€‚è¿™é¡¹å·¥ä½œä¸â€œåŒ–å­¦å¤§æ¨¡å‹â€é«˜åº¦ç›¸å…³ã€‚åŒ–å­¦é¢†åŸŸçš„æ•°æ®ï¼ˆå¦‚åˆ†å­ã€ååº”ï¼‰ä¹Ÿè•´å«ç€ä¸°å¯Œçš„å¯¹ç§°æ€§å’Œå‡ ä½•ç»“æ„ï¼ˆå¦‚æ—‹è½¬ã€åå°„å¯¹ç§°æ€§ï¼Œä»¥åŠå‘¨æœŸè¡¨ä¸­çš„å‘¨æœŸæ€§ï¼‰ã€‚æœ¬æ–‡æä¾›çš„ç†è®ºæ¡†æ¶â€”â€”ä»æ•°æ®ç»Ÿè®¡çš„å¯¹ç§°æ€§æ¨å¯¼å‡ºè¡¨ç¤ºå‡ ä½•â€”â€”ä¸ºç†è§£å’Œè®¾è®¡åŒ–å­¦é¢†åŸŸå¤§æ¨¡å‹çš„å†…éƒ¨è¡¨ç¤ºæä¾›äº†å¼ºå¤§çš„ç†è®ºå·¥å…·ã€‚ä¾‹å¦‚ï¼Œå¯ä»¥ç ”ç©¶åˆ†å­æè¿°ç¬¦æˆ–ååº”æ¡ä»¶åœ¨æ¨¡å‹è¡¨ç¤ºç©ºé—´ä¸­æ˜¯å¦ä¹Ÿå½¢æˆäº†æœ‰æ„ä¹‰çš„å‡ ä½•æµå½¢ï¼Œä»è€Œæå‡æ¨¡å‹çš„å¯è§£é‡Šæ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚

<details>
<summary><b>ğŸ” æŸ¥çœ‹åŸæ–‡æ‘˜è¦</b></summary>

The internal representations learned by language models consistently exhibit striking geometric structure: calendar months organize into a circle, historical years form a smooth one-dimensional manifold, and cities' latitudes and longitudes can be decoded using a linear probe. To explain this neural code, we first show that language statistics exhibit translation symmetry (for example, the frequency with which any two months co-occur in text depends only on the time interval between them). We prove that this symmetry governs these geometric structures in high-dimensional word embedding models, and we analytically derive the manifold geometry of word representations. These predictions empirically match large text embedding models and large language models. Moreover, the representational geometry persists at moderate embedding dimension even when the relevant statistics are perturbed (e.g., by removing all sentences in which two months co-occur). We prove that this robustness emerges naturally when the co-occurrence statistics are controlled by an underlying latent variable. These results suggest that representational manifolds have a universal origin: symmetry in the statistics of natural data.

</details>

---

### 42. [Composable and adaptive design of machine learning interatomic potentials guided by Fisher-information analysis](https://arxiv.org/abs/2504.19372)

**åŸºæœ¬ä¿¡æ¯**

- ğŸ”— arXiv: [`2504.19372`](https://arxiv.org/abs/2504.19372)
- ğŸ‘¥ ä½œè€…: Weishi Wang, Mark K. Transtrum, Vincenzo Lordi ç­‰5äºº
- ğŸ“„ PDF: [ä¸‹è½½](https://arxiv.org/pdf/2504.19372.pdf)

**ğŸ’¡ ç›¸å…³æ€§åˆ†æ**

æ»¡è¶³æ ‡å‡†1ï¼šè®ºæ–‡çš„æ ¸å¿ƒç ”ç©¶å†…å®¹æ˜¯å¼€å‘ä¸€ç§è‡ªé€‚åº”ã€å¯è§£é‡Šçš„æœºå™¨å­¦ä¹ åŸå­é—´åŠ¿ï¼ˆMLIPï¼‰è®¾è®¡æ¡†æ¶ã€‚MLIPæ˜¯åŒ–å­¦ä¿¡æ¯å­¦å’Œè®¡ç®—åŒ–å­¦ä¸­ç”¨äºæ¨¡æ‹Ÿåˆ†å­å’Œææ–™æ€§è´¨çš„æ ¸å¿ƒâ€œåŒ–å­¦å¤§æ¨¡å‹â€ä¹‹ä¸€ã€‚æœ¬æ–‡æå‡ºçš„ç­–ç•¥æ—¨åœ¨é€šè¿‡è¿­ä»£é‡æ„å’Œè´¹èˆå°”ä¿¡æ¯åˆ†ææ¥æ”¹è¿›MLIPçš„è®¾è®¡ï¼Œç›´æ¥å›´ç»•â€œåŒ–å­¦å¤§æ¨¡å‹â€çš„æ„å»ºã€ä¼˜åŒ–å’Œå¯è§£é‡Šæ€§å±•å¼€ã€‚

**ğŸ“– ä¸­æ–‡æ‘˜è¦**

æœ¬æ–‡æå‡ºäº†ä¸€ç§ç”¨äºæœºå™¨å­¦ä¹ åŸå­é—´åŠ¿ï¼ˆMLIPsï¼‰çš„è‡ªé€‚åº”ã€ç‰©ç†å¯å‘çš„æ¨¡å‹è®¾è®¡ç­–ç•¥ã€‚è¯¥ç­–ç•¥ä¾èµ–äºä»å•æœ¯è¯­æ¨¡å‹è¿­ä»£é‡æ„å¤åˆæ¨¡å‹ï¼Œå¹¶é‡‡ç”¨ç»Ÿä¸€çš„è®­ç»ƒç¨‹åºã€‚ä¸ºäº†æŒ‡å¯¼æ¨¡å‹é‡æ„å’Œè¶…å‚æ•°ä¼˜åŒ–ï¼Œä½œè€…æå‡ºäº†ä¸€ç§åŸºäºè´¹èˆå°”ä¿¡æ¯çŸ©é˜µï¼ˆFIMï¼‰å’Œå¤šå±æ€§è¯¯å·®åº¦é‡çš„æ¨¡å‹è¯„ä¼°æ–¹æ³•ã€‚é€šè¿‡ç»“åˆé‡æ„å’Œè¯„ä¼°å­ç¨‹åºï¼Œè¯¥æ¡†æ¶åœ¨çµæ´»æ€§å’Œå¯æ‰©å±•æ€§ä¹‹é—´å–å¾—äº†å¹³è¡¡ã€‚åœ¨ä¸€ä¸ªé’ˆå¯¹ç»“æ„å¤šæ ·çš„é“Œæ•°æ®é›†çš„æ¡ˆä¾‹ç ”ç©¶ä¸­ï¼Œè¯¥æ¡†æ¶ç”Ÿæˆçš„åŒ…å«75ä¸ªå‚æ•°çš„æœ€ä¼˜æ¨¡å‹é…ç½®ï¼Œå®ç°äº†0.172 eV/Ã…çš„åŠ›RMSEå’Œ0.013 eV/atomçš„èƒ½é‡RMSEã€‚è¿™é¡¹å·¥ä½œå±•ç¤ºäº†é€šè¿‡è¿­ä»£å¼ã€ä¿¡æ¯é©±åŠ¨çš„æ¨¡å‹è®¾è®¡æ¥æ”¹è¿›åŒ–å­¦ä¿¡æ¯å­¦ä¸­å…³é”®å·¥å…·ï¼ˆMLIPsï¼‰çš„æ–¹æ³•ï¼Œè¿™ä¸å¼€å‘æ›´æ™ºèƒ½ã€æ›´å¯è§£é‡Šçš„â€œåŒ–å­¦å¤§æ¨¡å‹â€çš„ç›®æ ‡é«˜åº¦ç›¸å…³ã€‚

<details>
<summary><b>ğŸ” æŸ¥çœ‹åŸæ–‡æ‘˜è¦</b></summary>

An adaptive physics-inspired model design strategy for machine-learning interatomic potentials (MLIPs) is proposed. This strategy relies on iterative reconfigurations of composite models from single-term models, followed by a unified training procedure. A model evaluation method based on the Fisher information matrix (FIM) and multiple-property error metrics is also proposed to guide the model reconfiguration and hyperparameter optimization. By combining the reconfiguration and the evaluation subroutines, we provide an adaptive MLIP design strategy that balances flexibility and extensibility. In a case study of designing models against a structurally diverse niobium dataset, we managed to obtain an optimal model configuration with 75 parameters generated by our framework that achieved a force RMSE of 0.172 eV/Ã… and an energy RMSE of 0.013 eV/atom.

</details>

---

### 43. [Understanding protein function with a multimodal retrieval-augmented foundation model](https://arxiv.org/abs/2508.04724)

**åŸºæœ¬ä¿¡æ¯**

- ğŸ”— arXiv: [`2508.04724`](https://arxiv.org/abs/2508.04724)
- ğŸ‘¥ ä½œè€…: Timothy Fei Truong Jr, Tristan Bepler
- ğŸ“„ PDF: [ä¸‹è½½](https://arxiv.org/pdf/2508.04724.pdf)

**ğŸ’¡ ç›¸å…³æ€§åˆ†æ**

æ»¡è¶³æ ‡å‡†1ï¼šè®ºæ–‡çš„æ ¸å¿ƒç ”ç©¶å†…å®¹æ˜¯å¼€å‘ä¸€ä¸ªåä¸ºPoET-2çš„å¤šæ¨¡æ€ã€æ£€ç´¢å¢å¼ºçš„è›‹ç™½è´¨åŸºç¡€æ¨¡å‹ã€‚è›‹ç™½è´¨è¯­è¨€æ¨¡å‹ï¼ˆPLMsï¼‰æ˜¯â€œåŒ–å­¦å¤§æ¨¡å‹â€åœ¨ç”Ÿç‰©åˆ†å­é¢†åŸŸçš„å…³é”®åº”ç”¨å’Œå‰æ²¿æ–¹å‘ã€‚æœ¬æ–‡ç›´æ¥å›´ç»•å¦‚ä½•æ„å»ºå’Œæå‡è¿™ç±»åŒ–å­¦å¤§æ¨¡å‹çš„æ€§èƒ½ï¼ˆå¦‚é›¶æ ·æœ¬é¢„æµ‹ã€è¡¨ç¤ºå­¦ä¹ ï¼‰å±•å¼€ï¼Œå¹¶å¼•å…¥äº†æ–°çš„æ¶æ„å’Œè®­ç»ƒèŒƒå¼ã€‚

**ğŸ“– ä¸­æ–‡æ‘˜è¦**

æœ¬æ–‡ä»‹ç»äº†PoET-2ï¼Œä¸€ä¸ªå¤šæ¨¡æ€ã€æ£€ç´¢å¢å¼ºçš„è›‹ç™½è´¨åŸºç¡€æ¨¡å‹ã€‚è¯¥æ¨¡å‹ç»“åˆäº†å®¶æ—ç‰¹å¼‚æ€§è¿›åŒ–çº¦æŸçš„ä¸Šä¸‹æ–‡å­¦ä¹ ä»¥åŠå¯é€‰çš„ç»“æ„æ¡ä»¶ï¼Œä»¥å­¦ä¹ è›‹ç™½è´¨åºåˆ—çš„ç”Ÿæˆåˆ†å¸ƒã€‚PoET-2é‡‡ç”¨åˆ†å±‚Transformerç¼–ç å™¨ï¼ˆå¯¹åºåˆ—ä¸Šä¸‹æ–‡é¡ºåºå…·æœ‰ç­‰å˜æ€§ï¼‰å’Œå…·æœ‰å› æœä¸æ©ç è¯­è¨€å»ºæ¨¡ç›®æ ‡çš„åŒè§£ç å™¨æ¶æ„ï¼Œä½¿å…¶èƒ½å¤Ÿåœ¨å®Œå…¨ç”Ÿæˆå’ŒåŒå‘è¡¨ç¤ºå­¦ä¹ ä¸¤ç§æ¨¡å¼ä¸‹è¿è¡Œã€‚PoET-2åœ¨é›¶æ ·æœ¬å˜ä½“æ•ˆåº”é¢„æµ‹ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå°¤å…¶åœ¨è¯„åˆ†å¤šé‡çªå˜å’Œå…·æœ‰æŒ‘æˆ˜æ€§çš„æ’å…¥ç¼ºå¤±çªå˜æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚åœ¨ç›‘ç£è®¾ç½®ä¸‹ï¼ŒPoET-2çš„åµŒå…¥åœ¨ä»åºåˆ—å­¦ä¹ åŠŸèƒ½å…³ç³»æ–¹é¢ä¼˜äºå…ˆå‰çš„æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨å°æ•°æ®é›†ä¸Šã€‚è¿™é¡¹å·¥ä½œå¼ºè°ƒäº†å°†æ£€ç´¢å¢å¼ºä¸å¤šæ¨¡æ€ã€ä»¥å®¶æ—ä¸ºä¸­å¿ƒçš„å»ºæ¨¡ç›¸ç»“åˆï¼Œå¯¹äºæ¨è¿›è›‹ç™½è´¨åŸºç¡€æ¨¡å‹çš„ç›Šå¤„ã€‚è›‹ç™½è´¨è¯­è¨€æ¨¡å‹æ˜¯â€œåŒ–å­¦å¤§æ¨¡å‹â€åœ¨ç”Ÿç‰©åŒ–å­¦å’Œè¯ç‰©å‘ç°é¢†åŸŸçš„ä¸€ä¸ªé‡è¦å­ç±»ã€‚

<details>
<summary><b>ğŸ” æŸ¥çœ‹åŸæ–‡æ‘˜è¦</b></summary>

Protein language models (PLMs) learn probability distributions over natural protein sequences. By learning from hundreds of millions of natural protein sequences, protein understanding and design capabilities emerge. Recent works have shown that scaling these models improves structure prediction, but does not seem to improve mutation understanding and representation quality for protein function prediction. We introduce PoET-2, a multimodal, retrieval-augmented protein foundation model that incorporates in-context learning of family-specific evolutionary constraints with optional structure conditioning to learn generative distributions over protein sequences. PoET-2 uses a hierarchical transformer encoder that is equivariant to sequence context ordering and a dual decoder architecture with both causal and masked language modeling objectives, allowing PoET-2 to operate in both fully generative and bidirectional representation learning modes. PoET-2 achieves state-of-the-art performance on zero-shot variant effect prediction, excelling at scoring variants with multiple mutations and challenging indel mutations. In supervised settings, PoET-2 embeddings outperform previous methods for learning sequence-function relationships, especially with small datasets. This work highlights the benefits of combining retrieval augmentation with multimodal, family-centric modeling for advancing protein foundation models.

</details>

---

### 44. [TokEye: Fast Signal Extraction for Fluctuating Time Series via Offline Self-Supervised Learning From Fusion Diagnostics to Bioacoustics](https://arxiv.org/abs/2602.20317)

**åŸºæœ¬ä¿¡æ¯**

- ğŸ”— arXiv: [`2602.20317`](https://arxiv.org/abs/2602.20317)
- ğŸ‘¥ ä½œè€…: Nathaniel Chen, Kouroche Bouchiat, Peter Steiner ç­‰9äºº
- ğŸ“„ PDF: [ä¸‹è½½](https://arxiv.org/pdf/2602.20317.pdf)

**ğŸ’¡ ç›¸å…³æ€§åˆ†æ**

æ»¡è¶³æ ‡å‡†1å’Œ2ï¼š1) æ ¸å¿ƒä¸»é¢˜ç›¸å…³ï¼šè®ºæ–‡æå‡ºäº†ä¸€ç§ä»é«˜å™ªå£°æ—¶é¢‘æ•°æ®ï¼ˆè°±å›¾ï¼‰ä¸­è‡ªåŠ¨æå–ç›¸å¹²å’Œç¬æ€æ¨¡å¼çš„é€šç”¨æ¡†æ¶ã€‚è´¨è°±æ•°æ®æœ¬è´¨ä¸Šæ˜¯è´¨è·æ¯”ä¸å¼ºåº¦çš„è°±å›¾ï¼Œå…¶ç»“æ„æ¨ç†é¢ä¸´ç±»ä¼¼çš„å™ªå£°å’Œç‰¹å¾æå–æŒ‘æˆ˜ã€‚è¯¥æ¡†æ¶çš„æ–¹æ³•è®ºï¼ˆè‡ªç›‘ç£å­¦ä¹ ã€ä¿¡å·å¤„ç†ã€ç¥ç»ç½‘ç»œä»£ç†ï¼‰å¯ç›´æ¥ç±»æ¯”æˆ–åº”ç”¨äºè´¨è°±è§£æé—®é¢˜ã€‚2) æ•°æ®èµ„æº/å·¥å…·ç›¸å…³ï¼šè®ºæ–‡å¼€å‘äº†ä¸€ä¸ªé€šç”¨å·¥å…·ï¼ˆTokEyeï¼‰ï¼Œå¹¶æä¾›äº†ä»£ç ä»“åº“ï¼Œå¯ç”¨äºå¤„ç†ç±»ä¼¼è°±å›¾çš„ä¿¡å·æå–ä»»åŠ¡ï¼Œè¿™ä¸ºè´¨è°±æ•°æ®åˆ†ææä¾›äº†æ½œåœ¨çš„å·¥å…·èµ„æºã€‚

**ğŸ“– ä¸­æ–‡æ‘˜è¦**

æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªâ€œä¿¡å·ä¼˜å…ˆâ€çš„è‡ªç›‘ç£æ¡†æ¶ï¼Œç”¨äºä»å„ç§ä¼ æ„Ÿå™¨çš„é«˜å™ªå£°æ—¶é¢‘æ•°æ®ä¸­è‡ªåŠ¨æå–ç›¸å¹²å’Œç¬æ€æ¨¡å¼ã€‚ä½œè€…å¼€å‘äº†ä¸€ç§é€šç”¨æ–¹æ³•å’Œå·¥å…·ï¼Œé€šè¿‡åœ¨å¤šé€šé“ä¿¡å·å¤„ç†ä¸­é‡‡ç”¨éçº¿æ€§æœ€ä¼˜æŠ€æœ¯ï¼Œå¹¶åˆ©ç”¨å¿«é€Ÿç¥ç»ç½‘ç»œä»£ç†ï¼Œä»æ‰˜å¡é©¬å…‹è£…ç½®ï¼ˆå¦‚DIII-Dï¼‰çš„å¿«ç£ã€ç”µå­å›æ—‹è¾å°„ã€CO2å¹²æ¶‰ä»ªå’ŒæŸå‘å°„å…‰è°±æµ‹é‡ä¸­æå–ç›¸å¹²ã€å‡†ç›¸å¹²å’Œç¬æ€æ¨¡å¼ã€‚è¯¥æ¡†æ¶åœ¨DIII-Dã€TJ-IIå’Œéèåˆè°±å›¾æ•°æ®ä¸Šè¿›è¡Œäº†æµ‹è¯•ã€‚æ¨ç†å»¶è¿Ÿä¸º0.5ç§’ï¼Œä½¿å¾—è¯¥æ¡†æ¶èƒ½å¤Ÿå®ç°å®æ—¶æ¨¡å¼è¯†åˆ«å’Œå¤§è§„æ¨¡è‡ªåŠ¨åŒ–æ•°æ®åº“ç”Ÿæˆï¼Œç”¨äºå…ˆè¿›çš„ç­‰ç¦»å­ä½“æ§åˆ¶ã€‚è¯¥æ–¹æ³•è™½ç„¶åº”ç”¨äºèšå˜è¯Šæ–­ï¼Œä½†å…¶æ ¸å¿ƒæ˜¯ä»å¤æ‚ã€é«˜å™ªå£°çš„è°±å›¾ï¼ˆä¸€ç§ä¸è´¨è°±å›¾åœ¨æ•°å­¦å’Œä¿¡å·å¤„ç†ä¸Šç±»ä¼¼çš„æ•°æ®å½¢å¼ï¼‰ä¸­æå–ç‰¹å¾å’Œæ¨¡å¼ã€‚è¿™ä¸ºâ€œè´¨è°±ç»“æ„æ¨ç†â€ä¸­ä»åŸå§‹è°±æ•°æ®è§£æå‡ºåŒ–å­¦ç»“æ„ä¿¡æ¯æä¾›äº†æ–¹æ³•è®ºä¸Šçš„å‚è€ƒã€‚

<details>
<summary><b>ğŸ” æŸ¥çœ‹åŸæ–‡æ‘˜è¦</b></summary>

Next-generation fusion facilities like ITER face a "data deluge," generating petabytes of multi-diagnostic signals daily that challenge manual analysis. We present a "signals-first" self-supervised framework for the automated extraction of coherent and transient modes from high-noise time-frequency data across a variety of sensors. We also develop a general-purpose method and tool for extracting coherent, quasi-coherent, and transient modes for fluctuation measurements in tokamaks by employing non-linear optimal techniques in multichannel signal processing with a fast neural network surrogate on fast magnetics, electron cyclotron emission, CO2 interferometers, and beam emission spectroscopy measurements from DIII-D. Results are tested on data from DIII-D, TJ-II, and non-fusion spectrograms. With an inference latency of 0.5 seconds, this framework enables real-time mode identification and large-scale automated database generation for advanced plasma control. Repository is in this https URL .

</details>

---

### 45. [Not Just How Much, But Where: Decomposing Epistemic Uncertainty into Per-Class Contributions](https://arxiv.org/abs/2602.21160)

**åŸºæœ¬ä¿¡æ¯**

- ğŸ”— arXiv: [`2602.21160`](https://arxiv.org/abs/2602.21160)
- ğŸ‘¥ ä½œè€…: Mame Diarra Toure, David A. Stephens
- ğŸ“„ PDF: [ä¸‹è½½](https://arxiv.org/pdf/2602.21160.pdf)

**ğŸ’¡ ç›¸å…³æ€§åˆ†æ**

æ»¡è¶³æ ‡å‡†1ï¼šè®ºæ–‡çš„æ ¸å¿ƒç ”ç©¶å†…å®¹æ˜¯æ”¹è¿›è´å¶æ–¯æ·±åº¦å­¦ä¹ ä¸­çš„è®¤çŸ¥ä¸ç¡®å®šæ€§é‡åŒ–ï¼Œæå‡ºäº†ä¸€ç§å¯è§£é‡Šçš„é€ç±»ä¸ç¡®å®šæ€§åˆ†è§£æ–¹æ³•ã€‚å¯¹äºâ€œåŒ–å­¦å¤§æ¨¡å‹â€å’ŒåŸºäºæœºå™¨å­¦ä¹ çš„â€œè´¨è°±ç»“æ„æ¨ç†â€æ¨¡å‹ï¼Œç†è§£æ¨¡å‹åœ¨å“ªäº›ç‰¹å®šåŒ–å­¦ç±»åˆ«æˆ–ç»“æ„ç‰¹å¾ä¸Šä¸ç¡®å®šï¼Œå¯¹äºæé«˜é¢„æµ‹çš„å¯é æ€§å’Œå¯è§£é‡Šæ€§è‡³å…³é‡è¦ã€‚è¿™é¡¹å·¥ä½œç›´æ¥æä¾›äº†æå‡æ­¤ç±»æ¨¡å‹å®‰å…¨æ€§å’Œå¯ä¿¡åº¦çš„æŠ€æœ¯è·¯å¾„ã€‚

**ğŸ“– ä¸­æ–‡æ‘˜è¦**

åœ¨å®‰å…¨å…³é”®åˆ†ç±»ä¸­ï¼Œå¤±è´¥çš„ä»£ä»·é€šå¸¸æ˜¯ä¸å¯¹ç§°çš„ï¼Œç„¶è€Œè´å¶æ–¯æ·±åº¦å­¦ä¹ ç”¨å•ä¸€æ ‡é‡â€”â€”äº’ä¿¡æ¯ï¼ˆMIï¼‰æ¥æ€»ç»“è®¤çŸ¥ä¸ç¡®å®šæ€§ï¼Œè¿™æ— æ³•åŒºåˆ†æ¨¡å‹çš„æœªçŸ¥æ€§æ¶‰åŠçš„æ˜¯è‰¯æ€§ç±»åˆ«è¿˜æ˜¯å®‰å…¨å…³é”®ç±»åˆ«ã€‚ä½œè€…å°†MIåˆ†è§£ä¸ºä¸€ä¸ªé€ç±»å‘é‡ï¼Œè¯¥åˆ†è§£æºäºç†µçš„äºŒé˜¶æ³°å‹’å±•å¼€ã€‚é€šè¿‡æ„é€ ï¼Œå„åˆ†é‡çš„å’Œè¿‘ä¼¼ç­‰äºMIã€‚ä½œè€…åœ¨ä¸‰ä¸ªä»»åŠ¡ä¸ŠéªŒè¯äº†è¯¥åˆ†è§£ï¼šç³–å°¿ç—…è§†ç½‘è†œç—…å˜çš„é€‰æ‹©æ€§é¢„æµ‹ã€ä¸´åºŠå’Œå›¾åƒåŸºå‡†ä¸Šçš„åˆ†å¸ƒå¤–æ£€æµ‹ï¼Œä»¥åŠå—æ§çš„æ ‡ç­¾å™ªå£°ç ”ç©¶ã€‚åœ¨ç³–å°¿ç—…è§†ç½‘è†œç—…å˜ä»»åŠ¡ä¸­ï¼Œé’ˆå¯¹å…³é”®ç±»åˆ«çš„åˆ†è§£åˆ†é‡å°†é€‰æ‹©æ€§é£é™©ç›¸å¯¹äºMIé™ä½äº†34.7%ï¼Œç›¸å¯¹äºæ–¹å·®åŸºçº¿é™ä½äº†56.2%ã€‚è¿™é¡¹å·¥ä½œè™½ç„¶ä¸»è¦é’ˆå¯¹é€šç”¨åˆ†ç±»ä»»åŠ¡ï¼Œä½†å…¶æ ¸å¿ƒæ˜¯æ”¹è¿›ä¸ç¡®å®šæ€§é‡åŒ–ï¼ˆUQï¼‰çš„å¯è§£é‡Šæ€§ã€‚åœ¨â€œåŒ–å­¦å¤§æ¨¡å‹â€å’Œâ€œè´¨è°±ç»“æ„æ¨ç†â€ä¸­ï¼Œæ¨¡å‹é¢„æµ‹çš„å¯ä¿¡åº¦è¯„ä¼°å’Œä¸ç¡®å®šæ€§åˆ†è§£ï¼ˆä¾‹å¦‚ï¼ŒåŒºåˆ†æ˜¯å¯¹åŒ–åˆç‰©ç±»åˆ«è¿˜æ˜¯ç‰¹å®šå®˜èƒ½å›¢ä¸ç¡®å®šï¼‰å¯¹äºå¯é éƒ¨ç½²è‡³å…³é‡è¦ã€‚æœ¬æ–‡æå‡ºçš„æ–¹æ³•ä¸ºåŒ–å­¦ä¿¡æ¯å­¦ä¸­å¤æ‚æ¨¡å‹çš„å¯é æ€§åˆ†ææä¾›äº†æ–°çš„æ€è·¯ã€‚

<details>
<summary><b>ğŸ” æŸ¥çœ‹åŸæ–‡æ‘˜è¦</b></summary>

In safety-critical classification, the cost of failure is often asymmetric, yet Bayesian deep learning summarises epistemic uncertainty with a single scalar, mutual information (MI), that cannot distinguish whether a model's ignorance involves a benign or safety-critical class. We decompose MI into a per-class vector $C_k(x)=\sigma_k^{2}/(2\mu_k)$, with $\mu_k{=}\mathbb{E}[p_k]$ and $\sigma_k^2{=}\mathrm{Var}[p_k]$ across posterior samples. The decomposition follows from a second-order Taylor expansion of the entropy; the $1/\mu_k$ weighting corrects boundary suppression and makes $C_k$ comparable across rare and common classes. By construction $\sum_k C_k \approx \mathrm{MI}$, and a companion skewness diagnostic flags inputs where the approximation degrades. After characterising the axiomatic properties of $C_k$, we validate it on three tasks: (i) selective prediction for diabetic retinopathy, where critical-class $C_k$ reduces selective risk by 34.7\% over MI and 56.2\% over variance baselines; (ii) out-of-distribution detection on clinical and image benchmarks, where $\sum_k C_k$ achieves the highest AUROC and the per-class view exposes asymmetric shifts invisible to MI; and (iii) a controlled label-noise study in which $\sum_k C_k$ shows less sensitivity to injected aleatoric noise than MI under end-to-end Bayesian training, while both metrics degrade under transfer learning. Across all tasks, the quality of the posterior approximation shapes uncertainty at least as strongly as the choice of metric, suggesting that how uncertainty is propagated through the network matters as much as how it is measured.

</details>

---

## ğŸ“Š æ•°æ®ç»Ÿè®¡
- ç´¯è®¡è¿è¡Œå¤©æ•°ï¼š3
- ç´¯è®¡è®ºæ–‡æ•°é‡ï¼š134

## ğŸ“ å†å²è®°å½•

> æš‚æ— å†å²æ•°æ®

