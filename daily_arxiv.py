import os
import re
import json
import time
import requests
import datetime
from bs4 import BeautifulSoup
from openai import OpenAI
import logging

# é…ç½® - å›ºå®šå€¼
LIST_URL = "https://arxiv.org/list/cs/new"
TOPICS = ["åŒ–å­¦å¤§æ¨¡å‹", "è´¨è°±ç»“æ„æ¨ç†"]
MODEL = "deepseek-chat"
HISTORY_PATH = "data/history.json"
OUTPUT_FILE = "README.md"

logging.basicConfig(format='[%(asctime)s %(levelname)s] %(message)s',
                    datefmt='%m/%d/%Y %H:%M:%S',
                    level=logging.INFO)


def normalize_whitespace(text):
    """æ¸…ç†ç©ºç™½å­—ç¬¦"""
    return re.sub(r"\s+", " ", text or "").strip()

def parse_submitted_date(raw_line):
    """è§£ææäº¤æ—¥æœŸ"""
    if not raw_line:
        return None
    match = re.search(r"Submitted on\s+([0-9]{1,2}\s+\w+\s+[0-9]{4})", raw_line)
    if not match:
        return None
    raw_date = match.group(1).strip()
    for fmt in ("%d %b %Y", "%d %B %Y"):
        try:
            parsed = datetime.datetime.strptime(raw_date, fmt)
            return parsed.strftime("%Y-%m-%d")
        except ValueError:
            continue
    return raw_date

def fetch_papers():
    """æŠ“å–arxivè®ºæ–‡åˆ—è¡¨"""
    logging.info(f"æ­£åœ¨æŠ“å–: {LIST_URL}")
    resp = requests.get(LIST_URL, timeout=30)
    resp.raise_for_status()
    soup = BeautifulSoup(resp.text, "html.parser")
    
    papers = []
    for dt in soup.find_all("dt"):
        dd = dt.find_next_sibling("dd")
        if not dd:
            continue
            
        link = dt.find("a", title="Abstract")
        if not link:
            continue
            
        arxiv_id = link.text.replace("arXiv:", "").strip()
        
        # æ ‡é¢˜
        title_node = dd.find("div", class_="list-title")
        title = normalize_whitespace(title_node.get_text(" ", strip=True).replace("Title:", "", 1)) if title_node else arxiv_id
        
        # ä½œè€…
        author_links = dd.select("div.list-authors a")
        authors = [normalize_whitespace(a.get_text()) for a in author_links]
        
        # æ‘˜è¦
        abstract_node = dd.find("p", class_="mathjax")
        abstract = normalize_whitespace(abstract_node.get_text(" ", strip=True).replace("Abstract:", "", 1)) if abstract_node else ""
        
        # æäº¤æ—¥æœŸ
        submitted = None
        submitted_node = dd.find("div", class_="list-submitted")
        if submitted_node:
            submitted = parse_submitted_date(normalize_whitespace(submitted_node.get_text(" ", strip=True)))
        
        papers.append({
            "arxiv_id": arxiv_id,
            "title": title,
            "authors": authors,
            "abstract": abstract,
            "link": f"https://arxiv.org/abs/{arxiv_id}",
            "pdf_link": f"https://arxiv.org/pdf/{arxiv_id}.pdf",
            "submitted": submitted
        })
    
    logging.info(f"æŠ“å–åˆ° {len(papers)} ç¯‡è®ºæ–‡")
    return papers

def analyze_papers(papers):
    """è°ƒç”¨å¤§æ¨¡å‹åˆ†æè®ºæ–‡"""
    if not papers:
        return []
    
    # ä»ç¯å¢ƒå˜é‡è·å–API key
    api_key = os.environ.get("DEEPSEEK_API_KEY")
    if not api_key:
        raise Exception("è¯·è®¾ç½® DEEPSEEK_API_KEY ç¯å¢ƒå˜é‡")
    
    client = OpenAI(
        api_key=api_key,
        base_url=os.environ.get("DEEPSEEK_API_BASE", "https://api.deepseek.com")
    )
    
    # åˆ†æ‰¹å¤„ç†ï¼Œé¿å…tokenè¶…é™
    chunk_size = 100
    all_results = []
    
    for i in range(0, len(papers), chunk_size):
        time.sleep(3)
        chunk = papers[i:i+chunk_size]
        
        # å‡†å¤‡æ•°æ®
        topics_text = "\n".join(f"- {t}" for t in TOPICS)
        papers_data = [{"arxiv_id": p["arxiv_id"], "title": p["title"], "abstract": p["abstract"]} for p in chunk]
        
        prompt = f"""ä½ æ˜¯ä¸€ä½åœ¨ã€åŒ–å­¦ä¿¡æ¯å­¦ã€‘å’Œã€è´¨è°±åˆ†æã€‘é¢†åŸŸçš„ç§‘ç ”ä¸“å®¶ã€‚è¯·ç­›é€‰å‡ºä¸æŒ‡å®šç ”ç©¶ä¸»é¢˜ç›¸å…³çš„è®ºæ–‡ã€‚

ã€é‡ç‚¹å…³æ³¨ä¸»é¢˜ã€‘
{topics_text}

ã€ç›¸å…³æ€§åˆ¤æ–­æ ‡å‡†ã€‘ï¼ˆæ»¡è¶³ä»»æ„ä¸€æ¡å³å¯è§†ä¸ºç›¸å…³ï¼‰ï¼š
1. âœ… æ ¸å¿ƒä¸»é¢˜ç›¸å…³ï¼šè®ºæ–‡çš„ä¸»è¦ç ”ç©¶å†…å®¹ç›´æ¥å›´ç»•è¿™äº›ä¸»é¢˜
2. âœ… æ•°æ®èµ„æºç›¸å…³ï¼šè®ºæ–‡æä¾›äº†å¯ç”¨äºè¿™äº›ä¸»é¢˜çš„æ•°æ®é›†ã€èµ„æºæˆ–å·¥å…·
3. âœ… ç»¼è¿°å±•æœ›ç›¸å…³ï¼šè®ºæ–‡æ˜¯ä¸“é—¨é’ˆå¯¹è¿™äº›ä¸»é¢˜çš„ç»¼è¿°ï¼Œæˆ–åŒ…å«é‡è¦çš„ç›¸å…³è®¨è®º

ã€ç­›é€‰åŸåˆ™ã€‘
- åªè¦æ»¡è¶³ä¸Šè¿°ä»»ä¸€æ ‡å‡†ï¼Œå³å¯çº³å…¥
- ä¸éœ€è¦è®ºæ–‡åŒæ—¶æ»¡è¶³å¤šä¸ªæ ‡å‡†
- åŒ…å®¹æ€§ç­›é€‰ï¼Œä½†éœ€è¦ç¡®å®ç›¸å…³ï¼ˆä¸æ˜¯å®Œå…¨æ— å…³ï¼‰
- ç»¼è¿°ç±»è®ºæ–‡å¦‚æœæ˜¯ä¸“é—¨é’ˆå¯¹è¿™äº›ä¸»é¢˜ï¼Œæˆ–åŒ…å«é‡è¦ç« èŠ‚è®¨è®ºè¿™äº›ä¸»é¢˜ï¼Œä¹Ÿåº”çº³å…¥

è¯·åˆ†æä»¥ä¸‹è®ºæ–‡åˆ—è¡¨ï¼Œç­›é€‰å‡ºã€ç›¸å…³ã€‘çš„è®ºæ–‡ã€‚å¯¹äºæ¯ç¯‡ç›¸å…³è®ºæ–‡ï¼Œéœ€è¦è¯´æ˜å®ƒæ»¡è¶³äº†å“ªæ¡æ ‡å‡†ã€‚

è¾“å‡ºæ ¼å¼å¿…é¡»æ˜¯ä¸¥æ ¼çš„JSONï¼š
{{
    "results": [
        {{
            "arxiv_id": "è®ºæ–‡ID",
            "zh_summary": "500å­—ä»¥å†…çš„ä¸­æ–‡æ‘˜è¦æ€»ç»“ï¼Œçªå‡ºè®ºæ–‡ä¸å…³æ³¨ä¸»é¢˜ç›¸å…³çš„å†…å®¹",
            "relevance_reason": "è¯´æ˜è¿™ç¯‡è®ºæ–‡çš„ç›¸å…³æ€§ï¼ŒæŒ‡å‡ºæ»¡è¶³äº†å“ªæ¡æ ‡å‡†ï¼ˆä¾‹å¦‚ï¼š'æ»¡è¶³æ ‡å‡†2ï¼šæå‡ºçš„å›¾ç¥ç»ç½‘ç»œæ–¹æ³•å¯ç”¨äºè´¨è°±ç»“æ„æ¨ç†'ï¼‰"
        }}
    ]
}}

å¦‚æœæ²¡æœ‰ç›¸å…³è®ºæ–‡ï¼Œè¯·è¿”å›ç©ºçš„resultsåˆ—è¡¨ï¼š{{"results": []}}

è®ºæ–‡åˆ—è¡¨ï¼š
{json.dumps(papers_data, ensure_ascii=False, indent=2)}"""

        # è°ƒç”¨APIï¼ˆå¸¦é‡è¯•ï¼‰
        for attempt in range(3):
            try:
                logging.info(f"æ­£åœ¨åˆ†æç¬¬ {i//chunk_size + 1}/{(len(papers)-1)//chunk_size + 1} æ‰¹ï¼Œå…± {len(chunk)} ç¯‡...")
                
                response = client.chat.completions.create(
                    model=MODEL,
                    temperature=0.6,
                    messages=[
                        {"role": "system", "content": """ä½ æ˜¯ä¸€ä¸ªç§‘ç ”è®ºæ–‡ç­›é€‰åŠ©æ‰‹ã€‚ä½ çš„ç‰¹ç‚¹æ˜¯ï¼š
1. æŒ‰ç…§ç»™å®šçš„ç›¸å…³æ€§æ ‡å‡†ç­›é€‰è®ºæ–‡
2. åªè¦æ»¡è¶³ä»»æ„ä¸€æ¡æ ‡å‡†å³å¯çº³å…¥
3. å¯¹æ¯ç¯‡é€‰ä¸­çš„è®ºæ–‡éƒ½è¦æ˜ç¡®æŒ‡å‡ºæ»¡è¶³äº†å“ªæ¡æ ‡å‡†
4. ç­›é€‰è¦åˆç†ï¼Œä¸è¦è¿‡åº¦ç­›é€‰ï¼Œä¹Ÿä¸è¦æ¼æ‰æ˜æ˜¾ç›¸å…³çš„è®ºæ–‡"""},
                        {"role": "user", "content": prompt}
                    ]
                )
                
                content = response.choices[0].message.content
                
                # æå–JSON
                json_match = re.search(r"\{[\s\S]*\}", content)
                if json_match:
                    result = json.loads(json_match.group())
                    results = result.get("results", [])
                    
                    # ç®€å•æ£€æŸ¥ï¼šå¦‚æœé€‰ä¸­æ¯”ä¾‹è¿‡ä½ï¼Œæé†’ä½†ç»§ç»­
                    if len(results) == 0 and len(chunk) > 10:
                        logging.info(f"â„¹ï¸ æç¤ºï¼šæ­¤æ‰¹æ¬¡æœªé€‰ä¸­ä»»ä½•è®ºæ–‡")
                    
                    all_results.extend(results)
                    logging.info(f"âœ… æ­¤æ‰¹æ¬¡ç­›é€‰å‡º {len(results)} ç¯‡ç›¸å…³è®ºæ–‡")
                    break
                else:
                    logging.info("æ— æ³•è§£ææ¨¡å‹è¾“å‡º")
                    
            except Exception as e:
                logging.error(f"è°ƒç”¨å¤±è´¥ (å°è¯• {attempt+1}/3): {e}")
                if attempt == 2:
                    logging.info(f"è·³è¿‡æ­¤æ‰¹æ¬¡")
                else:
                    time.sleep(2 ** attempt)
    
    # å»é‡å¤„ç†
    seen_ids = set()
    unique_results = []
    for r in all_results:
        if r["arxiv_id"] not in seen_ids:
            seen_ids.add(r["arxiv_id"])
            unique_results.append(r)
    
    logging.info(f"\nğŸ“Š ç­›é€‰å®Œæˆï¼Œæ€»å…±æ‰¾åˆ° {len(unique_results)} ç¯‡ç›¸å…³è®ºæ–‡")
    return unique_results        

def load_history():
    """åŠ è½½å†å²è®°å½•"""
    history = {}
    if os.path.exists(HISTORY_PATH):
        try:
            with open(HISTORY_PATH, "r", encoding="utf-8") as f:
                history = json.load(f)
            logging.info(f"åŠ è½½å†å²è®°å½•ï¼Œå…± {len(history)} å¤©")
        except:
            logging.error("å†å²æ–‡ä»¶ä¸å­˜åœ¨æˆ–æŸåï¼Œåˆ›å»ºæ–°çš„å†å²è®°å½•")
    return history

def save_history(history):
    """ä¿å­˜å†å²è®°å½•"""
    # åˆ›å»ºç›®å½•
    os.makedirs(os.path.dirname(HISTORY_PATH), exist_ok=True)
    
    # ä¿å­˜
    with open(HISTORY_PATH, "w", encoding="utf-8") as f:
        json.dump(history, f, ensure_ascii=False, indent=2)
    
    logging.info(f"å†å²è®°å½•å·²ä¿å­˜åˆ° {HISTORY_PATH}")

def generate_readme(today_papers):
    """ç”ŸæˆREADMEï¼Œå½“å¤©æœ€æ–°çš„å†…å®¹æ’å…¥åˆ°æœ€å‰é¢"""
    now = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    today = datetime.datetime.now().strftime("%Y-%m-%d")
    
    # è¯»å–ç°æœ‰çš„ README å†…å®¹
    existing_content = ""
    if os.path.exists(OUTPUT_FILE):
        try:
            with open(OUTPUT_FILE, "r", encoding="utf-8") as f:
                existing_content = f.read()
            logging.info("è¯»å–ç°æœ‰ README æ–‡ä»¶")
        except Exception as e:
            logging.error(f"è¯»å–ç°æœ‰ README å¤±è´¥: {e}")
    
    with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
        # å†™å…¥æ ‡é¢˜å’Œæ›´æ–°æ—¶é—´ï¼ˆå§‹ç»ˆä¿ç•™åœ¨æœ€ä¸Šé¢ï¼‰
        f.write("# ğŸ“š ArXiv è®ºæ–‡æ—¥æŠ¥\n\n")
        f.write(f"> æ¯å¤©è‡ªåŠ¨æ›´æ–°ï¼Œå…³æ³¨ **{', '.join(TOPICS)}** ç›¸å…³çš„æœ€æ–°è®ºæ–‡\n\n")
        
        f.write("## æ›´æ–°æ—¶é—´\n")
        f.write(f"â° {now}\n\n")
        
        # å†™å…¥å½“å¤©çš„è®ºæ–‡ï¼ˆæœ€æ–°çš„ï¼‰
        f.write(f"## ğŸ“… {today} (ä»Šæ—¥æœ€æ–°)\n\n")
        
        if not today_papers:
            f.write("> ä»Šæ—¥æ²¡æœ‰æ‰¾åˆ°ç›¸å…³è®ºæ–‡\n\n")
        else:
            f.write(f"**ç›¸å…³è®ºæ–‡æ•°ï¼š{len(today_papers)}**\n\n")
            
            for i, paper in enumerate(today_papers, 1):
                f.write(f"### {i}. [{paper['title']}]({paper['link']})\n\n")
                
                # åŸºæœ¬ä¿¡æ¯
                f.write("**åŸºæœ¬ä¿¡æ¯**\n\n")
                f.write(f"- ğŸ”— arXiv: [`{paper['arxiv_id']}`]({paper['link']})\n")
                if paper.get('submitted'):
                    f.write(f"- ğŸ“… æäº¤æ—¥æœŸ: {paper['submitted']}\n")
                if paper.get('authors'):
                    authors_display = ', '.join(paper['authors'][:3])
                    if len(paper['authors']) > 3:
                        authors_display += f" ç­‰{len(paper['authors'])}äºº"
                    f.write(f"- ğŸ‘¥ ä½œè€…: {authors_display}\n")
                f.write(f"- ğŸ“„ PDF: [ä¸‹è½½]({paper['pdf_link']})\n\n")
                
                # ç›¸å…³æ€§åˆ†æ
                if paper.get('relevance_reason'):
                    f.write("**ğŸ’¡ ç›¸å…³æ€§åˆ†æ**\n\n")
                    f.write(f"{paper['relevance_reason']}\n\n")
                
                # ä¸­æ–‡æ‘˜è¦
                if paper.get('zh_summary'):
                    f.write("**ğŸ“– ä¸­æ–‡æ‘˜è¦**\n\n")
                    f.write(f"{paper['zh_summary']}\n\n")
                
                # åŸæ–‡æ‘˜è¦ï¼ˆå¯æŠ˜å ï¼‰
                f.write("<details>\n")
                f.write("<summary><b>ğŸ” æŸ¥çœ‹åŸæ–‡æ‘˜è¦</b></summary>\n\n")
                f.write(f"{paper['abstract']}\n\n")
                f.write("</details>\n\n")
                
                f.write("---\n\n")
        
        # å†™å…¥å†å²ç»Ÿè®¡ï¼ˆå¦‚æœå­˜åœ¨ç°æœ‰å†…å®¹ï¼Œæå–å¹¶æ›´æ–°ç»Ÿè®¡ï¼‰
        f.write("## ğŸ“Š æ•°æ®ç»Ÿè®¡\n")
        
        # ä»ç°æœ‰å†…å®¹ä¸­æå–å†å²æ•°æ®
        total_days = 1  # å½“å‰è¿™ä¸€å¤©
        total_papers = len(today_papers)
        
        if existing_content:
            # å°è¯•ä»ç°æœ‰å†…å®¹ä¸­æå–ç»Ÿè®¡æ•°æ®
            stats_pattern = r"- ç´¯è®¡è¿è¡Œå¤©æ•°ï¼š(\d+)\n- ç´¯è®¡è®ºæ–‡æ•°é‡ï¼š(\d+)"
            stats_match = re.search(stats_pattern, existing_content)
            if stats_match:
                total_days = int(stats_match.group(1)) + 1
                total_papers = int(stats_match.group(2)) + len(today_papers)
        
        f.write(f"- ç´¯è®¡è¿è¡Œå¤©æ•°ï¼š{total_days}\n")
        f.write(f"- ç´¯è®¡è®ºæ–‡æ•°é‡ï¼š{total_papers}\n\n")
        
        # å†™å…¥å†å²è®°å½•ï¼ˆå¦‚æœå­˜åœ¨ç°æœ‰å†…å®¹ï¼Œä¿ç•™ä¹‹å‰çš„å†…å®¹ï¼‰
        if existing_content:
            # æŸ¥æ‰¾å†å²è®°å½•çš„èµ·å§‹ä½ç½®
            history_start = existing_content.find("## ğŸ“Š æ•°æ®ç»Ÿè®¡")
            if history_start != -1:
                # æ‰¾åˆ°æ•°æ®ç»Ÿè®¡éƒ¨åˆ†ä¹‹åçš„å†…å®¹
                after_stats = existing_content[history_start:]
                # æ‰¾åˆ°ç¬¬ä¸€ä¸ªå†å²æ—¥æœŸæ ‡é¢˜
                date_start = after_stats.find("## ğŸ“…")
                if date_start != -1:
                    # ä¿ç•™ä»å†å²æ—¥æœŸå¼€å§‹çš„æ‰€æœ‰å†…å®¹
                    historical_content = after_stats[date_start:]
                    f.write(historical_content)
                else:
                    # å¦‚æœæ²¡æœ‰æ‰¾åˆ°å†å²æ—¥æœŸï¼Œåˆ™å†™å…¥æ–°çš„å†å²è®°å½•éƒ¨åˆ†
                    f.write("## ğŸ“ å†å²è®°å½•\n\n")
                    f.write("> æš‚æ— å†å²æ•°æ®\n\n")
            else:
                # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ•°æ®ç»Ÿè®¡éƒ¨åˆ†ï¼Œåˆ™å†™å…¥æ–°çš„å†å²è®°å½•éƒ¨åˆ†
                f.write("## ğŸ“ å†å²è®°å½•\n\n")
                f.write("> æš‚æ— å†å²æ•°æ®\n\n")
        else:
            # å¦‚æœæ²¡æœ‰ç°æœ‰å†…å®¹ï¼Œå†™å…¥ç©ºçš„å†å²è®°å½•
            f.write("## ğŸ“ å†å²è®°å½•\n\n")
            f.write("> æš‚æ— å†å²æ•°æ®\n\n")
    
    logging.info(f"README å·²æ›´æ–°ï¼Œä»Šæ—¥æ·»åŠ  {len(today_papers)} ç¯‡è®ºæ–‡")

def main():
    """ä¸»å‡½æ•°"""
    try:
        logging.info("="*50)
        logging.info("å¼€å§‹æŠ“å– arXiv è®ºæ–‡")
        logging.info("="*50)
        
        # 1. åŠ è½½å†å²è®°å½•
        history = load_history()
        
        # 2. æŠ“å–è®ºæ–‡
        papers = fetch_papers()

        # papers = papers[:100]  # å–å‰100ç¯‡ï¼Œé¿å…è¿‡å¤šæ— å…³è®ºæ–‡å¹²æ‰°åˆ†æ

        logging.info(f"æŠ“å–åˆ° {len(papers)} ç¯‡è®ºæ–‡ï¼Œå‡†å¤‡åˆ†æ...")
        
        # 3. å¤§æ¨¡å‹åˆ†æ
        results = analyze_papers(papers)
        
        # 4. åˆå¹¶æ•°æ®
        today = datetime.datetime.now().strftime("%Y-%m-%d")
        result_dict = {r["arxiv_id"]: r for r in results}
        today_papers = []
        
        for paper in papers:
            if paper["arxiv_id"] in result_dict:
                today_papers.append({
                    **paper,
                    "zh_summary": result_dict[paper["arxiv_id"]].get("zh_summary", ""),
                    "relevance_reason": result_dict[paper["arxiv_id"]].get("relevance_reason", "")
                })
        
        # 5. æ›´æ–°å†å²è®°å½•
        history[today] = today_papers
        save_history(history)
        
        # 6. ç”ŸæˆREADMEï¼ˆæœ€æ–°çš„åœ¨ä¸Šé¢ï¼‰
        generate_readme(today_papers)
        
        logging.info("="*50)
        logging.info(f"âœ… è¿è¡Œå®Œæˆï¼")
        logging.info(f"ğŸ“Š ä»Šæ—¥æŠ“å–: {len(papers)} ç¯‡")
        logging.info(f"ğŸ¯ ä»Šæ—¥ç›¸å…³: {len(today_papers)} ç¯‡")
        logging.info(f"ğŸ“š ç´¯è®¡å¤©æ•°: {len(history)} å¤©")
        logging.info(f"ğŸ“ README å·²æ›´æ–°")
        logging.info("="*50)
        
    except Exception as e:
        logging.error(f"âŒ è¿è¡Œå¤±è´¥: {e}")
        raise

if __name__ == "__main__":
    main()
